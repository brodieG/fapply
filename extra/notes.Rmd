# r2c

# Next steps

* Implement Pow properly (see `R_pow`).
* Implement an option to translate a call to a different form (specifically for
  `pow` up to e.g power 3 or 4.  Note, it appears R does this for `x^2`.

# To Do / Questions

* Side effects from evaluation of control parameters?  Where should assigned
  variables reside?
* Add a finalizer to the r2c objects to clean-up after themselves when `rm`ed?
* Add check to `r2c` funs that verify the `r2c` version, the R version, and the
  platform (anything else)?
* What should the environment of an "r2c_fun" be?
* Figure out a better way to store the object than embedding it with `bquote` in
  the function.
* Test forwarding dots to a `group_exec` function.
* Look more into C99 and infinity, not super clear ATM whether we need to take
  special precautions when converting from long double to double.
* Think about `df %>% group_by(g) %>% filter(x > min(df$x)`.  This should
  resolve naturally if we allow `[['x']]` as "normal" expression.
* What do we do with NA groups?  Each it's own, or one big NA group?
* Can we optimize label generation for cases where the last step is constant
  size or size of groups?
* Add destructors that will unload the shlib and delete file file when the
  corresponding object is garbage collected.
* Better introspection tools for intermediate products so we can see what they
  are.
* Are names sufficient to disqualify from nakedness?  How do names behave in
  recycling?  Do we want to allow other attributes to persist?
* Only sort the columns that are used.
* Is it possible for e.g. someone doing something like `sum(a, trim=1:3)` to
  mismatch to the parameter name, or do dots handle it properly (thinking of
  validation function).
* Make sure to have a function that uses the `ctrl` functionality to check that
  it works properly (once we switch to `flag` for most functions).
* Can we get more memory efficient by e.g. using `+=` to avoid having to have
  memory allocated for both LHS and RHS for the arithmetic operators?  Might
  complicate things substantially.
* Issue recycling warnings?
* Make sure pointer arrays are aligned.  Use `malloc`?  If so internal code
  cannot use `error`, and we can't guarantee that?  Can we register a finalizer
  somehow (probably will need to).
* Look into GraalVM, Renjin?
* Check assumption that double will hold `R_xlen_t`?  Or that length is no
  longer that the allowable size?  A bit tricky; no way to know what double size
  is.  This is because we return the group size to R; otherwise it would just be
  an R_xlen_t vector.  We need this to get the group max for the allocation.
  It's possible R guarantees this will be no bigger than e.g. 2^53 or some such.
* Be sure to test situations where we have external data larger and smaller than
  group sizes.
* Do we need to handle the issue of `envir` for `match.call` at allocation time
  instead of at compile time?  Seems like the way this goes wrong is if the
  expression being looked at itself contains dots, e.g. literally `sum(...)`
  where the `...` need to be fetched.
* To allow assignments we can just use a series of nested environments such that
  each sub-call will then seek the symbol through the nested environments.
  We'll need to bind the data symbols to e.g. environments so they may be
  uniquely identified and we can confirm they've been found.
* Test a single symbol to make sure logic doesn't choke on that.
* Can we compute non-varying variables before compilation for sizing purposes?
  E.g. `1:10` is of knowable size at compilation assuming we don't mess with
  `:`.  Could allow this for non-control parameters.  Definitely non-trivial
  though.
* Make sure headers go in in correct order; it may not work if we structure
  workflow around C functions?  Maybe okay if each group of functions does the
  right thing?  Do we need to include the headers before each group of
  functions?  Do we need to keep translation units independent (but the lose the
  benefit of static funs)?  This is almost certainly a real problem that needs
  to be addressed.
* Ensure that all pointer parameters are allocated one extra element at end so
  that we can use the fun(++pointer) pattern without worrying about the last
  call overflowing (I think this might be allowed by the standard anyway,
  check).
* Ensure all entry point names are unique and that there are no collisions.
  Maybe this can be done on registration?
* Special case where all groups are same size and there is an external vector of
  that size?
* Complex expressions with curly braces, etc.
* Unary arithmetic funs.
* Add post-processing of function result (e.g. to attach names, etc, as in
  `quantile`).
* Support for functions with defaults that need to be evaluated?  No.  This
  substantially increases complexity because we have to do so in the function
  evaluation environment and need access to all the other arguments.
* Add capability to specify functions in non-standard ways (e.g. `base::sum`, or
  `(get("sum", as.environment("package:base"))`)?  Maybe not, that seems like
  potential trouble.  Certainly document also things like `f <- sum; f(x)`.
* Evaluation of non-compilable expressions in an env child of an env with
  appropriate symbols protected?  Such would be the symbols in the data, but
  what about the functions that are used?  That seems excessive.
* Make sure there are interrupts.  Can we use "R_ext/Intermacros.h".  It seems
  yes generally, but they are not explicitly part of the API, and then there is
  the question of whether it makes sense to do so, or if we should just be doing
  this at the group level?
* Think though corner case of R_NA, NaN, Inf, etc: are we preserving semantics.
* Is it faster to compute complex expressions pairwise (i.e. full vector scan of
  two vectors), or do each row in full once?
* Look into using R_GetCCallable (see WRE) instead of what we're doing now.  It
  seems though that to do so we'll need to generate a full package which will
  further slow down the compilation.
* Check name isn't used already.
* `?SHLIB` states some binary distributions don't include `R CMD SHLIB`.
* Figure out how to call the current version of R (e.g. `RD CMD SHLIB`).
* See how far back this works in terms of R versions.
* What happens when this stuff is serialized?
* What does inline do?
* Is there an alternative to R_FindSymbol?  Can we get it to be made part of the
  API.
* Recycling warnings?
* Can we use this in some way for e.g. really wide matrices where each column
  takes the place of an argument?  Generating a pointer for each column then may
  be less efficient.
* Does altrep play into this in any way?
* Lags and similar (i.e. `x[i] = x[i] * x[i - 1]`).
* Re-use repeated expressions.

## Done

* Really need to figure out whether we want a formal interface to the functions.
  It could be auto-generated from all the unbound symbols like gsubfn does it.
* Annotate code with the call that it corresponds to

# Integer variables

We wish to convert integer variables to real, but ideally we would return them
as integer if possible (or NA if overflow) at the end of the computation.  The
first part is relatively easy: if an object is integer, convert it to real.
What if the object is used both as a control and non-control parameter?  I
guess, too bad, it will be real.

If all non-integer

Functions will need to track whether they preserve integerness and under what
conditions.  Presumably it should be if all non-control, non-flag parameters are
integer, then the result is integer.

# Loop implementation

Loop variables can be created in a child frame to the data.

Each loop body will generate a `run` statement, e.g.:

```
run(...) {

}
run_loop_1(...) {
  for(intmax_t i = 0; i < len; ++i) {
    run(...);
  }
}
run_loop_0(...) {
  for(intmax_t i = 0; i < len; ++i) {
    statement_0(...);
    statement_1(...);
    run_loop_1(...);
  }
}
```

Probably want two types of loops, loops where we know the start and end as
constants or other derivable data (`seq_along`), and others where we have an
unknown vector?  Might not be able to allow the second type, because at that
point we can't know how large a vector that is assigned to with e.g. `x[i] <- y`
will get.

```
for(i in seq_along(x))
  y[i] <- x[i] + 1
```

Is the latter:

```
for(i in x)
  y[i] <- z[i] + 1
```

At the allocation stage we need to determine the size of `y`.  It's fine if
we're just referring to an external symbol (or even a group symbol) as we can
check the content sizes.  But things like:

```
for(i in rev(x))
```

Get messier.  Likely we just allow symbols, or `seq_along`, or constant
expressions like `a:b` where `a` and `b` are known at allocation time (changing
them inside loop is fine since):

> The seq in a for loop is evaluated at the start of the loop; changing it
> subsequently does not affect the loop.

But they can't be computed within the `r2c` code.

# Benchmarks

* 1e7 with group size 10, 1000.
* Pre-sorted
* sum(x), sum(x + y)


# Done / Answered

* Make sure that no non-numeric data exists other than the stuff going into
  control.

Yes, we only append the numeric data.

# Other Implementations

## Graal

Main question here is how limiting the R-C boundary is.  Is it the case that
code such as `data.table` is not any faster?  Does that even run?  How many of
the packages actually work?  What stuff does not work?  Seems like there should
be a bit of stuff that doesn't per TK.

## Renjin

## Odin

Fascinating implementation, but the key issue seems to be that it seems to be
very much focused on the derivative syntax of DeSolve and thus abandons R
semantics.

Supports things such as (from array portion of vignette):

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ]))

But not:

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ] + 3))

I.e. it won't recursively construct the expressions.  But it can be done with:

    tmp[,] <- ay[i,j] ^ 2
    deriv(y[]) <- y[i] * (1 - sum(tmp))
    ...
    dim(tmp) <- c(n_spp, 10)   # probably can submit the 10 as user()?

Additionally, the concept of arrays is really a stand in for multiple variables.
In the above example `i %in% 1:4` so it's a four variable system and could be
written as such, not really the concept of vectorized data intended to be
aggregated.

We can get "vectors" in the sense we're used to in there, but they need to go in
as matrices (but that's okay, we can just do this with 1D matrices).

# Concept

1. Given a parsed R expression
2. Check that expression / function has symbols that resolve only to known
   functions.
3. Translate it into C code.
4. Compile it.
5. Feed it.

# What needs to happen

1. A compiled function that accepts N double pointers.
2. A manager to call the compiled function repeatedly with variations.

# Features

1. Standard arithmetic.
2. Sum. Mean.
3. Assignment?  Probably not.

# Translation

## Memory

* Calling function will allocate a vector sufficiently large to hold the
  largest result and pass that to each computing function.
* Computing functions may only write to that vector?  Specifically, computing
  functions may not allocate?

## Length Computation

Each known function should have some method of ex-ante determining the length of
the outputs as a function of the lengths of the inputs (e.g. `quantile` would be
the length of the `probs` argument).

## Ex Ante Size Computation

We need some view of the size pre-compilation, but by virtue of allowing
references to external objects we can't resolve them fully.  So we have options:

* Group size
* Constant size (knowable at compile time)
    * Scalar (e.g. result of `mean(x)`)
    * Other constant (e.g. result of `range`)
* External, could be anything (zero, scalar, group size, whatever)

At compilation time we may be able to emit better code if we know if the inputs
are scalar / group size, vs if we don't we'll need a per-group conditional to
decided.  So the code generation needs to be able to run with the partial
information.

So we need a pre-processing pass that does all the size computations, but not
the actual allocations, and then a second pass once the input sizes are known.

How do we handle something like:

    pmax(scalar, external, group, constant)

For "vecrec" probably need to resolve to the worst in order:

    scalar > constant > group > external > external_or_group

But we must always keep track of whether group is involved.

One big issue is we don't know until the evaluation stage what columns are going
to be group vs external because we haven't seen the data.frame.  So we can
compile highly efficient code if we know the situation, but less so otherwise.

Similarly, if we want to pick code based on the value of control parameters, we
need to evaluate them at compile time.  This severely cramps our style.  The
control param evaluation at compile time is probably okay.

To resolve the knowing the data columns, we have two choices:

* Force all data columns to be the same size (what about non-data params like
  `probs`)?
* Make the code capable of handling different lengths inputs.

For the latter, we could have the code react to each group, but it would be much
better if it could before running pick the correct one.  Probably not possible
without recompilation.

Some costs of not being able to do the full optimization:

    > n <- 1e7
    > x <- runif(n)
    > g <- sample(n)
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 0L))
       user  system elapsed 
      1.077   0.005   1.087 
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 1L))
       user  system elapsed 
      1.076   0.004   1.085 

These above are with `na.rm` as an explicit if/else, where group size is 1..

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed 
      1.030   0.004   1.040 

Here instead we assume `na.rm = FALSE` so no extra if/else.  For reference this
is what the above looks like with group size = 10:

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed 
      0.778   0.004   0.788 

And finally back to group size = 1 we now add a `asInteger(VECTOR_ELT(x, 0))` to
retrieve the `na.rm` flag:

    > sys.time(res <- run_group1(shlib, "fun5", list(x), g, 0L))
       user  system elapsed 
      1.147   0.002   1.149 

So a 10% penalty, which suggests some value in allowing for a single flag to be
passed down.  But maybe this becomes a future feature.

For arithmetic, there is seems to be very little value in having the simplified
logic option (in fact, it looks slower, `fun7` is the one that goes straight to
the equal length loop).  So no huge cost.  At least when there is no contention
with any other processes.  Given that sorting is such a big part of the cost,
it's going to be hard to beat that.

    > sys.time(res1 <- run_group2(shlib, "fun6", list(x, y), g))
       user  system elapsed 
      1.058   0.007   1.071 
    > sys.time(res2 <- run_group2(shlib, "fun7", list(x, y), g))
       user  system elapsed 
      1.069   0.008   1.084 
    > sys.time({
    +   o <- order(g)
    +   xo <- x[o]
    +   yo <- y[o]
    + })
       user  system elapsed 
      0.783   0.005   0.794 
    > sys.time(res3 <- xo + yo)
       user  system elapsed 
      0.022   0.000   0.022 
    > all.equal(res1, res2)
    [1] TRUE
    > all.equal(res1, res3)
    [1] TRUE
    > 

## Allocation

Once we get to the run stage, we'll have for each sub-call, the type size of the
result, and the inputs, and the type of function.  This means we can resolve the
size requirements to a specific number for external things, and for external or
group we now know the "external" part of it as well.  Since we'll know the max
group size, we can run through?

To do this properly we need to know which column each thing refers to, but we
should be able to do it linearly.

So start going column-wise through our size matrix.  First item should be
standalone (i.e. not a call with arguments).  Add it to the allocation (unless
it's data, in which case it is already there), evaluating first if needed, and
recording the position in the data as well as the size (possibly NA if group).
Keep accumulating this info in a stack until we get to a call, which will then
reduce it down to a single value, and proceed accordingly.

For each item, we need to know what it is:

* Control (evaluate, store, and record index)
* Symbol external (evaluate, store, record size and index)
* Symbol data (record index)
* Expression (recurse, record size and index)

## Simple arithmetic

Compute length of each vector.  Generate the full C expression with index or
pointer offset access.

    x <- runif(5e6)
    y <- runif(5e6)
    z <- runif(5e6)
    w <- runif(5e6)
    >
    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed 
      0.025   0.000   0.025 
    > sys.time(fapply:::test2(x, y, z, w))
       user  system elapsed 
      0.018   0.000   0.018 

It is slightly faster to do x + y + z + w (test2) than do pairwise additions
across the full vectors (test1).  Maybe because we only increment the loop
variable once?

With `MOD_ITERATE_CHECK2` and checking every 1e6:

    > sys.time(fapply:::test3(x, y, z, w))
     user  system elapsed 
    0.036   0.000   0.038 

Interestingly `MOD_ITERATE2` is no faster, so the check is basically free.

`R_ITERATE_CHECK` is comparable to pairwise addition (test1):

    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed 
      0.026   0.000   0.028 
    > sys.time(fapply:::test5(x, y, z, w))
       user  system elapsed 
      0.026   0.000   0.028 


## Function Types

* Aggregation functions (result length == 1)
* Vectorized functions  (result length == 0 or longest vector)
* Arbitrary functions   (result length is known (above is degen case of this))

Maybe use iterator macros.  E.g. `MOD_ITERATE2_CHECK`.  These should work fine
and are fairly efficient (i.e. reset counter to zero intead of reading with
modulo).

When construction the expression, when do we need to recursively evaluate
sub-expressions?  E.g. in:

    x + y + z

Which is really:

   `+`(`+`(x, y), z)

How do we know we can just turn it into:

    for(i = 0; i < len; ++i) x[i] + y[i] + z[i];

Vs

    x + y + mean(z)
    x + y + sort(z)

It's really that there is no mixing moving of the vectors.  Associativity /
commutativity shouldn't matter as that's handled by C / parentheses.

If we have a complex expression with a lot of these then it potentially gets
tricky b/c we start requiring intermediate storage for computing the whole
expression.

So we need to distinguish between "vectorizable" functions that operate one
element at a time, and those that aren't that operate on more than one element
at a time (either by moving them, etc).  The latter have to be evaluated
separately.  So the "parser" will need to identify where such functions are and
then evaluate/reduce them to a form that can then be used in a vectorized
context.

Because of this we might just favor the iterative resolving of the expression.
This will be easier.  We could just have logic to try to make the biggest
vectorized expression.  This would avoid a lot of extraneous code.

So right now:

    x + y + z

Becomes

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];
    for(i = 0; i < n; ++i) res[i] = res[i] + z[i];

And:

    x + y + mean(z)

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];

    double tmp = 0;
    for(i = 0; i < n; ++i) tmp += z[i];
    tmp /= n;

    for(i = 0; i < n; ++i) res[i] += tmp;

So we'll need as many temporary scalars / vectors as there are arguments to
functions.  Maybe we don't allow anything but binary?  Hmm, so with:

    pmax(-x, log(y), mean(z), w - u)

In all cases we'll know the size each argument.  In the expression above we know
how many parameters there are so we can construct the correct code at
"compilation" time.  What about:

    pmax(pmin(y, z, w), pmin(w, q, f), mean(z + y))

Generated code:

    double * tmp0 = R_alloc();
    for(i = 0; i < n; ++i) {
      tmp[i] = z[i] + y[i];
    }
    double tmp1 = mean(z + y);

    for(i = 0; i < n; ++i) {
      tmp0 = pmin(y[i], z[i], w[i]);
      tmp2 = pmin(w[i], q[i], f[i]);
      res[i] = pmax(tmp2, tmp3, tmp1);
    }

And (numbers show depth):

    pmax(pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y))
         pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y)
                    mean(w)                  mean(z + y)
                                                  z + y


Generated code.  Coords are (depth, param)

    // coords (0,0),(1,0)(2,2)
    double stmp0;
    stmp0 = mean(w);

    // coords (0,0),(1,0)
    double * vtmp0 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(y[i], z[i], stmp0);

    // coords (0,0),(2,0)
    // * stmp0 no longer needed
    double * vtmp1 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(w[i], q[i], f[i]);

    // coords (0,0),(2,2),(3,0)
    double * vtmp2 = R_alloc();
    for(i = 0; i < n; ++i) vtmp2[i] = z[i] + w[i];

    // coords (0,0),(2,2),(2,0)
    stmp0 = mean(vtmp2);

    // coords (0,0)
    // * vtmp2 no longer needed, so re-use
    double * vtmp3 = R_alloc();
    for(i = 0; i < n; +++i) vtmp2[i] = pmax(vtmp0[i], stmp, vtmp1[i]);

We need to track how many variables we've generated and when each one is freed,
without garbage collection.  So for each variable we need to track the depth at
which they are used.

Ideally we'll know for each group exactly how many variables we'll need and of
which size.  So we want to allocate the minimal amount w/ re-use that we need.
Number of variables should be at worst `sum(pmax(length(args) - 1)` or some
such.

One problem is that computing ex-ante how big each of our intermediate
allocations is going to be could be costly.  For `pmax`, we have the group based
params that are known sizes, but there could be external params of unknown
sizes.  So we could reduce the size expectation to be something like
`max(c(G, a, b, c, ...))`.

Provided options:

* Length of group
* Constant length (mean/sum/prod->1; Max returns -Inf)
* Length of one specific argument (quantile)
* Recycled max of multiple arguments (different from plain max because if one of
  the arguments is known to be zero, then the lot is zero).

Not provided options:

* Function of length of single argument.
* Function of lengths of various arguments.

This means we only care about the maximum group size as we'll always want to
allocate to that group size at a minimum.

So we navigate the entire parse tree, and for each function we retrieve the type
from above.  This will require:

* Reducing the function to its numeric arguments.
* Looking up the type of function.
* Computing the maximum return size as a function of form `max(c(G, k))` where
  `k` is a constant presumably derived from external variables, or possibly 0.

For a single function:

* Confirm it is a known function by getting it and comparing it to our list.
* `match.call`.
* Identify all non-group parameters.
* Evaluate all non-group parameters and bind them to local symbols.
* Check that all the C parameters are double (either a group param, or double).
* Depending on function type
    * (if needed) Compute lengths of all non-group C parameters, and record max
      of them or zero in `K`.
    * Record shorthand size, e.g. `K`, `g`, or `max0(g, K)`, where `max0`
      returns 0 if any of `g`, `k`, are zero, and `K` is a scalar constant.
    * We thus need a structure that contains a constant, whether the constant is
      set (we need to distinguish zero), and whether the group size matters.

Across the full expression, given `g` is equal to max group size:

0. Initialize vector of temporary items.  This is an array of R_xlen_t sizes
   sorted by size and an array of integers denoting whether the corresponding
   entries are free or busy.
1. Confirm function is valid.
    * Either resolves against list.
    * Or does not contain any references against symbols in the grouped data.
2. Based on function type:
    * Invalid: STOP.
    * Valid no symbols: eval and return length as constant (SIDE EFFECTS?).
    * Valid referencing symbols: continue to 3.
3. Count function parameters that are of unknown length.
4. For each unknown parameter, recurse to 1.
5. Once all parameters are of known length, compute expression length.
6. Scan through free entries to see if any are big enough.
    * For entries with group size, use the biggest group size.
    * An entry is free if its depth is greater than one below the current level.
    * If yes, mark as busy with the current depth.
    * If no, find the spot to add a new entry and mark with current depth.
    * Process of adding an entry should preserve sorting of list.
5. Return expression length computed in 5.

Now that we have the expression length, compute the result vector size.  It
should be either a constant per group, or `g`.

The top level return should give us the result size, and our list should give us
the set of intermediate vectors we'll need.

## Parameter Types

There are:

* Numeric standard vectors from data
* Numeric standard vectors external
* Non-numeric data (do we even allow this?  How do offset work)
* Non-numeric externals

For non-numeric data, an example would be if someone wants to run e.g. a nested
tapply on each group (this is silly, should just group on the interaction, but
whatever).

Some of these will be "control" parameters, which loosely speaking are those
that are used in the code generation phase, and aren't necessarily (but can be)
passed on to the compiled code.

Key question right now is what gets into the `alloc.dat` structure.  Presumably
it is anything that could possibly feed a C parameter that it intends to compute
on, and more specifically, one that could be fed by the result of a previous
calculation.  Our key restriction is that the functions must all return doubles
no matter what.

Or maybe it is any parameter that in theory could be fed by group data.  Which
in theory is any numeric parameter that is not explicitly made to be a control
parameter?  Yes, this seems to be the most manageable approach.  This also
implies that all non-control parameters must be numeric.

Do we allow non-numeric, non-control parameters?  Is there any use for them
other than grouping?  Maybe something designating a type of calculation to be
done?  But when do we ever want those to be the same length as the vector?

## Interface

### R Level

Each special function must be pre-registered with some mechanism to distinguish
which parameters are vector ones that will be fed, vs which one should be
evaluated immediately for dispatch decisions (e.g. `na.rm=TRUE`).

How does this work for a newly defined function at the R level?  Does it matter?

### C Level

#### Basic

Target function:

* Array of double pointers the same length as it expects number of arguments.
* Int with number of arguments.
* Array of R_xlen_t with lengths of each of the arguments.
* Array of R_xlen_t with offsets of each of the arguments.
* Double array of right size for result.

Should we have some structs pre-defined to hold the above?

Can the function check it's doing the right thing in terms of argument count?
Would it be better to have the arguments provided explicitly (in which case we
really want structs?  Should the "Array of double pointers" already be at the
expected offsets (maybe).  Do we want iterators like R provides to walk through
all the arguments?

Maybe we don't want the lengths of each of the arguments; instead, we want the
length of the result, and the modulo to read all the other arguments not of that
length?

should accept an expects arguments.  All functions should be like that.

Parameters such as `na.rm=TRUE` actually cause dispatch to a different
function.  So when we register a function we need to declare this?  I guess it
should be optional and we use it to recreate `mean`, etc.

#### Allow SEXP

What about functions that accept non-double parameters?  In particular, we'll
have expressions that reference external/evaluated things, which might not be
double.  If we want to allow these to be fed to our functions, our functions
need to be able to accept SEXPs.  This substantially complicates the interface;
are there cases where we would want to do this?

An almost example is `quantile`, where the `probs` param is likely fetched from
elsewhere.  Or `filter`.  Essentially anything where the configure param
resolves to an arbitrary number of possibilities and thus can't be reduced to
different entry points.  `findInterval` might be a better example.

Seems like we have two choices:

* Allow additional SEXP arguments (and identification of what these are).
* Add ALTREP so that all arguments can be SEXP.

The latter seems cleaner.  What happens with the result vector at that point?
Probably can't be ALTREP as we'll need to modify it.

How much overhead does ALTREP add?  There is a good example from Gabe about
implementing window functions.

So is this worth doing?  What is the advantage of doing so.  More familiarity
for the implementer of new functions?  Maybe for now we just pass through as a
`moreArgs` list.  So we end up with `double **` for the data arguments, along
with offset and length, which could include some vectors that are not-data, and
a list of extra arguments.  Does this obviate the need for a configure function?
Probably, it then happens in C.  The sad thing about this is we have to redo all
the configure business for each call.  Hmm.

Related, could this eventually be made to use the actual R functions?  The
summary functions are all in "src/main/summary.c" and those are simple, but the
problem is they are all static and dispatched to by `do_summary`.  And calling
the latter will be very challenging from outside.

# Pre-Processing

Once we have the expression parsed, do we want to link in pre-compiled entry
points, or do we just want to generate the whole thing in one piece of generated
code?  Probably start with one piece of generated code.

    sum(((x - mean(x)) * (y - mean(y, na.rm=TRUE))) + z)

What does this become?  Each sub-expression needs to be generated in linearized
code.  There will be a vector of allocated vectors for temporary use.  So we
need a list where each element is a step, and each step should include:

> We probably want a version for variable argumens (e.g. `pmin`, and one for
> fixed argument count).

* That data structure, which is an array of double pointers with:
  * Group data at the beginning.
  * Non-group double data next.
  * Temporary arrays.
  * Result array.
* An integer count of parameters into the data structure.
* An array of offsets into the data structure.
* An array of offsets into each vector in the data structure.
* An array of lengths of each vector in the data structure.
* A `VECSCXP` of all the control parameters.

So in C our earlier expression will look like (this does not include `ctrl`):

Variable arg count:

    static void %s(
      double ** data, int narg, int * datai,
      R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Fixed arg count:

    static void %s(
      double ** data, R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Maybe an option without control?  So two switches:

* `vararg`
* include `ctrl`

Additionally, if we allow different signatures then they need different names.
To the extent we re-use a single function multiple times, we want to detect
this.

Each code gen call should return:

* Function definition.
* Function name.
* Function call (since we can have different # of arguments).

We can check that if multiple instances of a function name show up, the
definitions are the same, and then unique them out.


# Compilation

## Cost

For a super simple compile:

```
$ time clang -g -c -O2 test.c -o test.o

real	0m0.115s
user	0m0.065s
sys	0m0.045s
```

But first time calling clang can take a long time (e.g. 6 seconds).  One
question whether adding R stuff slows down the compilation.  Doing it with `R
CMD SHLIB` but then obviously gives us access to a lot more options:

```
time R CMD SHLIB test-r.c
clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -fno-common -std=c99 -pedantic -Wall -Wextra -c test-r.c -o test-r.o
clang -mmacosx-version-min=10.13 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o test-r.so test-r.o -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation

real	0m0.361s
user	0m0.235s
sys	0m0.112s
```

Also, nice thing about R CMD SHLIB is that it abstracts away calling the
compiler (on windows)?  Oddly, calling the plain c file is slower!  Well, at
least not faster, so the overhead is from firing up make, etc.

> Note It's not actually slower as we need the extra step to generate the .so.

## Loading

### Hack approach

Ideally we would be able to use `dyn.load`, but then we need `.Call` which
obviously we're trying to avoid.  This is going to be really complicated, see
"src/main/dodotcode.c".

So we can use `R CMD SHLIB` + `dyn.load`, but we now need to figure out how
`.Call` finds the symbol in that case.

    > xx <- dyn.load('test-r.so')
    > xx
    DLL name: test-r
    Filename: /Volumes/PERSONAL/repos/fapply/test-r.so
    Dynamic lookup: TRUE
    > .Call('test', runif(10), PACKAGE='test-r')
    [1] 6.270841

Looks like maybe we can use (cadged from `resolveNativeRoutine`):

    DL_FUNC *fun;
    char buf[MaxSymbolBytes];
    R_RegisteredNativeSymbol *symbol;
    R_RegisteredNativeSymbol symbol = {R_CALL_SYM, {NULL}, NULL};
    *fun = R_FindSymbol(buf, dll.DLLname, symbol);

**Bad News**: we need `Rf_RegisteredNativeSymbol` which is in Rdynpriv.h, which
is not installed, etc.  We can just dummy it since it is only used optionally.

Which is used in (where `*fun` above is loaded into `&ofun` below):

    args = resolveNativeRoutine(args, &ofun, &symbol, buf, NULL, NULL, call, env);
    retval = R_doDotCall(ofun, nargs, cargs, call);

All of these seem to be in "R_Ext/Rdynload.h" so I think we're okay (though not
strictly part of the API?).

### `R_GetCCallable`

An alternative we can use, but will likely require building a package like odin
does.

Is there a way to save the function and the compiled code other than via
package?  Maybe?


# vs Rcpp?

How is this different to Rcpp?  Very limited, but no learning curve.  Only
doubles.

# Interface

Now leaning towards something like this:

```
group_exec(slope, data, groups, x=v)              ## remap
group_exec_(slope, data, groups, remap=c(x='v'))  ## remap
```

But what if the `r2c` object contains `data`, or `groups`?  Then it becomes
impossible to remap those.  We could make those reserved terms in the
compilation analysis.  How does that evolve as we add more functions that have
different parameter names?  Do we just lock up too much?  The alternative seems
to be:

```
group_exec(r2c_slope, data, groups)()
group(r2c_slope, data, groups)()
slide(r2c_slope, data, width, ...)()
r2c_slope()
```

Do we require parameters?  Yes.

```
group(r2c_slope, data, groups)(x, y)
r2c_slope(x, y)
slide(r2c_slope, data, window=5)(x, y)
```

Setting the following as default just won't work, since the lookup semantics are
to look in the function environment (and would cause the recursive lookup
issue):

```
function(x=x, y=y)
```

So we can't have a function that observes R semantics but then also has the
unmapped parameters. So things like this won't work:

```
group(r2c_slope, data, groups)()
```

But this is okay:

```
group(r2c_slope, data, groups)(x, y)
```

Since we don't have a way to declare the mapping order in a way visible to
users.  So even if the print method does something like:

```
group(r2c_slope, data, groups)
## function(x, y) <...>
```

When in reality it's:

```
## function(...) <...>
```

What does `r2c_slope` look like:

```
r2c_slope
## function(x, y) <...>
```

Hmm, but are we satisfied with the data/groups specification?

```
group(r2c_slope, data, data['g'])(x, y)
with(data, group(r2c_slope, x, y, g))
```

How about this:

```
group(obj, ..., group)

groupwise(obj, group, ...)
groupwise(obj, data, .(group), .(remap))
```

If this is the NSE interface:

```
groupwise(data, group, obj(x=x, y=y))
obj(x=x, y=y)

with(data, groupwise(obj, list(x, y), list(g)))

with(data, r2c_group(obj, list(x, y), list(g)))

r2c_group(obj, x, g)

set.seed(1)
x <- y <- runif(1e5)
y[sample(1e5, 1e4)] <- NA
microbenchmark::microbenchmark(sum(x), sum(y), mean(x), mean(y), times=10)

set.seed(1)
x <- y <- runif(1e5)
y[sample(1e5, 1e4)] <- NA
microbenchmark::microbenchmark(sum(x), sum(y), mean(x), mean(y), times=10)
## Unit: microseconds
##     expr   min    lq  mean median    uq   max neval
##   sum(x)   299   315   342    335   367   413    10
##   sum(y) 18441 18684 19587  18973 20461 22382    10
##  mean(x)   234   265   281    276   294   343    10
##  mean(y) 33009 33201 34449  33421 34584 38751    10

```

So we need:

* data
* groups
* r2c object
* remap


```
groupwise(data, group, obj, x=x, y=y)

groupwise(obj, data, group, x=x, y=y)
groupwiseq(obj, data, group, x=x, y=y)

run(obj, x, y)
```

This allows arbitrary objects in ..., and we can require that they are named.
Group can be a single vector, does not need to be named, or it can be a list.

```
obj(x, y)
objg <- with(data, group(obj, x, y, g))
objg <- with(data, group(obj, x=x, y=y, g))
objg(x, y)
```

If unnamed we can rely on the symbol.



These are very appealing for group/window because we can easily re-use the same
shared object, and additionally 

```
obj <- r2c(<expr>)
obj$run(data)                # intended for code with loops.
obj$group(data, groups)
obj$slide(data, size, ...)
```

Are `groups` quoted or as is?  Do we need:

```
obj$groupq(data, groups)
```

What if we want to remap args?

```
obj$group(data, groups)(x=z, y=v)
obj $
  map(x=z, y=v) $
  group(data, groups)

group(remap(obj, x=x, y=y), data, groups)
```

Or should we really do:

```
slope <- r2c(
  function(x, y) (x - mean(x)) * (y - mean(y)) / (x - mean(x))^2
)
group(slope(x=v, y=z), data, groups)

with(data, group(slope(v, z), groups))
```




```
group(slope(x=v, y=z), data, groups)
group(slope, data, groups)                               # no remap
group_exec(slope, data, groups)                          # no remap
group_exec(slope(.remap=c(x="v", y="w")), data, groups)
group_exec(remap(slope, c(x="v", y="w")), data, groups)

group_exec(slope, data, groups)       ## no remap
group_exec(slope, data, groups, x=v)  ## remap


run(slope(x=v, y=z))
slide(slope(x=v, y=z), window=window)

slope(x=v, y=z) |> group(data, groups)
```

What would this do?  Run normally?

```
slope(x=v, y=z)
slope$group(data, groups)(x=v, y=z)
```

Then? Do we need:

```
group(slope, data, groups)(x=v, y=z)
group(slope(x=v, y=z), data, groups)     # Possible with NSE, but not ideal.
group(slope, data, groups)               # would this run without remapping?
group(slope, data, groups)()             # Or should it be this?

slope |> group(data, groups)(x, y)

slope$group(data, groups)(x, y)

group(slope, data, groups, map=.(x=v, y=z))
```

Is this confusing?  Yes, the duality of `slope` being a function but also not is
just weird.

What's the benefit of defining as a function?  Makes it easier to re-map the
variables in use, but then the remapping is weird.

```
slope$group(remap(data, x=v, y=z), groups)
remap(slope, x=v, y=z)
```


Old stuff:

```
fapply(X, INDEX, FUN)

group_exe(data, groups, obj)
window_exe(data, size, just, obj)
for_exe(data, obj)
```
