# Interrupt

See bench-interrupt for timings and related issues.

Ideally we would track how many iterations we have at some inner loop level, but
be able to keep that number alive across function iterations.  So we want a
looping construct that has access to how many iterations have occurred to this
point, and how many iterations we want per break.

Most annoying scenario, e.g.:

* We've already done 1 iteration.
* We want to break every 100.
* Our current length is 201.

So we need one loop to finish out the current iteration?  No, we can just have a
loop that starts.

    i = i_prev = 0;
    i_interrupt;    // interrupt counter
    interrupt_at;   // when to interrupt

    do {
      i_stop = i + interrupt_at;
      if(i_stop > i_max) i_stop = i_max;

      for(; i < i_stop; ++i) ...

      i_interrupt += i_stop - i_prev;
      if(i_interrupt >= interrupt_at) {
        CheckInterrupt();
        i_interrupt = 0;
      }
      if(i == i_max) break;
      i_prev = i;
    } while(1)

Gah, it's really annoying to have to do the loop interrupt at the iteration
level because the window code is a fair bit more complicated.  We need to
compute for each window how many elements there are in the window, which is
`iright - ileft + 1` or some such (empty windows)?

# Code Generation

Function needs to:

* Generate a signature.  Currently we auto-generate the call, but we then
  mash-in the signature with the body of the code.  We should be able to separte
  the two.
* We should be able to tell whether we need to generate control or flag
  parameters from the lengths of the `args.ctrl`, `args.flag` parameters.

Eh, this is a bit of a mess, so we're going to defer.

# Big Unitizers

Possibly part of the problem is the capture of the package env.

Oddly the uncompressed size is pretty similar.

Hmm, for summary it's not clear that's what's going on.  One difference is from:

    > rds_size(body(b@items.ref[[1]]@env[['r2c_sum_n']])[[2]][[3]][['envir']][['r2c_sum_na']])
    [1] 16778
    > rds_size(body(a@items.ref[[1]]@env[['r2c_sum_n']])[[2]][[3]][['envir']])
    [1] 43

In the former all the functions defined in the environment like 'r2c_sum_na' are
there.  In the latter they are not.  This is clearly related to the new capture
of the lexical environment, which we did not use to do.

So something about recording the lexical environment.  And it gets recorded in
all sorts of places, not just in the embedded objects.  For example:

    try <- tryCatch(.DAT0 <- as.list(environment(), all.names = TRUE),
        error = function(e) e)

Captures the environment as a function to the anonymous error function.

So there is a weird thing in the above case because we have several `r2c_sum_x`
functions defined in the one one environment, we end up saving them twice, once
as themselves, and once as a copy of the environment that they were created in.

The summary test creates a bunch of functions, so it's not surprising that it
ends up being much larger then as it doubles the storage necessary for all the
r2c functions.

Is there a way to work around this?  Does the serializer have special logic to
avoid this problem? R seems smart enough to avoid this problem with closures.
It's not obvious though how, thus how we can prevent this issue.  Hmm, no, it
doesn't seem so.  Best thing we could do is create the functions someplace they
are not stored?

Ugh, but moving big stuff out to `_pre` now causes all the big stuff to be
recorded.

And even if we fix the package env issue it's not clear that we'll resolve this
other issue.  We essentially need to declare all the functions up front, and
then all the data in a child environment, and then have everything that follows
be a child of that.  And even then all the functions are going to capture each
other in the environment.

# Package Env

Something somewhere is causing the package environment to be captured...

    = Finalize Unitizer ========================================================

    | You will IRREVERSIBLY modify 'unitizer/mechanics.unitizer' by:
    | - Adding 1 out of 1 new tests

    | Update unitizer ([Y]es, [N]o, [P]rev, [B]rowse, [R]erun)?

    unitizer> Y

    | unitizer updated.

    Warning message:
    In saveRDS(unitizer, paste0(store.id, "/data.rds"), version = 2) :
      'package:r2c' may not be available when loading

We reduced it to one file with `r2c_sum("hello")` but with `r2c_sum` defined
in the "pre" file.  For some reason that causes the package environment to be
captured.

It seems very likely it is because of:

    > x@items.ref@.items[[1]]@trace
    [[1]]
    r2c_sum("hello")

    [[2]]
    eval(bquote(group_exec_int(<environment>, formals = .(.FRM),
        groups = NULL, data = .(.DGRP), MoreArgs = .(.DAT[-1L]),
        call = quote(.(.CALL)))), envir = getNamespace("r2c"))

Examining that object doesn't show the package environment, but the presence of
trace would explain why this only happens when we have an error case.  It's not
clear why it happens only though when the function is defined in "pre".  We must
be doing some env surgery on the environments of the actual unitizers that
doesn't happen for "pre" ones.  From `?unitize`:

>  Unlike objects created during test evaluation, any objects created here will
>  not be stored in the unitizer so you will have not direct way to check
>  whether these objects changed across unitizer runs.

The problem child is the "envir" slot of the environment, not the environment
itself:

    unitizer:::env_ancestry(x@items.ref@.items[[1]]@trace[[4]][[2]][['envir']], emptyenv())
    ##  [1] "0x12b839b30"       "0x12b839af8"       "package:r2c"
    ##  [4] "package:stats"     "package:graphics"  "package:grDevices"
    ##  [7] "package:utils"     "package:datasets"  "package:methods"
    ## [10] "0x14bab6d10"       "base"              "R_EmptyEnv"

But oddly after the first step in store the parent env gets reset?  This is what
we see before and after we change the parent to zero.env:

    Browse[2]> unitizer:::env_ancestry(unitizer@items.ref[[1]]@trace[[4]][[2]][['envir']], emptyenv())
     [1] "0x11fc6b430"       "0x11a4bbfd0"       "0x11fee1c80"
     [4] "0x11fee1cb8"       "0x11a517da0"       "0x1281913c0"
     [7] "0x11fda32e0"       "package:r2c"       "package:unitizer"
    [10] "package:stats"     "package:graphics"  "package:grDevices"
    [13] "package:utils"     "package:datasets"  "package:methods"
    [16] "0x1292b4870"       "base"              "R_EmptyEnv"
    debug: parent.env(unitizer@zero.env) <- baseenv()
    Browse[2]> n
    Browse[2]> unitizer:::env_ancestry(unitizer@items.ref[[1]]@trace[[4]][[2]][['envir']], emptyenv())
    [1] "0x11fc6b430" "0x11a4bbfd0" "0x11fee1c80" "0x11fee1cb8" "base"
    [6] "R_EmptyEnv"

The problem sees to be that when that environment gets captured, the child env
of the package env is not separated from the chain.  Unfortunately if we do
things like this, we're going to retain the "pre" data for every single
unitizer.  So to do this properly, we would have to separately store the pre
data and integrate it into unitizer.

We also tested just using directly `r2c_sum` (i.e. the function object itself)
as a test and that caused the same error.  Generating the function in the test
file proper doesn't cause any issues at all.

Fixing this might be as simple as changing the parent environment of `par.env` /
`gpar.frame` (from `unitizer_core`) prior to storing, same as we do with the
zero env.  The challenge is that `par.env` depends on state, and might not be
the special unitizer environment.  So probably check whether the global
environment is in the ancestry (but need to be careful about having the correct
termination conditions) and if not, change it.

`par.env` comes from `global$par.env`.

    unitizer:::env_ancestry(unitizer@items.ref[[1]]@trace[[4]][[2]][['envir']], emptyenv())

The problem is actually that `trace[[4]][[2]][['envir']]` contains an `r2c_fun`,
and that itself captures a reference to the parent env.  So `store_unitizer`
needs to be given `par.env` to temporarily reset.  Again, should be fine so long
as `par.env` is really the special environment that latches on to the package
chain.  How do we check this?

But it must also be somewhere else?  We do reset the `@zero.env`, but the
problem is that the `par.env` is actually the grandparent (or great-grandparent)
of `zero.env`.

We fixed this just by resetting the `@global$par.env` (and fixing an
initialization of it that was creating duplicate ones).

# Alloc cleanup

## Rationalize Appending Data

`alloc_dat` is called only with a call, which means it has additional contextual
data such as the written-to symbol in the call.  It needs to:

* Cleanup expired symbols.
* Rebind overwritten symbols.
* Copy last action to result slot.
* Allocate or re-use a slot.
* Free depth-expired slots.

`append_dat` is called on "leaves", so it needs to be given names.  Will it ever
have names outside of the initial group data?  What about `MoreArgs` data?

Ideally we would modularize the code so that `alloc_dat` generates a new vector
to append, and then all the other pathways do something similar, and then they
all use `append_dat` to add the data to the allocation.

What doesn't work here is that `alloc_dat` might not append data, but instead
could re-use a slot.  In this case skip by returning NULL data?

So we'll need an additional branch for calls to:

* Free allocations.
* Reduce stack.
* Append call data.

Old Notes

1. Update Allocation (alloc_dat or append_dat)
2. If call, update call.dat (after updating allocation)
3. Update stack

Additionally the call branch will have reduced the stack.  Problems are:
* The use of alloc for call vs everything else
* The use of `id` instead of `alloc[['i']]` for name found in allocation
* That `call.dat` must be updated after `alloc` is.

Does `append_stack` really need to be told:
* `size` (yes, could be different to alloc size by being NA, but would be
  nice if it could infer it from `alloc`, can it?)

We should have a different `append_dat` for adding the group data vs a
single vector, should cleanup the interface.

## External Symbols

Currently these are added as many times as they are referenced. To avoid this we
would need a secondary names lookup table for external symbols.  By the
structure of the code it is subordinate to any assigned symbols.  Then, when we
go to eval a leaf expression, if it is a symbol, we can first look in our
external symbol table.  In fact, we could modify the `if` condition to check the
external symbol table in addition to the internal one, and fall back to the
`name_to_id` branch.  The only change would have to be for `data.used` tracking.

# Eval Env

We have an inconsistency with how evaluation environments are handled in runners
vs naked calls.  As we've settled on the interface to the `r2c` code to be
functions, we should respect that those define their own enclosing environment.

We've set this up so that the compilation stores the lexical environment, which
is then used stand-alone and runners.

In the case we store the function in a library, `saveRDS` appears to save the
entire environment chain, although presumably it will complain about those saved
in packages, which will be the biggest issue?  We can intercept the package envs
and instead provide a stub that maybe points to the empty env, to be restored
later.  Or we just rely on R to do the right thing, possibly capturing the
warning about such and such namespace may not be available.

Should we support that someone might want to change the lexical environment?  We
can't do that with `environment()<-` since the actual environment of the `r2c`
function is going to be the `r2c` namespace (currently it is albeit indirectly,
as we don't set the environment when we create the function inside `r2c_core`)?

Not clear why R isn't warning about package environment not available when
saving an "r2c_fun" (maybe it only warns about package environments?).

# Single Group Performance

Much slower than base, would be interesting to track down what's going on.  It's
particularly bad if we reverse the order, but it looks like we've got a whole
extra vector being allocated that maybe doesn't need to be.

    library(r2c)
    slope2 <- function(x, y) {
      mux <- mean(x)
      x_mux <- x - mux
      sum(x_mux * (y - mean(y))) / sum(x_mux^2)
    }
    r2c_slope2 <- r2cf(slope2)
    n <- 1e8
    a <- runif(n)
    b <- runif(n) * .5 + a * .5
    gc()
    ###             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ### Ncells    362973   19.4     698777   37.4         NA    677183   36.2
    ### Vcells 200733947 1531.5  362973722 2769.3      16384 300734934 2294.5
    system.time(slope2(a, b))
    ##   user  system elapsed
    ##  0.834   0.192   1.026
    gc()
    ##            used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    363549   19.5     698777   37.4         NA    677183   36.2
    ## Vcells 200735202 1531.5  579650024 4422.4      16384 400737393 3057.4
    system.time(r2c_slope2(a, b))
    ##   user  system elapsed
    ##  0.807   0.210   1.069
    gc()
    ##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    373739   20.0     698777   37.4         NA    677183   36.2
    ## Vcells 200759255 1531.7  579650024 4422.4      16384 550877897 4202.9
    system.time(slope2(a, b))
    ##   user  system elapsed
    ##  0.867   0.228   1.094
    gc()
    ##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    432238   23.1     885577   47.3         NA    677183   36.2
    ## Vcells 200891389 1532.7  579650024 4422.4      16384 550877897 4202.9
    system.time(r2c_slope2(a, b))
    ##   user  system elapsed
    ##  0.864   0.326   1.265
    gc()
    ##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    434879   23.3     885577   47.3         NA    677183   36.2
    ## Vcells 200897934 1532.8  579650024 4422.4      16384 550990626 4203.8

Ok, some more testing, the problem is almost certainly that there we generate an
integer group vector that is the difference here.  Almost certainly the group
order vector that isn't altrep as noted.  That might be enough to explain the
difference in timing (note there is a lot of sensitivity to the order in which
things happen above given we make big memory requests from the OS).

# Linearized vs Not

One big advantage of the linearlized processing is that it lets us track lots of
relevant meta data in the same form.

One issue we're running into is that we lose some of the context.  For example,
in an assignment expression the first thing we hit in the linearized call is the
symbol to assign to, with no understanding that symbol is being assigned to
later.  So we'll try to resolve it and will fail because it does not exist yet.

For now the hack would be to leave the symbol unresolved?  Yes, but how do we
make sure that it does get resolved and allocated at the right time?  We can
mark it as an assignment symbol, along with the depth at which it should be
resolved, and once we get to the assignment call that matches that depth, we can
do all the update stuff.  I think if we just ignore it completely it's fine.  We
just need to record that it's an assign target.

# If / Loop

## Constraint Overview

The over riding constraint is that every symbol must resolve to an unambiguous
size.  `if` and `for` control structures can create ambiguity in sizes (e.g.
different sizes in branches of if, different sizes within the loop / loop can
act like a branch if not taken).

Disallow cases where a variable might not be created that is later used:

* for:
  * Zero iterations (maybe).
  * `break` / `continue`
* if:
  * Brand new symbol.
  * Not all branches contain it.
  * What if the symbol exists from elsewhere?

Additionally:

* for:
  * variables read and written from must be consistent size before and
    after loop
  * variables must remain constant size in the loop (e.g. cannot grow)
  * variables written to in loop must be either unchanged in size, or the loop
    must be guaranteed to have at least one iteration.
* if:
  * variables written to must be the same size across all branches; branches
    that don't write ok so long as prior var same size.

Need to think about what compatible allocations are.  Certainly different
branches must produce the same size result.  Does the result need to end up in
the same allocation id?

## Loop C Implementation

Probably the way a loop would work is:

```
void run(
  double ** data, R_xlen_t * lens, int ** di, int * narg, int * flag, SEXP ctrl
) {
  (void) narg; // unused
  (void) ctrl; // unused

  dibase = *di;
  flagbase= *flag;
  for(i = 0; i < n; ++i) {
    // - Loop Overhead ---------------------------------------------------------
    // Do something to update the value of `i` in the data
    ...

    // Then reset index, flags, more args
    di = dibase;
    flag = flagbase;

    // - Normal Code -----------------------------------------------------------
    // mean(x)
    mean(data, lens, *di++, *flag);
    ++flag;

    // x - mean(x)
    subtract(data, lens, *di++);
    ++flag;
  }
  // If there are no iterations at all, we need to skip ahead to what *di and
  // *flag would have been.
}
```

What about `if`? e.g.:

```
test <- mean(x)
if(test > .5) {
  y <- mean(x)
} else if (test < .25) {
  y <- sum(x)
} else {
  y <- sum(z)
}
```

```
void run(
  double ** data, R_xlen_t * lens, int ** di, int * narg, int * flag, SEXP ctrl
) {
  (void) narg; // unused
  (void) ctrl; // unused

  // mean(x)
  mean(data, lens, *di++, *flag);
  ++flag;

  // test <- mean(x) NO-OP
  // NO-OP: assign(data, lens, *di++);
  ++flag; ++di;

  // test > 0.5
  gt(data, lens, *di++);
  ++flag;

  r2c_if();
  r2c_elseif();
  r2c_else();

  if(r2c_if(data, lens, *di, *flag)) {
    ++flag; ++di;
    // mean(x)
    mean(data, lens, *di++, *flag);
    ++flag;
    // y <- mean(x) NO-OP
    // NO-OP: assign(data, lens, *di++);
    ++flag; ++di;
  } else {
    flag+=3; di+=3;
    if(r2c_if(data, lens, *di, *flag)) {
      ++flag; ++di;
      // sum(x)
      mean(data, lens, *di++, *flag);
      ++flag;
      // y <- sum(x) NO-OP
      // NO-OP: assign(data, lens, *di++);
      ++flag; ++di;
    } else {
      flag+=3; di+=3;
      if(r2c_if(data, lens, *di, *flag)) {
        ++flag; ++di;
        // sum(z)
        mean(data, lens, *di++, *flag);
        ++flag;
        // y <- sum(z) NO-OP
        // NO-OP: assign(data, lens, *di++);
        ++flag; ++di;
    }
  }
}
```

Every `if` must always emit an `else`, even if it is only to advance the
pointers for the next thing.

A bit tricky to think about where the result of the `if/else` should be stored.

How will code generation work?  Linearization would naturally:

```
// Input
if(X, A, if(Y, B, C)

// Becomes
X
A

Y
B
C

if(Y, B, C)

if(X, A, if(Y, B, C))
```

This is good, except that everything but the last statement might not run.  So
the code processor needs to add some special things?

```

__COND__
X
__YES__:1
A
__NO__:3
  __COND__
  Y
  __YES__:1
  B
  __NO__:1
  C

if(X, A, if(Y, B, C))
```

So maybe what we really need is the code linearization to produce:

```
X
list(
  A,
  list(
    Y,
    list(
      B,
      C
) ) )
```

Should `Y` be on a different level?  Probably.  This would signal the allocator
to fork itself at each level, and on exit of each fork evaluate the different
branches to ensure they have self consistent sets of live symbols.  For this to
work properly on exit need to reconcile all the allocations and make sure that
the same symbols point to the same allocations, and that they are the same size.
If they point to different allocations, how do we reconcile them?  We can't
necessarily just swap them as each allocation creates a potential chain of
re-uses (although we know that surviving symbols will still be using their
allocation).  We could require that assignments use a fresh allocation?  This is
hard to know ahead of time as the assignment first just evaluates it's
expression, and only after locks up the memory.  The problem is that in one
branch, an assignment might end up using an over-large allocation, and in
another a just right one.

A naive view is that we look at all the new allocations created by the two
branches, and generate a new set of allocations that will fit both.  Then we
remap all the allocations to the new set, ensuring that persistent symbols get
assigned the same allocation.  This should work?  Need to think through the
possibility that the order in which the allocations are used creates a problem,
but it seems that so long as every allocation has a corresponding new allocation
bigger than or equal to itself, it should work itself out?

How does code generation fit in?  That happens before the allocation.

## Interesting Example

Pretty challenging dealing with loops.

```
x <- integer()
for(i in 1:10) {
  x <- c(x, i)
}
```

Each loop iteration is going to have a different size for x. The best we can
hope for is consistent size every iteration including the first.  This should
handle the case where the loop doesn't even run.

What about variables created in loops that are not taken?

## Tracking Implementation

### Implementation

Trick with all this is we need to check the state of the stack at every use of a
symbol, so we're back to doing the whole tree as described in (old)...

        x
      / | \
     a  b  |
     |  |  |
     a  b  x

Not only that, we can't even know if everything is the same size or not unless
we process the expressions.

So we do all the renaming and stuff first, and allocation time, we build the
symbol mapping tree for each symbol with each step in the evaluation.

```
x <- w
if(a) {
  x <- r
} else if (b) {
  x <- s
} else {
  if(c) {
    x <- t
  } else if (d) {
    x <- u
  }
}
sum(x)
```

In order for this to work, `x` must always refer to the same allocation, and
have the same size.  More generally, any symbol written to in the `if` call with
a lifetime outside the `if` call must obey that constraint.  This will not
happen naturally if things are created in different order in different branches,
so seems like we 


`if` stack contains a count of how many `else` at any given `if` depth.  For
each depth, for each variable, what symbol-nodes it has been bound to (a symbol
node can contain the result of a nested if).

So the processing of an `if` should return a list with as many elements as there
are `else` clauses, plus 1 for the original, plus an indication if there is not
a terminal else.  Then, the function that initiated the processing of the `if`,
can check whether for each symbol, there are the right number of things.  For
those that don't have enough, add the previously bound symbol(s).  Then, flatten
all the lists in case there was any nesting.

Within each branch of the if, we don't have to worry about what's going on in
the alternate branches, only the symbol ambiguities accrued to date.

`for` loops are different because we need to forward check all the symbols due
to the same code being run more than once.  We simply disallow `for` loops from
changing a variable size.

Maybe overcomplicating this.  A `for` loop can only add a new rename for each

Ugh, and almost forgot about parameter order (it's okay, this is "unknowable" in
R anyway because it depends on when the params are touched inside functions, so
we have to declare it as a difference to `r2c`).

Maybe this isn't too bad, the main thing is to distinguish between data/external
symbols and new ones we add.  So when we enter a branch, we just create a copy
of the whole allocation structure, and when we exit, we compare to all the other
branches (including the original) and make sure they are compatible.  Actually,
for the `if` statements maybe we first compare across all branches and replace a
missing `else` with the original.  This should work recursively I think.  From
this perspective we don't need to keep any of the assignment symbol renames.
The only renames we care about are the subexpression substitutions.  The initial
renames were just to prevent invalid substitutions, but once that's handled, we
can revert them all (so we need to track original symbol).  This seems like it
could work.

Next steps:

* Tests (or do we wait to make sure this stuff actually works)?  Kind of, the
  code is all broken atm.
* If the first instance of an expression is assigned, and the assigned symbol is
  available, can we just use that as the name of the sub-expression?  Reasons
  not to are that these symbols can be overwritten (that's okay, renaming
  handles this), and it's also harder to debug because we won't be able to
  distinguish that a particular subexpression was replaced.  So probably lean
  against, as it would only "help" in the weird case where a user saved an
  intermediate expression, but then failed to use it.
* Start rebuilding the allocations mechanism.

## Track Substituted Expressions

* For the substituted ones it's trivial because their own index is the index
  of what should be referenced.
* For the one with the assignment....
* Seems like we really want for each call in the new tree, a reference to the
  old.

But, what we actually want is when we're going through the pre-process
linearization, for each thing in the tree we're traversing, have the original
call.  We can't do this for one tree alone.  Conceivably we could if we attached
the original call as an attribute, but stops working with symbols.  Maybe we
attach the original call to calls when there is a difference, and separately we
return a list of the calls corresponding to the re-use symbols.  This should
make it relatively straight-forward.

# Assignments

## Re-Writeable assignment

This raises an interesting point: if we allow modification of the assignment we
must create a new allocation and make a copy.  Maybe we can forward analyze and
see if future calls modify the symbol?

So, if each assignment generates a call, if it is to a RO variable it can record
the parameter and result ids as the same, and set the depth of that variable to
zero.  If it is a RW variable, then it allocates something to hold the copy, and
sets the depth of that to zero.

`preproc` will have separately generated a C command that is a either a NULL
operation (so that all the parameter incrementing stays in sync - hmm, looks
we do have some incrementing that doesn't require a call, e.g. for flag) or a
`memcpy`.  So `preproc` needs to track whether a variable is written to which it
can do in a second pass.  Actually not that trivial because `code.gen` is run
during the recursion stage, so we can't just go back and change it in the
linearized list.  We would have to defer running it, which means recording all
the parameters for it.

So how would we detect that an assign created variable is written to later?
preproc has the argument name.  It would then keep a top-level list of all the
assigned to arg names accumulated to date, and if at some point later down the
track a write function involves that parameter, it would know.  That means that

## Memory Re-Use

### Arith Assign, and Other Potential Re-use targets.

Can we use `+=` to avoid having to have memory allocated for both LHS and RHS
for the arithmetic operators?  Might complicate things substantially.  The main
problem with this is we have a protected expression for re-use, we cannot
overwrite it.  Additionally, we only ever want to overwrite the longer of the
two parameters.  So we could do this in the subset of situations where the first
argument is longer, but to do it `alloc` would have to realize this, know that
is a special vecrec + arith situation, and then set the result vector to be the
same.  We would also need to modify the arith code to have one additional
conditional layer that needs to be called once per iteration.

In general, we could tag a set of functions as being able to re-use memory if
they are vecrec and the internal code is single pass.  Then `alloc` will check
to see if the longer of the two parameters is protected by symbol reference, and
if not set that as the result.  Obviously functions tagged this way need to be
very aware that's what they are doing.  We don't need to use `+=` or whatever as
the target will be the same, but the C compiler won't be able to know that,
which might affect performance a little?

Better would be to have a second version of arith that does the `+=` business
and we substitute that one in for the calls that support it.  This doesn't
resolve `alloc` still having to do work.

It does create a second situation where we want to be able to code gen two
different pathways depending on the nature of subsequent code.  This should all
be able to happen at the preproc level though, I think.

### Do we do it already?

For the case of assigned to variables it seems maybe the code already does it,
as we see things like:

    ..$ :List of 4
    .. ..$ call: language square(x_mux)
    .. ..$ ids : int [1:2] 5 5
    .. ..$ ctrl: list()
    .. ..$ flag: int 0

In this case, `x_mux` is not referenced any further, and is thus free.  Seems
like this can work in some cases, but not all?

    f(x_mux, some_calc)

In this case, there is a risk that `some_calc` will end up using `x_mux`'s
allocation, so this is definitely too aggressive.  When a terminal symbol is
used, it should update the depth of the corresponding allocation.

## Memory

IIRC currently each call can re-use a memory slot deeper than itself.  This
makes sense in the context of a completely nested single line expression.  It
should extend naturally to assignments that are made at the top level.  Gets a
bit tricky if there is an assignment made inside a call (and also we need to be
able to distinguish between a param specification and an assignment; it's
probably okay though b/c of `match.call`).

So all assignments are made directly at the top level automatically since we do
not support `local` or anything like it.  Possibly instead of "top-level" we
just keep track of the current function's depth to allow for nested "r2c_fun"
functions.

We then need to figure out how to resolve symbols.

In the reduce step, we can just change the depth of the prior call's result on
the stack (remember to test sequential allocations `a <- b <- fun(c)`).

Things to do:

1. Allow preproc to match to newly created variables.
2. Add generator functions for RO and RW assignment.
3. Add a mechanism for alloc to know how to set the depth for assignment
   (actually this might be fine since `preproc` does it, problem is that `alloc`
   uses `depth + 1` to get parameters to current call, which won't work anymore.
4. Related to ^, change how `alloc` finds the parameters to each call.  We might
   need a separate flag to have `alloc` change the depth after identifying
   parameters, or alternative set a `callid` of some sort to identify parameters.
   Maybe have `preproc` add a `protect` flag.
5. Structure to track variables needs to contain:
   * Depth?
   * Size (including NA for group size).
   * Read Only flag.
   * Id of call that generated them (so we can go back and edit the call if an
     RO flag was toggled off).
6. Auto-free memory once there are no further calls to it (also true of
   sub-expressions).

First thing to solve then is allowing preproc to match to newly created
variables.  Hmm, not sure preproc is the one that does this.  preproc only
matches to parameter names.  So it's alloc that needs to do it.

We could do it by adding an additional match layer ahead of the `data.naked`
one.  If there is a match in this additional layer, then we use the variable
(and inherit size, etc.).

How does an overwrite work in this scheme?  It should mark the data allocation
as available, and ...

Also, things are a bit complicated by the call being in effect `"<-"(x, value)`.
We need to get the target name from the call, which isn't hard, but just doesn't
quite fit into the framework we have.

## Assignment and Control Variables

One big complication is that control variables could reference variables that
are created as part of the expression, e.g.:

    x <- a > b
    sum(y, na.rm=x)

How do we resolve this?  Currently the semantics are to treat symbols as if they
are the full data the symbols.  We could have a version of each assignment that
resolves also to the full data symbols used purely for the controls, but this is
very tricky because we don't want to carry out all those evaluations that define
each of the symbols unless we actually have to as those would compute on the
entire data vectors potentially.  We could define them as `delayedAssign`
into an environment, and then evaluate the control parameters with that as an
environment in the chain.

So every time we encounter an assignment, we need to record the assignment
expression as a `delayedAssign` in our special environment that has `data` for
parent.

Possibly even worse:

    sum(x, na.rm=(x <- y))

As it stands now we'll make a copy of `x` in the initial allocation pass,
whereas probably we would expect the opposite to happen (i.e. actually get `y`).
Probably we just warn people about it and let them make their bed.  Since we use
a new environment each time that `x` likely is lost.

## Assignment and Allocation

When we encounter an assignment, we need to know not to allocate additional
memory.  Additionally, we need to know to free whatever data was previously
pointed to by that symbol.

When we encounter a symbol, we need a mapping table to know whether the symbol
is an assigned symbol.  If it is, then we know it's size from the `alloc` value.

We want to track when each symbol is last used so we know we can trash the
allocation.  We can probably do this in a first pass through the linearized
calls where, for each symbol, we find the latest read reference to it.  Then,
anytime a symbol is "protected" by being written to, the "protected" value is
the last reference to it.  Each iteration then frees all the wrong depth stuff,
and any expired protections.

Any overwriting of a symbol will also effectively free the prior allocation (and
transfer the protection value to the new allocation (although they may be the
same size and re-use the slot).

## Renaming

Preprocess does match each call.  Whenever it matches a call, we can check if
any of the parameter expressions reference a prior expression, and at that point
we can go back and mark as protected.  Need to make sure anything data/external
is default protected.  Interestingly we don't need to auto-protect an
assignment, it only needs to be protected if at some point it is re-used (but it
would be interesting to let ppl know if it is never used).

So we want a structure with all the prior expressions so we can test them, and
go back and mark them as needing to be protected, and also the point at which
they are last used.

Assignment can use this.

Then, at the end we can go back and code gen everything, substituting when
appropriate functions that are dealing with protected parameters.

The renaming process likely needs to be a shadow rename, meaning we need the
original copy too.

Anytime we rename, we need to process all remaining expressions and rename them
too.  Likely we want a deferred rename, i.e. it happens just before each
expression is processed, so we need to accumulate all the renames.  If we don't
do this we have to maintain two copies of the call tree which is going to be
annoying to recurse through.

* Need a stack of renames.
* Need a stack of past expressions to identify re-use (we have this already from
  the linearized stack).

## Nested Functions

In order to support nested functions, we need to prevent renames of external
variables.  We also need to affect symbol resolution.  So essentially each
function layer will need a whole frame of `call_dat` or some such.  We will also
need to re-match to the outer function environment as that could cause renaming
of the data variables, etc.

What a nested function does is effectively:

    function(a, b) {
      a <- force(a)
      b <- force(b)
      ...
    }

In other words, it generates implicit bindings to its local variables.  To
shoehorn it into our framework we can just keep applying the renames on a
rolling basis, and they should then naturally expire by the time the function
exits scope.  So `preprocess` will need to identify a function to symbol in the
call, realize that it can get a body for it, bind the formals to force-renamed
symbols, and proceed as normal.  The allocator should then behave correctly from
that point forward, without needing an explicit frame stack.

What we can do is anytime we encounter a function, generate the assignment calls
for all the parameters.  We only allow constant default params, so should be
okay (but need to double check what we do with those at preprocess time).

One problem to resolve with the renaming is the nested function might use a
component from the outer one (e.g. internally it computes `mean(a)`, but
`mean(a)` has already been computed previously); the renaming would prevent
re-use of the expression.  So maybe we don't need to worry about renaming.  The
only real issue is how unbound symbols are resolved, and making sure that
such symbols are not resolved against symbols bound in the calling frame.

So it seems that anytime we enter a new function, we need to reset the portion
of the allocated data structure that we can look through.

## Preproc

Shouldn't actually generate any code?  But we need a mechanism to tell the
allocator that the result of the prior call should be assigned to the top level,
and we need to bind it to a symbol (duh) that will be on the search path.



# Interface

## apply

Evaluating whether we should shift to `tapply`.

    function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)

What would that do?

    r2c_tapply(x, g.r2c, r2c_sum, ...)

Is it okay to have `MoreArgs` as dots?  Probably, the issue being conflict with
additional parameters, which in this case is just `enclos`.  In terms of
implementation we would just capture them in a list and pass them on.

Names?  We want to add window functions, so do we end up with:

    r2cgapply
    r2cwapply

Or

    gapply
    g_apply    # makes it clear not base R
    wapply
    w_apply

Another option

    ctapply
    c_tapply
    cwapply
    c_wapply

Should the name communicate in some way that these only run with `r2c` funs, or
do we wish to allow them to run with other functions?

By extension we should probably do:

    csum <- r2c(sum(x))

The misleading thing for that one is `sum` is also essentially just C code, but
for the more complicated cases that makes more sense.

## tapply

Semantic incongruities:

* We accept lists as the first parameter, so not based on "split" method.
* Functions can accept multiple parameters, and they are matched into `X`.
* Result is neither array nor list, always simplified into a vector or a
  data.frame.

`tmapply`?

    tmapply(FUN, INDEX, ..., MoreArgs)

Main issue here is what if we want to pass a data frame in ...?  This would
require always specifying each element individually.  Additionally,
if we have `MoreArgs` we're required to type it out.  We could use:

    tmapply(FUN, INDEX, DATA, MoreArgs)

This is pretty close.  Or maybe:

    tmapply(FUN, INDEX, DATA, ...)

Although same issue of wanting to pass a list in ..., and additionally name
overlap with other parameters.

    grapply(FUN, INDEX, DATA, MoreArgs)
    grapply(FUN, INDEX, DATA, ...)
    grapply(FUN, INDEX, X, ...)

Dots are appealing, except for the potential for name collision.

    grapply(FUN, INDEX, DATA, ...)
    group_apply(fun, index, data, MoreArgs=list())

    grexec(FUN, INDEX, DATA, ...)  # too close to gregexec?
    group_exec(fun, groups, data, MoreArgs=list())

    window_exec()
    window_apply()
    wapply()

Do we need to provide `enclos`?  Probably not.

## NSE

Things like:

    tcapply(
      iris[paste0('Sepal.', c('Width', 'Length'))],
      iris[['Species']], csum
    )

Are annoying.  This okay enough?

    with(iris, tcapply(list(Sepal.Width, Sepal.Length), Species, csum))
    with(iris, tcapply(l(Sepal.Width, Sepal.Length), Species, csum))


# Loop implementation

Loop variables can be created in a child frame to the data.

Each loop body will generate a `run` statement, e.g.:

```
run(...) {

}
run_loop_1(...) {
  for(intmax_t i = 0; i < len; ++i) {
    run(...);
  }
}
run_loop_0(...) {
  for(intmax_t i = 0; i < len; ++i) {
    statement_0(...);
    statement_1(...);
    run_loop_1(...);
  }
}
```

Probably want two types of loops, loops where we know the start and end as
constants or other derivable data (`seq_along`), and others where we have an
unknown vector?  Might not be able to allow the second type, because at that
point we can't know how large a vector that is assigned to with e.g. `x[i] <- y`
will get.

```
for(i in seq_along(x))
  y[i] <- x[i] + 1
```

Is the latter:

```
for(i in x)
  y[i] <- z[i] + 1
```

At the allocation stage we need to determine the size of `y`.  It's fine if
we're just referring to an external symbol (or even a group symbol) as we can
check the content sizes.  But things like:

```
for(i in rev(x))
```

Get messier.  Likely we just allow symbols, or `seq_along`, or constant
expressions like `a:b` where `a` and `b` are known at allocation time (changing
them inside loop is fine since):

> The seq in a for loop is evaluated at the start of the loop; changing it
> subsequently does not affect the loop.

But they can't be computed within the `r2c` code.

# Further Optimizations

## Overview

Thoughts to try to reduce the group overhead.

* How much overhead is just from `run`?
* Evaluate overhead from generating result labels.

It's actually significant: out of ~.9 secs for 1e8, .2 is spent on label
generation.  In this case we need to be careful due to hashing of labels.  We
can reduce this dramatically for the case we know that group.res.sizes are all
size one, and a little bit if we know they are all the same size.

We're actually getting a bit of overhead from generating the labels, so should
just generate them directly in C (and now we do).

It doesn't seem like order affects collapse much.

## Group Sizes

Group size computation is significant.  The need to do two pass is also not
super (group count, vs recording group sizes).  Group count is actually quite
fast because it can use vectorized instructions to increment the group counter
without the need for branches.

We also have the issue that we don't allow more than INT_MAX groups, and related
that we do allow R_xlen_t max group sizes (the former needs to be decided on as
a limitation, the latter needs to be decided on as a feature we want to keep).

Big question is whether we want to change the logic to avoid doing two passes.
We can do something that e.g. allocates 1/4 of the size (but it has to be for
labels and for group sizes), but if it fails will need to go to 1/2 the size or
larger.  Maybe start at 1/8.  There is an outer loop that checks if we hit the
last value in the loop, and if we did triggers running the inner loop again
after "growing" the vector (i.e. re-alloc, copying, unprotecting).

The other thing we could do is an analysis of the first 1000 elements to
estimate the group size, but this will fail miserably in the case where there is
a single e.g. 1K initial group, but then much smaller group sizes after.  So it
gets pretty complicated: you need to do that first 1K pass, assume e.g. smallest
group, and if you fail, fall back to old method.  If you succeed you're still
left with an over-allocated vector you likely need to subset or copy later
(unclear whether we are allowed to use "true length" for this, or whether we
want to manually generate the label vectors manually).

The pattern we're looking for is "increment counter on condition", which we can
use to increment a pointer.  But then we are going to need to write to write to
memory every iteration, which is probably worse.  So likely the best we can do
is reduce time by 1/3 by avoiding the initial group number counting, which seems
like a lot of complexity and work.  Instead maybe we really should have a
separate group computation.

## Notes vs Collapse

For a single stat like `sum(x)` collapse is more memory efficient because it
does not reorder the data vectors.  Also, while generating the radix sort
ordering vector is fast, the actual reordering of the vector is much slower.
Would be interesting to figure out why that is (actually this untrue, I was
remember the cases where we were hitting the memory limits with 1e8, it takes
the same amount to order as it does to re-order with the order index).

For multiple stats like in `slope(x)`, `r2c` is more efficient.

For smaller group counts, the hash algorithm from collapse seems much faster.

Idea: could we do something with only sorting the result at the end?  Seems like
it should be terrible to access the data vectors in an unsorted order though.

From analysis it looks like what's going on is that (unlike `GRP`), `fsum`
defaults to the hash method.  There are many pathways in the hash code, but for
our data it seems the most likely outcome is to end up in one where the hash
table size is the size of our input vector.

The hash function is just the value of the group index modulo the has table
size.

But for some reason this doesn't seem to have terrible corner cases.  Once we
get to small group sizes, we start underperforming the radix sort, but not by
massive amounts, and we're way faster for larger group sizes.

On my system this is the cost of accessing the vectors at random (well, this
includes the physical re-ordering):

> system.time(x[io])       # ordered
   user  system elapsed
  0.052   0.003   0.057
> system.time(x[ir])       # random
   user  system elapsed
  0.286   0.007   0.311

And most of the cost is just accessing the vector (`order_sum` below adds each
value to a single accumulator in the order specified by io).

> n <- 1e7
> x <- runif(n)
> io <- seq_len(n) - 1L
> ir <- sample(io)
> system.time(order_sum(x, io))
   user  system elapsed
  0.022   0.000   0.023
> system.time(order_sum(x, ir))
   user  system elapsed
  0.210   0.006   0.230

The key to collapse speed is that it reads the large input vectors in order, but
then writes to the small group vector out of order.

> system.time(fsum(x, g4, na.rm=FALSE))
   user  system elapsed
  0.081   0.004   0.087

This advantage falls apart as we grow the size of the group vector by increasing
the number of repeats:

> system.time(fsum(x, g6, na.rm=FALSE))
   user  system elapsed
  0.865   0.017   0.910

But exactly what is happening is not 100% clear:

    1e4 groups                             milliseconds
    fsum ------------------------------ : 98.79 -  0.00
        fsum.default ------------------ : 98.79 - 12.73
            qF ------------------------ : 86.06 -  0.00
                hashfact -------------- : 86.06 -  0.00
                    groupfact_sorted -- : 86.06 - 70.91
                        Csv ----------- : 15.15 - 15.15

    1e6 groups                             milliseconds
    fsum ------------------------------ : 872.0 -   0.0
        fsum.default ------------------ : 872.0 - 103.5
            qF ------------------------ : 768.5 -   0.0
                hashfact -------------- : 768.5 -   0.0
                    groupfact_sorted -- : 768.5 - 636.6
                        forder.int ---- :  81.8 -  81.8
                        Csv ----------- :  47.3 -  47.3

The main incremental cost is in groupfact_sorted, and we can tell from the C
trace that's almost all from `dupVecIndex` which is where each element of the
group vector is given the order of appearance of its group (which can be used to
map inputs to group positions, possibly even after re-ordering).

We're actually possibly faster if we take up the whole hash space, which
trounces the theory?  Eh, maybe not, we're still only loading 1e3 cache lines,
which actually will fit in L1 cache.

    all.range <- as.integer(seq(1, n, length.out=1e3L))
    g4.ar <- sample(all.range, n, replace=TRUE)
    treeprof::treeprof(fsum(x, g4.ar, na.rm=FALSE))
                                           milliseconds
    fsum ------------------------------ : 80.52 -  0.00
        fsum.default ------------------ : 80.52 -  9.25
            qF ------------------------ : 71.28 -  0.00
                hashfact -------------- : 71.28 -  0.00
                    groupfact_sorted -- : 71.28 - 54.77
                        Csv ----------- : 16.50 - 16.50

If this is true it should be the case that contiguous should remain fast at
higher sizes.  Let

    n <- 1e7; ng <- 1e3
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.069   0.003   0.072
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.064   0.004   0.068


    n <- 1e7; ng <- 1e4
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.132   0.003   0.136
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.077   0.003   0.080

Let's try with different types to see what happens:

    n <- 1e7; ng <- 1e4
    set.seed(1)
    x <- runif(n) * runif(n)
    all.range <- as.integer(seq(1, n, length.out=ng))
    g <- sample(ng, n, replace=TRUE)
    g.ar <- sample(all.range, n, replace=TRUE)
    gt <- as.character(g)
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.373   0.020   0.401
    gt <- as.character(g.ar)
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.412   0.016   0.435
    gt <- g + 0
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.621   0.030   0.668
    gt <- g.ar + 0
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.693   0.027   0.730
    gt <- factor(as.character(g))
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.032   0.000   0.033
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.085   0.007   0.094
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  0.763   0.020   0.802
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.142   0.016   0.160

Pretty much as expected.  Big benefit from factors.  So really the hashes work
very well with sequential integers, whereas the sorting version should be about
the same for everything?.

    system.time(order(x))
    ##   user  system elapsed
    ##  0.587   0.026   0.620
    system.time(order(g))
    ##   user  system elapsed
    ##  0.201   0.005   0.208
    gt <- as.character(g)
    system.time(order(gt, method='radix'))
    ##   user  system elapsed
    ##  0.271   0.006   0.281

Largest number of groups that will fit in L1 would be 16K, not enough to start
evicting out of L2 for the all range one.  To do that we need 32K entries.  At
64K entries we should still fit adjacent in L2.

    n <- 1e7; ng <- 2^16
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.358   0.008   0.371
    system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.136   0.004   0.140

This suggests a massive crash in performance as for all range half will have
been done in L2, and half out.  We can go up to 500K entries contiguous before
we start exhausting L2 cache, so let's try with 250K groups:

    n <- 1e7; ng <- 2^18
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.599   0.010   0.616
    system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.343   0.007   0.353

**We're assuming no hash collisions whatsoever**

Hmm, not what I expected.  For this case we should be needing 16MB worth of
cache lines in the ar case, but only 1MB of cache for the contiguous case,
thinking about the hash alone.  But there are other memory accesses too:

* Seq - `iid <- px[i]` read entire vector sequentially.
* Rand - `hid <- h[iid]` read the hash per the input vector.
* Rand - `px[hid-1]` read input per hash value for first obs of group.
* Seq - `pans_i[i]` write

Basically, how much of the hash table can we keep in cache given competing
demands from the sequential reads and writes, and the random accesses.  So we
need a measure of age of the oldest hash access as a function of the number of
entries read?

For the 1e3 groups case, let's say on average we hit every group every thousand
reads (going to be more than that, but it's all probabilistic), then we're
using:

* 1000 x 2 x 4 (sequential integers, read and write).
* 1000 x 64 (random integers, but focused on the front part of the input)
* For the hash acces:
    * 1000 x 64 for the spaced out case
    * 1000 x 4 for the normal case

Or:

    x * 2 * 4 + x * 64 + x * 64  == x * (8 + 64 + 64) == 136 * x
    x * 2 * 4 + x * 64 + x * 4   == x * (8 + 64 + 4)  ==  76 * x

Second number is questionable, and also whether we can assume we're going to hit
every group value after ng writes and thus avoid eviction.

One conclusion from this is if we know the range and we know we won't have
collisions in the hash, we can dramatically reduce our cache utilization by
avoiding the random read back from the input vector to check whether the hash
hit is a match or a collision.

The most questionable number is the second one.  Not quite sure how to think
about the fact we're only reading from the section that contains every first
instance of a group, and at what point we start overlapping in cache lines.
Probably not a factor at 1e3.

So, per these numbers, in the contiguous case we would start thrashing L1 at 826
groups, and L2 at 27.6K, vs 481 and 15.4K.

    n <- 1e7; ng <- 1600000
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    system.time(fsum(x, g, na.rm=FALSE))

Empirical tests:

     ng   all  cntg
    480  .086  .076
    826  .081  .074
   1600  .088  .076
   2400  .115  .072
   4800  .151  .074
   5200  .134  .081
   7500  .143  .091
  15000  .178  .097
  25000  .195  .124
  35000  .340  .148
  50000  .393  .156
  75000  .446  .192
 100000  .496  .200
 200000  .636  .349
 400000  .807  .549
 800000 1.040  .821
1600000 1.208 1.081

Empirical data suggests a 5-6x cache utilization advantage for contiguous.

    x * 2 * 4 + x * 64 + x * 64  == x * (8 + 64 + 64) == 136 * x
    x * 2 * 4 + x * 64 + x * 4   == x * (8 + 64 + 4)  ==  76 * x

So if we drop the middle term:

    x * 2 * 4 + x * 64  == x * (8 + 64) == 72 * x
    x * 2 * 4 + x * 4   == x * (8 + 4)  == 12 * x

We get very close, and also this matches the key break points, with 5.4K and 1K
entries for L1, and 175K and 30K entries for L2.  This matches the timings very
well.  Don't understand why the reads back from the input vector (which should
require whole cache line loads) don't seem to figure in this math.  Actually,
it's likely because of things being concentrated in the beginning of the vector
as speculated:

    > z <- 10000; sum(1:5000 %in% g[1:z])/z
    [1] 0.4358
    > z <- 10000; sum(1:5000 %in% g[1:z])/5000
    [1] 0.8716

So most initial group instances are going to be very concentrated, with only a
few stragglers.

Tradeoffs are:

* Only access ooo the smaller group vector (we would need to do this for every
  input vector).
* Only need to do the very slow stop once.

What's still not clear is why the magnitude of the slowdown is so large for
`groupfact_sorted` (`dupVecIndex`).  Most likely once the group vector gets
large enough we start trashing cache.  E.g. at 1e6 group size, we're at 2x cache
(4MB).  But that should affect both the generation of the hash as well as the
writing to the group vector.  It does, in fact the writing to the group vector
proportionately becomes much slower (assuming that's the unattributed time in
`fsum.default`, we see  271:51 (5.3:1) vs 501:160 (3.13) from the trace, so
increasingly the sum access becomes a bigger issue.

Running some timings on a larger memory system (16GB, 4MB of L3 cache).  Here
we're dealing with group sizes that in the contiguous case just fit in cache at
1e6 * 4 (n/100):

    set.seed(1)
    n <- 1e8
    x <- runif(n) * runif(n)
    g <- sample(ng, n, replace=TRUE)
    library(r2c)
    library(collapse)
    f <- r2cq(sum(x))

    ng <- n/10

    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  7.421   0.477   7.919
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ## 12.264   0.580  12.888

    ng <- n/100
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  7.174   0.571   7.766
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  5.101   0.401   5.520

    ng <- n/1000
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  6.512   0.543   7.077
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  1.280   0.184   1.471

    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  3.672   0.445   4.131

At n/1000 we're talking 400KB contiguous or 6.4MB, so for the latter just 50%
overflowing L3 cache.

    ng <- n/100
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  6.877   0.590   7.566

At 2n with n/100 we're 50% in cache for the contiguous case.  Maybe should try
the n/10 case with g.ar?

    set.seed(1)
    n <- 2e8
    ng <- n/100
    x <- runif(n) * runif(n)
    g <- sample(ng, n, replace=TRUE)
    ## system.time(res <- group_exec(f, g, x))
    ##    user  system elapsed
    ##  18.133   1.306  19.516
    ## system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed
    ##  10.954   0.814  11.783
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    ## system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed
    ##  17.222   1.158  18.458

## Notes vs data.table

Not entirely sure what's going on with `data.table`.  For the 10e7 case we're
seeing (run 10x and profiled with instruments, cleaned up):

    2.87 s            0 s	  R_doDotCall
    2.41 s      967.00 ms	   gforce
    1.22 s            0 s	         R_doDotCall
    1.22 s      302.00 ms	          gsum
    908.00 ms   908.00 ms	           gather
    11.00 ms     11.00 ms	           platform_bzero$VARIANT$Haswell
    222.00 ms          0 s	    Rf_allocVector
    354.00 ms   340.00 ms	   uniqlist
    71.00 ms    20.00 ms	   uniqlengths
    29.00 ms    29.00 ms	   DYLD-STUB$$INTEGER

Only ~1/10th of the time seems to be spent within gsum itself.

# Other Implementations

## ast2ast

https://github.com/Konrad1991/ast2ast/

## sph-sc

Scheme to C via @wdkrnls (Kyle Andrews).

## armacmp

https://github.com/dirkschumacher/armacmp

## nCompiler

https://github.com/nimble-dev/nCompiler

## Graal

Main question here is how limiting the R-C boundary is.  Is it the case that
code such as `data.table` is not any faster?  Does that even run?  How many of
the packages actually work?  What stuff does not work?  Seems like there should
be a bit of stuff that doesn't per TK.


## Renjin

## Odin

Fascinating implementation, but the key issue seems to be that it seems to be
very much focused on the derivative syntax of DeSolve and thus abandons R
semantics.

Supports things such as (from array portion of vignette):

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ]))

But not:

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ] + 3))

I.e. it won't recursively construct the expressions.  But it can be done with:

    tmp[,] <- ay[i,j] ^ 2
    deriv(y[]) <- y[i] * (1 - sum(tmp))
    ...
    dim(tmp) <- c(n_spp, 10)   # probably can submit the 10 as user()?

Additionally, the concept of arrays is really a stand in for multiple variables.
In the above example `i %in% 1:4` so it's a four variable system and could be
written as such, not really the concept of vectorized data intended to be
aggregated.

We can get "vectors" in the sense we're used to in there, but they need to go in
as matrices (but that's okay, we can just do this with 1D matrices).

# Ex Ante Size Computation

We need some view of the size pre-compilation, but by virtue of allowing
references to external objects we can't resolve them fully.  So we have options:

* Group size
* Constant size (knowable at compile time)
    * Scalar (e.g. result of `mean(x)`)
    * Other constant (e.g. result of `range`)
* External, could be anything (zero, scalar, group size, whatever)

At compilation time we may be able to emit better code if we know if the inputs
are scalar / group size, vs if we don't we'll need a per-group conditional to
decided.  So the code generation needs to be able to run with the partial
information.

So we need a pre-processing pass that does all the size computations, but not
the actual allocations, and then a second pass once the input sizes are known.

How do we handle something like:

    pmax(scalar, external, group, constant)

For "vecrec" probably need to resolve to the worst in order:

    scalar > constant > group > external > external_or_group

But we must always keep track of whether group is involved.

One big issue is we don't know until the evaluation stage what columns are going
to be group vs external because we haven't seen the data.frame.  So we can
compile highly efficient code if we know the situation, but less so otherwise.

Similarly, if we want to pick code based on the value of control parameters, we
need to evaluate them at compile time.  This severely cramps our style.  The
control param evaluation at compile time is probably okay.

To resolve the knowing the data columns, we have two choices:

* Force all data columns to be the same size (what about non-data params like
  `probs`)?
* Make the code capable of handling different lengths inputs.

For the latter, we could have the code react to each group, but it would be much
better if it could before running pick the correct one.  Probably not possible
without recompilation.

Some costs of not being able to do the full optimization:

    > n <- 1e7
    > x <- runif(n)
    > g <- sample(n)
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 0L))
       user  system elapsed
      1.077   0.005   1.087
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 1L))
       user  system elapsed
      1.076   0.004   1.085

These above are with `na.rm` as an explicit if/else, where group size is 1..

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed
      1.030   0.004   1.040

Here instead we assume `na.rm = FALSE` so no extra if/else.  For reference this
is what the above looks like with group size = 10:

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed
      0.778   0.004   0.788

And finally back to group size = 1 we now add a `asInteger(VECTOR_ELT(x, 0))` to
retrieve the `na.rm` flag:

    > sys.time(res <- run_group1(shlib, "fun5", list(x), g, 0L))
       user  system elapsed
      1.147   0.002   1.149

So a 10% penalty, which suggests some value in allowing for a single flag to be
passed down.  But maybe this becomes a future feature.

For arithmetic, there is seems to be very little value in having the simplified
logic option (in fact, it looks slower, `fun7` is the one that goes straight to
the equal length loop).  So no huge cost.  At least when there is no contention
with any other processes.  Given that sorting is such a big part of the cost,
it's going to be hard to beat that.

    > sys.time(res1 <- run_group2(shlib, "fun6", list(x, y), g))
       user  system elapsed
      1.058   0.007   1.071
    > sys.time(res2 <- run_group2(shlib, "fun7", list(x, y), g))
       user  system elapsed
      1.069   0.008   1.084
    > sys.time({
    +   o <- order(g)
    +   xo <- x[o]
    +   yo <- y[o]
    + })
       user  system elapsed
      0.783   0.005   0.794
    > sys.time(res3 <- xo + yo)
       user  system elapsed
      0.022   0.000   0.022
    > all.equal(res1, res2)
    [1] TRUE
    > all.equal(res1, res3)
    [1] TRUE
    >

# Simple arithmetic

Compute length of each vector.  Generate the full C expression with index or
pointer offset access.

    x <- runif(5e6)
    y <- runif(5e6)
    z <- runif(5e6)
    w <- runif(5e6)
    >
    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed
      0.025   0.000   0.025
    > sys.time(fapply:::test2(x, y, z, w))
       user  system elapsed
      0.018   0.000   0.018

It is slightly faster to do x + y + z + w (test2) than do pairwise additions
across the full vectors (test1).  Maybe because we only increment the loop
variable once?

With `MOD_ITERATE_CHECK2` and checking every 1e6:

    > sys.time(fapply:::test3(x, y, z, w))
     user  system elapsed
    0.036   0.000   0.038

Interestingly `MOD_ITERATE2` is no faster, so the check is basically free.

`R_ITERATE_CHECK` is comparable to pairwise addition (test1):

    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed
      0.026   0.000   0.028
    > sys.time(fapply:::test5(x, y, z, w))
       user  system elapsed
      0.026   0.000   0.028


# Function Types

* Aggregation functions (result length == 1)
* Vectorized functions  (result length == 0 or longest vector)
* Arbitrary functions   (result length is known (above is degen case of this))

Maybe use iterator macros.  E.g. `MOD_ITERATE2_CHECK`.  These should work fine
and are fairly efficient (i.e. reset counter to zero instead of reading with
modulo).

When construction the expression, when do we need to recursively evaluate
sub-expressions?  E.g. in:

    x + y + z

Which is really:

   `+`(`+`(x, y), z)

How do we know we can just turn it into:

    for(i = 0; i < len; ++i) x[i] + y[i] + z[i];

Vs

    x + y + mean(z)
    x + y + sort(z)

It's really that there is no mixing moving of the vectors.  Associativity /
commutativity shouldn't matter as that's handled by C / parentheses.

If we have a complex expression with a lot of these then it potentially gets
tricky b/c we start requiring intermediate storage for computing the whole
expression.

So we need to distinguish between "vectorizable" functions that operate one
element at a time, and those that aren't that operate on more than one element
at a time (either by moving them, etc).  The latter have to be evaluated
separately.  So the "parser" will need to identify where such functions are and
then evaluate/reduce them to a form that can then be used in a vectorized
context.

Because of this we might just favor the iterative resolving of the expression.
This will be easier.  We could just have logic to try to make the biggest
vectorized expression.  This would avoid a lot of extraneous code.

So right now:

    x + y + z

Becomes

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];
    for(i = 0; i < n; ++i) res[i] = res[i] + z[i];

And:

    x + y + mean(z)

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];

    double tmp = 0;
    for(i = 0; i < n; ++i) tmp += z[i];
    tmp /= n;

    for(i = 0; i < n; ++i) res[i] += tmp;

So we'll need as many temporary scalars / vectors as there are arguments to
functions.  Maybe we don't allow anything but binary?  Hmm, so with:

    pmax(-x, log(y), mean(z), w - u)

In all cases we'll know the size each argument.  In the expression above we know
how many parameters there are so we can construct the correct code at
"compilation" time.  What about:

    pmax(pmin(y, z, w), pmin(w, q, f), mean(z + y))

Generated code:

    double * tmp0 = R_alloc();
    for(i = 0; i < n; ++i) {
      tmp[i] = z[i] + y[i];
    }
    double tmp1 = mean(z + y);

    for(i = 0; i < n; ++i) {
      tmp0 = pmin(y[i], z[i], w[i]);
      tmp2 = pmin(w[i], q[i], f[i]);
      res[i] = pmax(tmp2, tmp3, tmp1);
    }

And (numbers show depth):

    pmax(pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y))
         pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y)
                    mean(w)                  mean(z + y)
                                                  z + y


Generated code.  Coords are (depth, param)

    // coords (0,0),(1,0)(2,2)
    double stmp0;
    stmp0 = mean(w);

    // coords (0,0),(1,0)
    double * vtmp0 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(y[i], z[i], stmp0);

    // coords (0,0),(2,0)
    // * stmp0 no longer needed
    double * vtmp1 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(w[i], q[i], f[i]);

    // coords (0,0),(2,2),(3,0)
    double * vtmp2 = R_alloc();
    for(i = 0; i < n; ++i) vtmp2[i] = z[i] + w[i];

    // coords (0,0),(2,2),(2,0)
    stmp0 = mean(vtmp2);

    // coords (0,0)
    // * vtmp2 no longer needed, so re-use
    double * vtmp3 = R_alloc();
    for(i = 0; i < n; +++i) vtmp2[i] = pmax(vtmp0[i], stmp, vtmp1[i]);

We need to track how many variables we've generated and when each one is freed,
without garbage collection.  So for each variable we need to track the depth at
which they are used.

Ideally we'll know for each group exactly how many variables we'll need and of
which size.  So we want to allocate the minimal amount w/ re-use that we need.
Number of variables should be at worst `sum(pmax(length(args) - 1)` or some
such.

One problem is that computing ex-ante how big each of our intermediate
allocations is going to be could be costly.  For `pmax`, we have the group based
params that are known sizes, but there could be external params of unknown
sizes.  So we could reduce the size expectation to be something like
`max(c(G, a, b, c, ...))`.

Provided options:

* Length of group
* Constant length (mean/sum/prod->1; Max returns -Inf)
* Length of one specific argument (quantile)
* Recycled max of multiple arguments (different from plain max because if one of
  the arguments is known to be zero, then the lot is zero).

Not provided options:

* Function of length of single argument.
* Function of lengths of various arguments.

This means we only care about the maximum group size as we'll always want to
allocate to that group size at a minimum.

So we navigate the entire parse tree, and for each function we retrieve the type
from above.  This will require:

* Reducing the function to its numeric arguments.
* Looking up the type of function.
* Computing the maximum return size as a function of form `max(c(G, k))` where
  `k` is a constant presumably derived from external variables, or possibly 0.

For a single function:

* Confirm it is a known function by getting it and comparing it to our list.
* `match.call`.
* Identify all non-group parameters.
* Evaluate all non-group parameters and bind them to local symbols.
* Check that all the C parameters are double (either a group param, or double).
* Depending on function type
    * (if needed) Compute lengths of all non-group C parameters, and record max
      of them or zero in `K`.
    * Record shorthand size, e.g. `K`, `g`, or `max0(g, K)`, where `max0`
      returns 0 if any of `g`, `k`, are zero, and `K` is a scalar constant.
    * We thus need a structure that contains a constant, whether the constant is
      set (we need to distinguish zero), and whether the group size matters.

Across the full expression, given `g` is equal to max group size:

0. Initialize vector of temporary items.  This is an array of R_xlen_t sizes
   sorted by size and an array of integers denoting whether the corresponding
   entries are free or busy.
1. Confirm function is valid.
    * Either resolves against list.
    * Or does not contain any references against symbols in the grouped data.
2. Based on function type:
    * Invalid: STOP.
    * Valid no symbols: eval and return length as constant (SIDE EFFECTS?).
    * Valid referencing symbols: continue to 3.
3. Count function parameters that are of unknown length.
4. For each unknown parameter, recurse to 1.
5. Once all parameters are of known length, compute expression length.
6. Scan through free entries to see if any are big enough.
    * For entries with group size, use the biggest group size.
    * An entry is free if its depth is greater than one below the current level.
    * If yes, mark as busy with the current depth.
    * If no, find the spot to add a new entry and mark with current depth.
    * Process of adding an entry should preserve sorting of list.
5. Return expression length computed in 5.

Now that we have the expression length, compute the result vector size.  It
should be either a constant per group, or `g`.

The top level return should give us the result size, and our list should give us
the set of intermediate vectors we'll need.

# Interface

## R Level

Each special function must be pre-registered with some mechanism to distinguish
which parameters are vector ones that will be fed, vs which one should be
evaluated immediately for dispatch decisions (e.g. `na.rm=TRUE`).

How does this work for a newly defined function at the R level?  Does it matter?

## C Level

### Basic

Target function:

* Array of double pointers the same length as it expects number of arguments.
* Int with number of arguments.
* Array of R_xlen_t with lengths of each of the arguments.
* Array of R_xlen_t with offsets of each of the arguments.
* Double array of right size for result.

Should we have some structs pre-defined to hold the above?

Can the function check it's doing the right thing in terms of argument count?
Would it be better to have the arguments provided explicitly (in which case we
really want structs?  Should the "Array of double pointers" already be at the
expected offsets (maybe).  Do we want iterators like R provides to walk through
all the arguments?

Maybe we don't want the lengths of each of the arguments; instead, we want the
length of the result, and the modulo to read all the other arguments not of that
length?

should accept an expects arguments.  All functions should be like that.

Parameters such as `na.rm=TRUE` actually cause dispatch to a different
function.  So when we register a function we need to declare this?  I guess it
should be optional and we use it to recreate `mean`, etc.

### Allow SEXP

What about functions that accept non-double parameters?  In particular, we'll
have expressions that reference external/evaluated things, which might not be
double.  If we want to allow these to be fed to our functions, our functions
need to be able to accept SEXPs.  This substantially complicates the interface;
are there cases where we would want to do this?

An almost example is `quantile`, where the `probs` param is likely fetched from
elsewhere.  Or `filter`.  Essentially anything where the configure param
resolves to an arbitrary number of possibilities and thus can't be reduced to
different entry points.  `findInterval` might be a better example.

Seems like we have two choices:

* Allow additional SEXP arguments (and identification of what these are).
* Add ALTREP so that all arguments can be SEXP.

The latter seems cleaner.  What happens with the result vector at that point?
Probably can't be ALTREP as we'll need to modify it.

How much overhead does ALTREP add?  There is a good example from Gabe about
implementing window functions.

So is this worth doing?  What is the advantage of doing so.  More familiarity
for the implementer of new functions?  Maybe for now we just pass through as a
`moreArgs` list.  So we end up with `double **` for the data arguments, along
with offset and length, which could include some vectors that are not-data, and
a list of extra arguments.  Does this obviate the need for a configure function?
Probably, it then happens in C.  The sad thing about this is we have to redo all
the configure business for each call.  Hmm.

Related, could this eventually be made to use the actual R functions?  The
summary functions are all in "src/main/summary.c" and those are simple, but the
problem is they are all static and dispatched to by `do_summary`.  And calling
the latter will be very challenging from outside.

# Pre-Processing

Once we have the expression parsed, do we want to link in pre-compiled entry
points, or do we just want to generate the whole thing in one piece of generated
code?  Probably start with one piece of generated code.

    sum(((x - mean(x)) * (y - mean(y, na.rm=TRUE))) + z)

What does this become?  Each sub-expression needs to be generated in linearized
code.  There will be a vector of allocated vectors for temporary use.  So we
need a list where each element is a step, and each step should include:

> We probably want a version for variable argumens (e.g. `pmin`, and one for
> fixed argument count).

* That data structure, which is an array of double pointers with:
  * Group data at the beginning.
  * Non-group double data next.
  * Temporary arrays.
  * Result array.
* An integer count of parameters into the data structure.
* An array of offsets into the data structure.
* An array of offsets into each vector in the data structure.
* An array of lengths of each vector in the data structure.
* A `VECSCXP` of all the control parameters.

So in C our earlier expression will look like (this does not include `ctrl`):

Variable arg count:

    static void %s(
      double ** data, int narg, int * datai,
      R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Fixed arg count:

    static void %s(
      double ** data, R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Maybe an option without control?  So two switches:

* `vararg`
* include `ctrl`

Additionally, if we allow different signatures then they need different names.
To the extent we re-use a single function multiple times, we want to detect
this.

Each code gen call should return:

* Function definition.
* Function name.
* Function call (since we can have different # of arguments).

We can check that if multiple instances of a function name show up, the
definitions are the same, and then unique them out.

# Compilation

## Cost

For a super simple compile:

```
$ time clang -g -c -O2 test.c -o test.o

real	0m0.115s
user	0m0.065s
sys	0m0.045s
```

But first time calling clang can take a long time (e.g. 6 seconds).  One
question whether adding R stuff slows down the compilation.  Doing it with `R
CMD SHLIB` but then obviously gives us access to a lot more options:

```
time R CMD SHLIB test-r.c
clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -fno-common -std=c99 -pedantic -Wall -Wextra -c test-r.c -o test-r.o
clang -mmacosx-version-min=10.13 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o test-r.so test-r.o -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation

real	0m0.361s
user	0m0.235s
sys	0m0.112s
```

Also, nice thing about R CMD SHLIB is that it abstracts away calling the
compiler (on windows)?  Oddly, calling the plain c file is slower!  Well, at
least not faster, so the overhead is from firing up make, etc.

> Note It's not actually slower as we need the extra step to generate the .so.

## Loading

### Hack approach

Ideally we would be able to use `dyn.load`, but then we need `.Call` which
obviously we're trying to avoid.  This is going to be really complicated, see
"src/main/dodotcode.c".

So we can use `R CMD SHLIB` + `dyn.load`, but we now need to figure out how
`.Call` finds the symbol in that case.

    > xx <- dyn.load('test-r.so')
    > xx
    DLL name: test-r
    Filename: /Volumes/PERSONAL/repos/fapply/test-r.so
    Dynamic lookup: TRUE
    > .Call('test', runif(10), PACKAGE='test-r')
    [1] 6.270841

Looks like maybe we can use (cadged from `resolveNativeRoutine`):

    DL_FUNC *fun;
    char buf[MaxSymbolBytes];
    R_RegisteredNativeSymbol *symbol;
    R_RegisteredNativeSymbol symbol = {R_CALL_SYM, {NULL}, NULL};
    *fun = R_FindSymbol(buf, dll.DLLname, symbol);

**Bad News**: we need `Rf_RegisteredNativeSymbol` which is in Rdynpriv.h, which
is not installed, etc.  We can just dummy it since it is only used optionally.

Which is used in (where `*fun` above is loaded into `&ofun` below):

    args = resolveNativeRoutine(args, &ofun, &symbol, buf, NULL, NULL, call, env);
    retval = R_doDotCall(ofun, nargs, cargs, call);

All of these seem to be in "R_Ext/Rdynload.h" so I think we're okay (though not
strictly part of the API?).

### `R_GetCCallable`

An alternative we can use, but will likely require building a package like odin
does.

Is there a way to save the function and the compiled code other than via
package?  Maybe?


# vs Rcpp?

How is this different to Rcpp?  Very limited, but no learning curve.  Only
doubles.

