# r2c

# To Do / Questions

* `DYNLOAD` warnings around `run()`; maybe that function doesn't need to be of
  the type DYNLOAD?  We know what the interface of it is, so we just need to
  check whether that's compatible with R's compilation mechanism.
* Test that everything works if there is no group varying data (i.e. everything
  fed through MoreArgs).  Not completely trivial to test as we need a function
  that allows us to specify a `data` parameter, but then uses it as a control
  instead of data (at least for `group_exec`).  We just need the group varying
  data to be referenced somewhere in the call, which 
* Recycle warning for windows, does it make sense?  It might with the partial
  windows, but for the main section we should only need to check once?
* Be sure to test a window that is wider than the data to see that partial
  windows are computed correctly on both sides?  Should be okay.


## For CRAN

* Image links from README should be to raw github URLs.
* Figure out why winbuilder doesn't work.
* Errors in `match.call` that show the deparsed argument might deparse
  differently on different systems (`group_sum` error tests).
* Add iterator breaks?

## Other

* Special case of constant result lengths could be an optimization for group
  where we don't have to generate the vector, although there should always be
  few groups relative to vector elements, so complexity not worth it?
* Migrate to tapply structure.  More generally, explain interface?
* Allow more then `INT_MAX` groups (labels are limiting right now)?
* What if compilation fails, do we have good error messages?
* Don't use double for group sizes unless we have groups that are too large.
  **probably not worth it** b/c the group sizes result is only as big as the
  number of groups, so very small in the grand scheme of things.  First level is
  simply to check input vector size, and if less than `INT_MAX` use that, next
  step would be to switch from int to double if we do have a vector that exceeds
  the size limit (but then we need to test for that...).  Does that mean we need
  a limit to int?  Will be annoying to have logic for both.
* Should sorted be "assume.sorted"?  Probably.  Could add check that it is (see
  next).
* Process groups might be able to determine that things are sorted?  If things
  are always e.g. increasing, then it must be the case that they are sorted.
  Evaluate the overhead of this; should be low as it would just be done in the
  second pass that stores the sizes?  Something still needs to be declared as
  sorted though.  It's relatively cheap to check that it is, but not so cheap
  that we would always want to run that check.
* Perhaps we got too lazy with POW, do we need to handle all the other cases
  arith handles?  Maybe, but if we do we'll likely need our own `pow` function,
  as it stands right now we're likely to get non-identical results in the case
  of NA or Inf (or at least particular variations of them).
* Re-implement modulo.
* The `check=TRUE` might be better done with an explicit check function.
* For running function directly, can we pass data as an env so the traceback
  doesn't explode (but make sure this doesn't cause reference / refcount issues)?
* What is the implementation cost of allowing internal functions to advance the
  data pointers so they don't have to be manually moved after each function
  iteration?
* Implement sum64 and mean64 to use double accumulator (note we've tested that
  double accumulators are slower).
* We can no longer use `mean <- mean.default` and `r2c` at same time (this might
  be correct, but will make readme a bit more complicated).  Also, error message
  is very confusing in this case as `mean.default` is also from the base
  namespace, even though the masking is defined at the global level.
* Currently we require at least one data column so we can get away with using
  the group logic for the stand alone evaluation of the r2c funs.
* Add destructors that will unload the shlib and delete file file when the
  corresponding object is garbage collected (finalizer).
* Add check to `r2c` funs that verify the `r2c` version, the R version, and the
  platform (anything else)?
* Figure out a better way to store the object than embedding it with `bquote` in
  the function (e.g. retrieve from the call like `rlang`, but not sure that's
  better).
* Think about `df %>% group_by(g) %>% filter(x > min(df$x)`.  This should
  resolve naturally if we allow `[['x']]` as "normal" expression.
* Can we optimize label generation for cases where the last step is constant
  size or size of groups?
* Better introspection tools for intermediate products so we can see what they
  are.
* Are names sufficient to disqualify from nakedness?  How do names behave in
  recycling?  Do we want to allow other attributes to persist?
* Make sure to have a function that uses the `ctrl` functionality to check that
  it works properly (once we switch to `flag` for most functions).
* Can we get more memory efficient by e.g. using `+=` to avoid having to have
  memory allocated for both LHS and RHS for the arithmetic operators?  Might
  complicate things substantially.
* Make sure pointer arrays are aligned.  Use `malloc`?  If so internal code
  cannot use `error`, and we can't guarantee that?  Can we register a finalizer
  somehow (probably will need to).
* To allow assignments we can just use a series of nested environments such that
  each sub-call will then seek the symbol through the nested environments.
  We'll need to bind the data symbols to e.g. environments so they may be
  uniquely identified and we can confirm they've been found.
* Make sure headers go in in correct order; it may not work if we structure
  workflow around C functions?  Maybe okay if each group of functions does the
  right thing?  Do we need to include the headers before each group of
  functions?  Do we need to keep translation units independent (but the lose the
  benefit of static funs)?  This is almost certainly a real problem that needs
  to be addressed.
* Ensure all entry point names are unique and that there are no collisions.
  Maybe this can be done on registration?
* Special case where all groups are same size and there is an external vector of
  that size?
* Complex expressions with curly braces, etc.
* Unary arithmetic funs.
* Add post-processing of function result (e.g. to attach names, etc, as in
  `quantile`).
* Support for functions with defaults that need to be evaluated?  No.  This
  substantially increases complexity because we have to do so in the function
  evaluation environment and need access to all the other arguments.
* Side effects from evaluation of control parameters?  Where should assigned
  variables reside?
* Add capability to specify functions in non-standard ways (e.g. `base::sum`, or
  `(get("sum", as.environment("package:base"))`)?  Maybe not, that seems like
  potential trouble.  Certainly document also things like `f <- sum; f(x)`.
* Make sure there are interrupts.  Can we use "R_ext/Intermacros.h".  It seems
  yes generally, but they are not explicitly part of the API, and then there is
  the question of whether it makes sense to do so, or if we should just be doing
  this at the group level?
* Is it faster to compute complex expressions pairwise (i.e. full vector scan of
  two vectors), or do each row in full once?
* Look into using R_GetCCallable (see WRE) instead of what we're doing now.  It
  seems though that to do so we'll need to generate a full package which will
  further slow down the compilation.  What does `inline` do?
* Check name isn't used already.
* `?SHLIB` states some binary distributions don't include `R CMD SHLIB`.
* Figure out how to call the current version of R (e.g. `RD CMD SHLIB`).
* See how far back this works in terms of R versions.
* What happens when this stuff is serialized?
* Is there an alternative to R_FindSymbol?  Can we get it to be made part of the
  API.
* Can we use this in some way for e.g. really wide matrices where each column
  takes the place of an argument?  Generating a pointer for each column then may
  be less efficient.
* Does altrep play into this in any way?
* Lags and similar (i.e. `x[i] = x[i] * x[i - 1]`).
* Re-use repeated expressions.

## Done

* Tests groups that aren't just 1:n or 0:n (I think we do this)?
* Test on windows.  Instructions for windows users about toolchain, etc?
* Fill out acknowledgments.
* Rename `mean0` to `mean1`
* Benchmarks:
  * Randomly ordered groups?
  * Larger group sizes?
* Try to interfere with r2c funs.  Need to document exactly what environment
  they are exposed to (base namespace).
* Pass data around in environments for better deparse (can't do this because the
  parameters might be unnamed via dots, or at least not easily, plus it only
  matters for the naked call).
* Add tests:
    * For parenthesis removal in mechanics.
    * For square tranform.
* When we dropped the use of env to match the arguments, how did keep track of
  the environment to look up functions in?  We don't, we just check function
  validity at run time.
* Emit warning/errors with the function call.
* Can we come up with a better error message for the matching of arguments?  Do
  we need our own matching instead of relying on `match.call`.
* Check dots with names (i.e. names that don't match formas but are caught by
  dots).  What should be done with these?  They probably should be unnamed.
* Check dots with multiple sets of dots, trying to e.g. pass both to group
  varying and flags/control params.
* Can we truly support dots (I think so, see dots section).
* Does arg order matching make sense?  We get: `function (y, x, ..., na.rm)` out
  of `r2cq(y - sum(x, na.rm = na.rm, ...), check = TRUE)`, dots get moved up,
  doesn't seem right.
* Are we checking that the functions resolve correctly at run time (and not just
  at compile time)? Yes, we check in `alloc`.
* Test that dots still work after we changed the eval environment; almost
  certainly won't so we have to think how to handle things that get matched to
  e.g. `..1`, etc.
* How can we efficiently warn of "longer object length is not a multiple" in
  vecrec?  We don't want a modulo for each group.  But we could check that the
  shorter object has not hit its length at the end of the loop and set a global
  variable?  Or use `flags` to communicate back?  When we compute each
  interaction size, we do (or could know) the group sizes, including possibly
  max and min group size.  But to be certain of a multiple you must be certain
  that there are no groups of odd sizes...  So I think it has to be done at run
  time, annoyingly.
* Do we need to handle the issue of `envir` for `match.call` at allocation time
  instead of at compile time?  Seems like the way this goes wrong is if the
  expression being looked at itself contains dots, e.g. literally `sum(...)`
  where the `...` need to be fetched.

    > (\(...) r2cq(sum(...)))(1, 2)
    Error in match.call(definition = definition, call = call, envir = envir,  : 
      ... used in a situation where it does not exist

Right now preprocess is just using `parent.frame()` for match.call, which
doesn't make any sense.

* Check assumption that double will hold `R_xlen_t`?  Or that length is no
  longer that the allowable size?  A bit tricky; no way to know what double size
  is.  This is because we return the group size to R; otherwise it would just be
  an R_xlen_t vector.  We need this to get the group max for the allocation.
  It's possible R guarantees this will be no bigger than e.g. 2^53 or some such.
  We added a check to assumptions.c.
* Ensure that all pointer parameters are allocated one extra element at end so
  that we can use the fun(++pointer) pattern without worrying about the last
  call overflowing (I think this might be allowed by the standard anyway,
  check).  Yes, allowed by C99 6.5.6 p8.
* Is it possible that set.seed could interfere badly with the random file name
  generation?  Yes, maybe try to initialize the pool
* Evaluation of non-compilable expressions in an env child of an env with
  appropriate symbols protected?  Such would be the symbols in the data, but
  what about the functions that are used?  That seems excessive.  No, we won't
  support this.
* Think though corner case of R_NA, NaN, Inf, etc: are we preserving semantics.
  Yes, mostly, but maybe not always (e.g. when we use `pow` instead of `square`)
* Look into GraalVM, Renjin?
* Only sort the columns that are used.  This is the case already as only things
  that match to the `r2c_fun` are submitable.
* Be sure to test situations where we have external data larger and smaller than
  group sizes.
* What do we do with NA groups?  Each it's own, or one big NA group?  One big NA
  group.  Single group; now documented.
* Document that we assume IEEE-754 (and thus existence of infinity and no
  overflow on conversion from long double to double), and check whether this is
  a reasonable assumption under C99 (probably no), or failing that under R.
* Figure out why external vectors are being duplicated.  Were seeing:

    Browse[2]> .Internal(inspect(alloc$alloc$dat[[5]]))
    @7fe16de86b68 14 REALSXP g0c0 [MARK,REF(65535)]  1 : 3 (compact)
    Browse[2]> .Internal(inspect(alloc$alloc$dat[[6]]))
    @7fe16de8b7b8 14 REALSXP g0c0 [MARK,REF(65535)]  1 : 3 (compact)

  for the slope calculation with y as 1:3 (literal).  Duh, is this just because
  we use `as.numeric`.

* Really need to figure out whether we want a formal interface to the functions.
  It could be auto-generated from all the unbound symbols like gsubfn does it.
* Annotate code with the call that it corresponds to

# Window Implementation

## r2c

Advance: group size -> by
Length: group size -> window

Additionally, need to handle cases where the window overflows and fill it.

Currently we advance the data pointer by group size, but instead we want to
advance by 'by'.

So we need to split the `grp_lens` into those two different things, that are the
same for the group mode.

Additionally, we need to add support for the recycling of the indices.  For
simplicity we could start by recycling them in R?  Not ideal as it creates big
vectors that are completely unnecessary for the default case of scalar offset
and scalar window.  And these vectors are large.

Need to ensure that we have nothing in the offset vector that causes a problem.

Likely want a special branch for the scalar-scalar default case.

Currently the code assumes that `g_lens` is equal in size to the number of
offsets, which certainly won't be the case if we're recycling.  This also means
that the looping is completely different because for the window case we're
recycling offset and group size until the time we run out of data.  So we're
going to have a double nested loop with a break.

We need a check that the final result is scalar as it's not guaranteed ex-ante?
We could guarantee it ex-ante, but that's dangerous if we're wrong (but then all
our size calculations would be wrong anyway).

What if we allow non-scalar results?  Probably immediate complication is that we
can't just go straight to "NA" result when windows are OOB.  If we do resolve
that, we need to do a first pass pre-computing the result vector size.

For now focus on developing the scalar step and scalar window size, and
recognize the possibility for:

* Vector of steps.
* Vector of window sizes.
* A location vector to allow data to have different positions than their rank
  in the input vector.

## For Benchmarks

* slider
* data.table
* zoo rollapply


## Others

### data.table

#### Key Points

* Slow and fast algorithm
* Adaptive width

#### Details

https://github.com/Rdatatable/data.table/pull/5441#issuecomment-1236104263

### Slider

#### Key Points

* Irregular series can still be windowed when providing an index.
* Uses segment trees for the fast calculations to reduce cost from O(n^2) to
  O(nlogn)
* Uses before/after instead of left/center/right for more flexible window
  computation.

#### Details

Interesting mention of "segment trees".  This allows to partially cache sums by
pre-aggregating the vector in a binary fashion, and then re-using the aggregates
when possible.

Also interesting that instead of using "left", "right", etc., it uses before and
after.

Would it work with variance?  What does it generalize to?

    sum((x - mean(x))^2)

Default is to return a list.

Concept of index column for irregular series.

Referenced paper also has a few other interesting topics:

* Partitions, where you compute the sliding function within each partition
  independently.
* Variable size windows, where you cannot easily deal with removal/addition
  window because you don't know how much you need to remove/add and doing so
  could require scanning the entire thing again.

We can compute the variance of two buckets from the individual stats according
to "Maintaining variance and k-medians over data stream windows." (referenced in
[1])

    n<i,j> = n<i> + n<j>
    u<i,j> = (n<i>*u<i> + n<j>*u<j>) / (n<i> + n<j>)
    V<i,j> = V<i> + V<j> + (n<i>*n<j> / n<i,j>) * (u<i> - u<j>)^2

So it should be possible to derive the equivalent formulas for most statistics
for combining buckets, but there might not be an automatic way of doing so given
a function (hard to believe there would be looking at the above).  Once we have
the statistics we can use segment trees to govern how we combine things.

So ultimately this business comes down to how much it costs in common use cases
to re-run the entire function.

[1]: https://www.researchgate.net/publication/221559611_Variance_estimation_over_sliding_windows/link/56a2194208ae2afab885c5ba/download

### runner

Doesn't seem to add too much interesting.

# Interface

## apply

Evaluating whether we should shift to `tapply`.

    function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)

What would that do?

    r2c_tapply(x, g.r2c, r2c_sum, ...)

Is it okay to have `MoreArgs` as dots?  Probably, the issue being conflict with
additional parameters, which in this case is just `enclos`.  In terms of
implementation we would just capture them in a list and pass them on.

Names?  We want to add window functions, so do we end up with:

    r2cgapply
    r2cwapply

Or

    gapply
    g_apply    # makes it clear not base R
    wapply
    w_apply

Another option

    ctapply
    c_tapply
    cwapply
    c_wapply

Should the name communicate in some way that these only run with `r2c` funs, or
do we wish to allow them to run with other functions?

By extension we should probably do:

    csum <- r2c(sum(x))

The misleading thing for that one is `sum` is also essentially just C code, but
for the more complicated cases that makes more sense.

## tapply

Semantic incongruities:

* We accept lists as the first parameter, so not based on "split" method.
* Functions can accept multiple parameters, and they are matched into `X`.
* Result is neither array nor list, always simplified into a vector or a
  data.frame.

`tmapply`?

    tmapply(FUN, INDEX, ..., MoreArgs)

Main issue here is what if we want to pass a data frame in ...?  This would
require always specifying each element individually.  Additionally,
if we have `MoreArgs` we're required to type it out.  We could use:

    tmapply(FUN, INDEX, DATA, MoreArgs)

This is pretty close.  Or maybe:

    tmapply(FUN, INDEX, DATA, ...)

Although same issue of wanting to pass a list in ..., and additionally name
overlap with other parameters.

    grapply(FUN, INDEX, DATA, MoreArgs)
    grapply(FUN, INDEX, DATA, ...)
    grapply(FUN, INDEX, X, ...)

Dots are appealing, except for the potential for name collision.

    grapply(FUN, INDEX, DATA, ...)
    group_apply(fun, index, data, MoreArgs=list())

    grexec(FUN, INDEX, DATA, ...)  # too close to gregexec?
    group_exec(fun, groups, data, MoreArgs=list())

    window_exec()
    window_apply()
    wapply()

Do we need to provide `enclos`?  Probably not.

## NSE

Things like:

    tcapply(
      iris[paste0('Sepal.', c('Width', 'Length'))],
      iris[['Species']], csum
    )

Are annoying.  This okay enough?

    with(iris, tcapply(list(Sepal.Width, Sepal.Length), Species, csum))
    with(iris, tcapply(l(Sepal.Width, Sepal.Length), Species, csum))

# Dots

## Trying to Get Dots Working normally

### Preprocess

Do we allow dots in `ctrl` and `flag`?  Maybe, they are not really used at all
in preprocess.  The only thing we see is computing of length of dots to
determine whether e.g. to use the multi-arg version of sum or not.  The only
thing we seem to preserve is the name of the argument, and it's not clear that's
actually used by alloc.

One challenge is how do we distinguish between these two:

    r2cq(sum(...))
    r2cq(sum(x, y, z))

In the former we want to literally match `...` against the dots argument,
whereas in the latter we want the `x`, `y`, `z` to be expanded out.  In a more
complex expression `...` might match more variables:

    r2cq(sum(x, y, z) + sum(...))

Produces:

    function(x, y, z, ...)

Consider for:

    function(..., na.rm=TRUE)

Things like:

    f(..., x, na.rm=y)

We need both dots and `x` to be matched to the actual dots.  So we need to
replace `...` in the call with e.g. something like `.R2C.DOTS`, which will be
recorded in the symbols as `...`.

### Alloc / Run

It seems here most of the work is done by the matching in `group_exec`.  Might
just work?

No, we need to expand out the dots in the linearized call and related data.
Seems right now the fields that need to be expanded are call, argn, depth, and
type, where we look for any `call` of type `leaf` that is `...`, and sub in as
many leaves as there are elements.  One issue to figure out is if it's a problem
that `argn` is just going to be "..." repeated.  Do we use this to match
anyplace?  We must for allocation.

`argn` is, what?  We're seeing:

     $ argn     : chr [1:4] "..." "..." "na.rm" ""

Not sure why there are two "...", although those do match to the `...` in the
formals of `sum`.  So `argn` is the argument name in the original.  But why do
we even use this in alloc?

Strongly suggests that we just cannot allow "..." to be matched to control
parameters?  Seems like we need to generate "..1", etc, to be orderly matched to
the data proper, and that data derived from "..." should get those names (unless
it already has names)?.

## Previous Notes

Do we allow dots in the call?  This could come up if there is an `r2c`
construction inside of a function.  But it's a bit silly because if we don't do
it inside a function we get a failure from dots not existing, but when we run
elsewhere we might not be inside the function anymore.

Let's say we allowed it, how would it play out?  Are we counting the args at
compile time (probably).

Should we disallow no free symbols?  Probably should be at least one, it
complicates logic a bit if we don't as we need to know the size of the result.

    f <- r2cq(sum(1, 2))
    f

Similarly, should this really work?

    > f5 <- (function(...) r2cq(sum(...)))(1:10, 2)
    > (function(...) f5(...))(1:10)  # okay
    [1] 57
    > sum(1:10
    + )
    [1] 55
    > f5
    function (..1)
    {

It's kind of an accident that we generate the `..1` symbol, and only because
`1:10` is a call:

    > (function(...) r2cq(sum(...)))(1, 2)
    function () 
    {

So probably just need to figure out why the call version generates the `..1`
parameter.  Has to do with the fact that for some reason dots generate `..1` for
calls, but not for constants.  So there is a weirdness that something that seems
like a constant (`1:10`) because R can't discern that it is and is in dots
generates a free symbol.

Maybe that's okay, and really what we need is a better error message when there
are no data columns, or a better way to manage that scenario.  When there are no
data columns all inputs are length 1 as that's the only way to truly generate a
constant?  Can we use this or is that begging for trouble?

We won't use that, we just require a free symbol.  There is the issue of
"constants' like 1:10 generating free symbols

Thinking about this a bit more on dots, all we need to do is make sure that the
code gen functions are aware of dots, and have code that can handle dots.  We
then need to make sure there are no issues of expanding or evaluating dots in
the compile stage, that functions that support dots get told they are getting
dots, and those that don't fail, and in the allocation stage the dots should
match gracefully to the data and everything should be great.

# Loop implementation

Loop variables can be created in a child frame to the data.

Each loop body will generate a `run` statement, e.g.:

```
run(...) {

}
run_loop_1(...) {
  for(intmax_t i = 0; i < len; ++i) {
    run(...);
  }
}
run_loop_0(...) {
  for(intmax_t i = 0; i < len; ++i) {
    statement_0(...);
    statement_1(...);
    run_loop_1(...);
  }
}
```

Probably want two types of loops, loops where we know the start and end as
constants or other derivable data (`seq_along`), and others where we have an
unknown vector?  Might not be able to allow the second type, because at that
point we can't know how large a vector that is assigned to with e.g. `x[i] <- y`
will get.

```
for(i in seq_along(x))
  y[i] <- x[i] + 1
```

Is the latter:

```
for(i in x)
  y[i] <- z[i] + 1
```

At the allocation stage we need to determine the size of `y`.  It's fine if
we're just referring to an external symbol (or even a group symbol) as we can
check the content sizes.  But things like:

```
for(i in rev(x))
```

Get messier.  Likely we just allow symbols, or `seq_along`, or constant
expressions like `a:b` where `a` and `b` are known at allocation time (changing
them inside loop is fine since):

> The seq in a for loop is evaluated at the start of the loop; changing it
> subsequently does not affect the loop.

But they can't be computed within the `r2c` code.

# Further Optimizations

## Overview

Thoughts to try to reduce the group overhead.

* How much overhead is just from `run`?
* Evaluate overhead from generating result labels.

It's actually significant: out of ~.9 secs for 1e8, .2 is spent on label
generation.  In this case we need to be careful due to hashing of labels.  We
can reduce this dramatically for the case we know that group.res.sizes are all
size one, and a little bit if we know they are all the same size.

We're actually getting a bit of overhead from generating the labels, so should
just generate them directly in C (and now we do).

It doesn't seem like order affects collapse much.

## Group Sizes

Group size computation is significant.  The need to do two pass is also not
super (group count, vs recording group sizes).  Group count is actually quite
fast because it can use vectorized instructions to increment the group counter
without the need for branches.

We also have the issue that we don't allow more than INT_MAX groups, and related
that we do allow R_xlen_t max group sizes (the former needs to be decided on as
a limitation, the latter needs to be decided on as a feature we want to keep).

Big question is whether we want to change the logic to avoid doing two passes.
We can do something that e.g. allocates 1/4 of the size (but it has to be for
labels and for group sizes), but if it fails will need to go to 1/2 the size or
larger.  Maybe start at 1/8.  There is an outer loop that checks if we hit the
last value in the loop, and if we did triggers running the inner loop again
after "growing" the vector (i.e. re-alloc, copying, unprotecting).

The other thing we could do is an analysis of the first 1000 elements to
estimate the group size, but this will fail miserably in the case where there is
a single e.g. 1K initial group, but then much smaller group sizes after.  So it
gets pretty complicated: you need to do that first 1K pass, assume e.g. smallest
group, and if you fail, fall back to old method.  If you succeed you're still
left with an over-allocated vector you likely need to subset or copy later
(unclear whether we are allowed to use "true length" for this, or whether we
want to manually generate the label vectors manually).

The pattern we're looking for is "increment counter on condition", which we can
use to increment a pointer.  But then we are going to need to write to write to
memory every iteration, which is probably worse.  So likely the best we can do
is reduce time by 1/3 by avoiding the initial group number counting, which seems
like a lot of complexity and work.  Instead maybe we really should have a
separate group computation.

## Notes vs Collapse

For a single stat like `sum(x)` collapse is more memory efficient because it
does not reorder the data vectors.  Also, while generating the radix sort
ordering vector is fast, the actual reordering of the vector is much slower.
Would be interesting to figure out why that is (actually this untrue, I was
remember the cases where we were hitting the memory limits with 1e8, it takes
the same amount to order as it does to re-order with the order index).

For multiple stats like in `slope(x)`, `r2c` is more efficient.

For smaller group counts, the hash algorithm from collapse seems much faster.

Idea: could we do something with only sorting the result at the end?  Seems like
it should be terrible to access the data vectors in an unsorted order though.

From analysis it looks like what's going on is that (unlike `GRP`), `fsum`
defaults to the hash method.  There are many pathways in the hash code, but for
our data it seems the most likely outcome is to end up in one where the hash
table size is the size of our input vector.

The hash function is just the value of the group index modulo the has table
size.

But for some reason this doesn't seem to have terrible corner cases.  Once we
get to small group sizes, we start underperforming the radix sort, but not by
massive amounts, and we're way faster for larger group sizes.

On my system this is the cost of accessing the vectors at random (well, this
includes the physical re-ordering):

> system.time(x[io])       # ordered
   user  system elapsed 
  0.052   0.003   0.057 
> system.time(x[ir])       # random
   user  system elapsed 
  0.286   0.007   0.311 

And most of the cost is just accessing the vector (`order_sum` below adds each
value to a single accumulator in the order specified by io).

> n <- 1e7
> x <- runif(n)
> io <- seq_len(n) - 1L
> ir <- sample(io)
> system.time(order_sum(x, io))
   user  system elapsed 
  0.022   0.000   0.023 
> system.time(order_sum(x, ir))
   user  system elapsed 
  0.210   0.006   0.230 

The key to collapse speed is that it reads the large input vectors in order, but
then writes to the small group vector out of order.

> system.time(fsum(x, g4, na.rm=FALSE))
   user  system elapsed 
  0.081   0.004   0.087 

This advantage falls apart as we grow the size of the group vector by increasing
the number of repeats:

> system.time(fsum(x, g6, na.rm=FALSE))
   user  system elapsed 
  0.865   0.017   0.910 

But exactly what is happening is not 100% clear:

    1e4 groups                             milliseconds
    fsum ------------------------------ : 98.79 -  0.00
        fsum.default ------------------ : 98.79 - 12.73
            qF ------------------------ : 86.06 -  0.00
                hashfact -------------- : 86.06 -  0.00
                    groupfact_sorted -- : 86.06 - 70.91
                        Csv ----------- : 15.15 - 15.15

    1e6 groups                             milliseconds
    fsum ------------------------------ : 872.0 -   0.0
        fsum.default ------------------ : 872.0 - 103.5
            qF ------------------------ : 768.5 -   0.0
                hashfact -------------- : 768.5 -   0.0
                    groupfact_sorted -- : 768.5 - 636.6
                        forder.int ---- :  81.8 -  81.8
                        Csv ----------- :  47.3 -  47.3

The main incremental cost is in groupfact_sorted, and we can tell from the C
trace that's almost all from `dupVecIndex` which is where each element of the
group vector is given the order of appearance of its group (which can be used to
map inputs to group positions, possibly even after re-ordering).

We're actually possibly faster if we take up the whole hash space, which
trounces the theory?  Eh, maybe not, we're still only loading 1e3 cache lines,
which actually will fit in L1 cache.

    all.range <- as.integer(seq(1, n, length.out=1e3L))
    g4.ar <- sample(all.range, n, replace=TRUE)
    treeprof::treeprof(fsum(x, g4.ar, na.rm=FALSE))
                                           milliseconds
    fsum ------------------------------ : 80.52 -  0.00
        fsum.default ------------------ : 80.52 -  9.25
            qF ------------------------ : 71.28 -  0.00
                hashfact -------------- : 71.28 -  0.00
                    groupfact_sorted -- : 71.28 - 54.77
                        Csv ----------- : 16.50 - 16.50

If this is true it should be the case that contiguous should remain fast at
higher sizes.  Let

    n <- 1e7; ng <- 1e3
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.069   0.003   0.072 
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.064   0.004   0.068 


    n <- 1e7; ng <- 1e4
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.132   0.003   0.136 
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.077   0.003   0.080 

Let's try with different types to see what happens:

    n <- 1e7; ng <- 1e4
    set.seed(1)
    x <- runif(n) * runif(n)
    all.range <- as.integer(seq(1, n, length.out=ng))
    g <- sample(ng, n, replace=TRUE)
    g.ar <- sample(all.range, n, replace=TRUE)
    gt <- as.character(g)
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.373   0.020   0.401 
    gt <- as.character(g.ar)
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.412   0.016   0.435 
    gt <- g + 0
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.621   0.030   0.668 
    gt <- g.ar + 0
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.693   0.027   0.730 
    gt <- factor(as.character(g))
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.032   0.000   0.033 
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.085   0.007   0.094 
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed 
    ##  0.763   0.020   0.802 
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed 
    ##  0.142   0.016   0.160 

Pretty much as expected.  Big benefit from factors.  So really the hashes work
very well with sequential integers, whereas the sorting version should be about
the same for everything?.

    system.time(order(x))
    ##   user  system elapsed 
    ##  0.587   0.026   0.620 
    system.time(order(g))
    ##   user  system elapsed 
    ##  0.201   0.005   0.208 
    gt <- as.character(g)
    system.time(order(gt, method='radix'))
    ##   user  system elapsed 
    ##  0.271   0.006   0.281 

Largest number of groups that will fit in L1 would be 16K, not enough to start
evicting out of L2 for the all range one.  To do that we need 32K entries.  At
64K entries we should still fit adjacent in L2.

    n <- 1e7; ng <- 2^16
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed 
    ##   0.358   0.008   0.371 
    system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed 
    ##   0.136   0.004   0.140 

This suggests a massive crash in performance as for all range half will have
been done in L2, and half out.  We can go up to 500K entries contiguous before
we start exhausting L2 cache, so let's try with 250K groups:

    n <- 1e7; ng <- 2^18
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed 
    ##   0.599   0.010   0.616 
    system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed 
    ##   0.343   0.007   0.353 

**We're assuming no hash collisions whatsoever**

Hmm, not what I expected.  For this case we should be needing 16MB worth of
cache lines in the ar case, but only 1MB of cache for the contiguous case,
thinking about the hash alone.  But there are other memory accesses too:

* Seq - `iid <- px[i]` read entire vector sequentially.
* Rand - `hid <- h[iid]` read the hash per the input vector.
* Rand - `px[hid-1]` read input per hash value for first obs of group.
* Seq - `pans_i[i]` write

Basically, how much of the hash table can we keep in cache given competing
demands from the sequential reads and writes, and the random accesses.  So we
need a measure of age of the oldest hash access as a function of the number of
entries read?

For the 1e3 groups case, let's say on average we hit every group every thousand
reads (going to be more than that, but it's all probabilistic), then we're
using:

* 1000 x 2 x 4 (sequential integers, read and write).
* 1000 x 64 (random integers, but focused on the front part of the input)
* For the hash acces:
    * 1000 x 64 for the spaced out case
    * 1000 x 4 for the normal case

Or:

    x * 2 * 4 + x * 64 + x * 64  == x * (8 + 64 + 64) == 136 * x
    x * 2 * 4 + x * 64 + x * 4   == x * (8 + 64 + 4)  ==  76 * x

Second number is questionable, and also whether we can assume we're going to hit
every group value after ng writes and thus avoid eviction.

One conclusion from this is if we know the range and we know we won't have
collisions in the hash, we can dramatically reduce our cache utilization by
avoiding the random read back from the input vector to check whether the hash
hit is a match or a collision.

The most questionable number is the second one.  Not quite sure how to think
about the fact we're only reading from the section that contains every first
instance of a group, and at what point we start overlapping in cache lines.
Probably not a factor at 1e3.

So, per these numbers, in the contiguous case we would start thrashing L1 at 826
groups, and L2 at 27.6K, vs 481 and 15.4K.

    n <- 1e7; ng <- 1600000
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    system.time(fsum(x, g, na.rm=FALSE))

Empirical tests:

     ng   all  cntg
    480  .086  .076
    826  .081  .074
   1600  .088  .076
   2400  .115  .072
   4800  .151  .074
   5200  .134  .081
   7500  .143  .091
  15000  .178  .097
  25000  .195  .124
  35000  .340  .148
  50000  .393  .156
  75000  .446  .192
 100000  .496  .200
 200000  .636  .349
 400000  .807  .549
 800000 1.040  .821
1600000 1.208 1.081

Empirical data suggests a 5-6x cache utilization advantage for contiguous.

    x * 2 * 4 + x * 64 + x * 64  == x * (8 + 64 + 64) == 136 * x
    x * 2 * 4 + x * 64 + x * 4   == x * (8 + 64 + 4)  ==  76 * x

So if we drop the middle term:

    x * 2 * 4 + x * 64  == x * (8 + 64) == 72 * x
    x * 2 * 4 + x * 4   == x * (8 + 4)  == 12 * x

We get very close, and also this matches the key break points, with 5.4K and 1K
entries for L1, and 175K and 30K entries for L2.  This matches the timings very
well.  Don't understand why the reads back from the input vector (which should
require whole cache line loads) don't seem to figure in this math.  Actually,
it's likely because of things being concentrated in the beginning of the vector
as speculated:

    > z <- 10000; sum(1:5000 %in% g[1:z])/z
    [1] 0.4358
    > z <- 10000; sum(1:5000 %in% g[1:z])/5000
    [1] 0.8716

So most initial group instances are going to be very concentrated, with only a
few stragglers.

Tradeoffs are:

* Only access ooo the smaller group vector (we would need to do this for every
  input vector).
* Only need to do the very slow stop once.

What's still not clear is why the magnitude of the slowdown is so large for
`groupfact_sorted` (`dupVecIndex`).  Most likely once the group vector gets
large enough we start trashing cache.  E.g. at 1e6 group size, we're at 2x cache
(4MB).  But that should affect both the generation of the hash as well as the
writing to the group vector.  It does, in fact the writing to the group vector
proportionately becomes much slower (assuming that's the unattributed time in
`fsum.default`, we see  271:51 (5.3:1) vs 501:160 (3.13) from the trace, so
increasingly the sum access becomes a bigger issue.

Running some timings on a larger memory system (16GB, 4MB of L3 cache).  Here
we're dealing with group sizes that in the contiguous case just fit in cache at
1e6 * 4 (n/100):

    set.seed(1)
    n <- 1e8
    x <- runif(n) * runif(n)
    g <- sample(ng, n, replace=TRUE)
    library(r2c)
    library(collapse)
    f <- r2cq(sum(x))

    ng <- n/10

    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed 
    ##  7.421   0.477   7.919 
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed 
    ## 12.264   0.580  12.888 

    ng <- n/100
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed 
    ##  7.174   0.571   7.766 
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed 
    ##  5.101   0.401   5.520 

    ng <- n/1000
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed 
    ##  6.512   0.543   7.077 
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed 
    ##  1.280   0.184   1.471 

    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed 
    ##  3.672   0.445   4.131 

At n/1000 we're talking 400KB contiguous or 6.4MB, so for the latter just 50%
overflowing L3 cache.

    ng <- n/100
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed 
    ##  6.877   0.590   7.566 

At 2n with n/100 we're 50% in cache for the contiguous case.  Maybe should try
the n/10 case with g.ar?

    set.seed(1)
    n <- 2e8
    ng <- n/100
    x <- runif(n) * runif(n)
    g <- sample(ng, n, replace=TRUE)
    ## system.time(res <- group_exec(f, g, x))
    ##    user  system elapsed 
    ##  18.133   1.306  19.516 
    ## system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed 
    ##  10.954   0.814  11.783 
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    ## system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed 
    ##  17.222   1.158  18.458 

## Notes vs data.table

Not entirely sure what's going on with `data.table`.  For the 10e7 case we're
seeing (run 10x and profiled with instruments, cleaned up):

    2.87 s            0 s	  R_doDotCall
    2.41 s      967.00 ms	   gforce
    1.22 s            0 s	         R_doDotCall
    1.22 s      302.00 ms	          gsum
    908.00 ms   908.00 ms	           gather
    11.00 ms     11.00 ms	           platform_bzero$VARIANT$Haswell
    222.00 ms          0 s	    Rf_allocVector
    354.00 ms   340.00 ms	   uniqlist
    71.00 ms    20.00 ms	   uniqlengths
    29.00 ms    29.00 ms	   DYLD-STUB$$INTEGER

Only ~1/10th of the time seems to be spent within gsum itself.

# Benchmarks

* 1e7 with group size 10, 1000.
* Pre-sorted
* sum(x), sum(x + y)

In this case mean did not get inlined, whereas the others did.  Oddly subtract
shows up three times, but still not inlined.

    # 3.01 s   10.2%	54.00 ms	                       run
    # 1.90 s    6.4%	1.75 s	 	                       mean
    # 153.00 ms    0.5%	37.00 ms                        R_finite
    # 166.00 ms    0.5%	166.00 ms                      multiply
    # 160.00 ms    0.5%	160.00 ms                      sum
    # 156.00 ms    0.5%	156.00 ms                      sum
    # 128.00 ms    0.4%	128.00 ms                      subtract
    # 125.00 ms    0.4%	125.00 ms                      subtract
    # 109.00 ms    0.3%	109.00 ms                      sqr
    # 105.00 ms    0.3%	105.00 ms                      subtract
    # 96.00 ms    0.3%	96.00 ms                       divide
    # 6.00 ms    0.0%	6.00 ms	 	                       DYLD-STUB$$R_finite

We tried only having two calls to mean, but there is something preventing
inlining, and non-inlining is expensive.  Maybe R_FINITE prevents inlining?
Although that's weird b/c from timings, it seems the inlining stopped happening
after we revised the interface to have all the parameters.

`mean0` does show non-inlining.  Simplifying parameter structure does not seem
to allow `mean` to become inlined.

To conclude, there is no evidence that `mean` was ever inlined in the earlier
tests.  This was a whole big red herring for a performance "regression" that was
really just accidentally computing on `x` as integer instead of `numeric`.

Comparing to `fsum`, `fsum` appears to have less overhead (as it should), at
group sizes of 1000 its performance is comparable to `r2c`, at group sizes of 10
it is twice as fast (i.e. overhead for r2c is about what it takes to compute 10
additions).

It appears `fmean` runs a single pass.  Additionally both `fsum` and `fmean` use
64 bit accumulators, which performance wise is not an advantage on my CPU
(looking at timings at group size of 1000).  It's actually a small disadvantage.


# Done / Answered

* Make sure that no non-numeric data exists other than the stuff going into
  control.

Yes, we only append the numeric data.

# Other Implementations

## ast2ast

https://github.com/Konrad1991/ast2ast/

## sph-sc

Scheme to C via @wdkrnls (Kyle Andrews).

## armacmp

https://github.com/dirkschumacher/armacmp

## nCompiler

https://github.com/nimble-dev/nCompiler

## Graal

Main question here is how limiting the R-C boundary is.  Is it the case that
code such as `data.table` is not any faster?  Does that even run?  How many of
the packages actually work?  What stuff does not work?  Seems like there should
be a bit of stuff that doesn't per TK.


## Renjin

## Odin

Fascinating implementation, but the key issue seems to be that it seems to be
very much focused on the derivative syntax of DeSolve and thus abandons R
semantics.

Supports things such as (from array portion of vignette):

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ]))

But not:

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ] + 3))

I.e. it won't recursively construct the expressions.  But it can be done with:

    tmp[,] <- ay[i,j] ^ 2
    deriv(y[]) <- y[i] * (1 - sum(tmp))
    ...
    dim(tmp) <- c(n_spp, 10)   # probably can submit the 10 as user()?

Additionally, the concept of arrays is really a stand in for multiple variables.
In the above example `i %in% 1:4` so it's a four variable system and could be
written as such, not really the concept of vectorized data intended to be
aggregated.

We can get "vectors" in the sense we're used to in there, but they need to go in
as matrices (but that's okay, we can just do this with 1D matrices).

# Concept

1. Given a parsed R expression
2. Check that expression / function has symbols that resolve only to known
   functions.
3. Translate it into C code.
4. Compile it.
5. Feed it.

# What needs to happen

1. A compiled function that accepts N double pointers.
2. A manager to call the compiled function repeatedly with variations.

# Features

1. Standard arithmetic.
2. Sum. Mean.
3. Assignment?  Probably not.

# Translation

## Memory

* Calling function will allocate a vector sufficiently large to hold the
  largest result and pass that to each computing function.
* Computing functions may only write to that vector?  Specifically, computing
  functions may not allocate?

## Length Computation

Each known function should have some method of ex-ante determining the length of
the outputs as a function of the lengths of the inputs (e.g. `quantile` would be
the length of the `probs` argument).

## Ex Ante Size Computation

We need some view of the size pre-compilation, but by virtue of allowing
references to external objects we can't resolve them fully.  So we have options:

* Group size
* Constant size (knowable at compile time)
    * Scalar (e.g. result of `mean(x)`)
    * Other constant (e.g. result of `range`)
* External, could be anything (zero, scalar, group size, whatever)

At compilation time we may be able to emit better code if we know if the inputs
are scalar / group size, vs if we don't we'll need a per-group conditional to
decided.  So the code generation needs to be able to run with the partial
information.

So we need a pre-processing pass that does all the size computations, but not
the actual allocations, and then a second pass once the input sizes are known.

How do we handle something like:

    pmax(scalar, external, group, constant)

For "vecrec" probably need to resolve to the worst in order:

    scalar > constant > group > external > external_or_group

But we must always keep track of whether group is involved.

One big issue is we don't know until the evaluation stage what columns are going
to be group vs external because we haven't seen the data.frame.  So we can
compile highly efficient code if we know the situation, but less so otherwise.

Similarly, if we want to pick code based on the value of control parameters, we
need to evaluate them at compile time.  This severely cramps our style.  The
control param evaluation at compile time is probably okay.

To resolve the knowing the data columns, we have two choices:

* Force all data columns to be the same size (what about non-data params like
  `probs`)?
* Make the code capable of handling different lengths inputs.

For the latter, we could have the code react to each group, but it would be much
better if it could before running pick the correct one.  Probably not possible
without recompilation.

Some costs of not being able to do the full optimization:

    > n <- 1e7
    > x <- runif(n)
    > g <- sample(n)
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 0L))
       user  system elapsed 
      1.077   0.005   1.087 
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 1L))
       user  system elapsed 
      1.076   0.004   1.085 

These above are with `na.rm` as an explicit if/else, where group size is 1..

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed 
      1.030   0.004   1.040 

Here instead we assume `na.rm = FALSE` so no extra if/else.  For reference this
is what the above looks like with group size = 10:

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed 
      0.778   0.004   0.788 

And finally back to group size = 1 we now add a `asInteger(VECTOR_ELT(x, 0))` to
retrieve the `na.rm` flag:

    > sys.time(res <- run_group1(shlib, "fun5", list(x), g, 0L))
       user  system elapsed 
      1.147   0.002   1.149 

So a 10% penalty, which suggests some value in allowing for a single flag to be
passed down.  But maybe this becomes a future feature.

For arithmetic, there is seems to be very little value in having the simplified
logic option (in fact, it looks slower, `fun7` is the one that goes straight to
the equal length loop).  So no huge cost.  At least when there is no contention
with any other processes.  Given that sorting is such a big part of the cost,
it's going to be hard to beat that.

    > sys.time(res1 <- run_group2(shlib, "fun6", list(x, y), g))
       user  system elapsed 
      1.058   0.007   1.071 
    > sys.time(res2 <- run_group2(shlib, "fun7", list(x, y), g))
       user  system elapsed 
      1.069   0.008   1.084 
    > sys.time({
    +   o <- order(g)
    +   xo <- x[o]
    +   yo <- y[o]
    + })
       user  system elapsed 
      0.783   0.005   0.794 
    > sys.time(res3 <- xo + yo)
       user  system elapsed 
      0.022   0.000   0.022 
    > all.equal(res1, res2)
    [1] TRUE
    > all.equal(res1, res3)
    [1] TRUE
    > 

## Allocation

Once we get to the run stage, we'll have for each sub-call, the type size of the
result, and the inputs, and the type of function.  This means we can resolve the
size requirements to a specific number for external things, and for external or
group we now know the "external" part of it as well.  Since we'll know the max
group size, we can run through?

To do this properly we need to know which column each thing refers to, but we
should be able to do it linearly.

So start going column-wise through our size matrix.  First item should be
standalone (i.e. not a call with arguments).  Add it to the allocation (unless
it's data, in which case it is already there), evaluating first if needed, and
recording the position in the data as well as the size (possibly NA if group).
Keep accumulating this info in a stack until we get to a call, which will then
reduce it down to a single value, and proceed accordingly.

For each item, we need to know what it is:

* Control (evaluate, store, and record index)
* Symbol external (evaluate, store, record size and index)
* Symbol data (record index)
* Expression (recurse, record size and index)

## Simple arithmetic

Compute length of each vector.  Generate the full C expression with index or
pointer offset access.

    x <- runif(5e6)
    y <- runif(5e6)
    z <- runif(5e6)
    w <- runif(5e6)
    >
    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed 
      0.025   0.000   0.025 
    > sys.time(fapply:::test2(x, y, z, w))
       user  system elapsed 
      0.018   0.000   0.018 

It is slightly faster to do x + y + z + w (test2) than do pairwise additions
across the full vectors (test1).  Maybe because we only increment the loop
variable once?

With `MOD_ITERATE_CHECK2` and checking every 1e6:

    > sys.time(fapply:::test3(x, y, z, w))
     user  system elapsed 
    0.036   0.000   0.038 

Interestingly `MOD_ITERATE2` is no faster, so the check is basically free.

`R_ITERATE_CHECK` is comparable to pairwise addition (test1):

    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed 
      0.026   0.000   0.028 
    > sys.time(fapply:::test5(x, y, z, w))
       user  system elapsed 
      0.026   0.000   0.028 


## Function Types

* Aggregation functions (result length == 1)
* Vectorized functions  (result length == 0 or longest vector)
* Arbitrary functions   (result length is known (above is degen case of this))

Maybe use iterator macros.  E.g. `MOD_ITERATE2_CHECK`.  These should work fine
and are fairly efficient (i.e. reset counter to zero intead of reading with
modulo).

When construction the expression, when do we need to recursively evaluate
sub-expressions?  E.g. in:

    x + y + z

Which is really:

   `+`(`+`(x, y), z)

How do we know we can just turn it into:

    for(i = 0; i < len; ++i) x[i] + y[i] + z[i];

Vs

    x + y + mean(z)
    x + y + sort(z)

It's really that there is no mixing moving of the vectors.  Associativity /
commutativity shouldn't matter as that's handled by C / parentheses.

If we have a complex expression with a lot of these then it potentially gets
tricky b/c we start requiring intermediate storage for computing the whole
expression.

So we need to distinguish between "vectorizable" functions that operate one
element at a time, and those that aren't that operate on more than one element
at a time (either by moving them, etc).  The latter have to be evaluated
separately.  So the "parser" will need to identify where such functions are and
then evaluate/reduce them to a form that can then be used in a vectorized
context.

Because of this we might just favor the iterative resolving of the expression.
This will be easier.  We could just have logic to try to make the biggest
vectorized expression.  This would avoid a lot of extraneous code.

So right now:

    x + y + z

Becomes

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];
    for(i = 0; i < n; ++i) res[i] = res[i] + z[i];

And:

    x + y + mean(z)

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];

    double tmp = 0;
    for(i = 0; i < n; ++i) tmp += z[i];
    tmp /= n;

    for(i = 0; i < n; ++i) res[i] += tmp;

So we'll need as many temporary scalars / vectors as there are arguments to
functions.  Maybe we don't allow anything but binary?  Hmm, so with:

    pmax(-x, log(y), mean(z), w - u)

In all cases we'll know the size each argument.  In the expression above we know
how many parameters there are so we can construct the correct code at
"compilation" time.  What about:

    pmax(pmin(y, z, w), pmin(w, q, f), mean(z + y))

Generated code:

    double * tmp0 = R_alloc();
    for(i = 0; i < n; ++i) {
      tmp[i] = z[i] + y[i];
    }
    double tmp1 = mean(z + y);

    for(i = 0; i < n; ++i) {
      tmp0 = pmin(y[i], z[i], w[i]);
      tmp2 = pmin(w[i], q[i], f[i]);
      res[i] = pmax(tmp2, tmp3, tmp1);
    }

And (numbers show depth):

    pmax(pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y))
         pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y)
                    mean(w)                  mean(z + y)
                                                  z + y


Generated code.  Coords are (depth, param)

    // coords (0,0),(1,0)(2,2)
    double stmp0;
    stmp0 = mean(w);

    // coords (0,0),(1,0)
    double * vtmp0 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(y[i], z[i], stmp0);

    // coords (0,0),(2,0)
    // * stmp0 no longer needed
    double * vtmp1 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(w[i], q[i], f[i]);

    // coords (0,0),(2,2),(3,0)
    double * vtmp2 = R_alloc();
    for(i = 0; i < n; ++i) vtmp2[i] = z[i] + w[i];

    // coords (0,0),(2,2),(2,0)
    stmp0 = mean(vtmp2);

    // coords (0,0)
    // * vtmp2 no longer needed, so re-use
    double * vtmp3 = R_alloc();
    for(i = 0; i < n; +++i) vtmp2[i] = pmax(vtmp0[i], stmp, vtmp1[i]);

We need to track how many variables we've generated and when each one is freed,
without garbage collection.  So for each variable we need to track the depth at
which they are used.

Ideally we'll know for each group exactly how many variables we'll need and of
which size.  So we want to allocate the minimal amount w/ re-use that we need.
Number of variables should be at worst `sum(pmax(length(args) - 1)` or some
such.

One problem is that computing ex-ante how big each of our intermediate
allocations is going to be could be costly.  For `pmax`, we have the group based
params that are known sizes, but there could be external params of unknown
sizes.  So we could reduce the size expectation to be something like
`max(c(G, a, b, c, ...))`.

Provided options:

* Length of group
* Constant length (mean/sum/prod->1; Max returns -Inf)
* Length of one specific argument (quantile)
* Recycled max of multiple arguments (different from plain max because if one of
  the arguments is known to be zero, then the lot is zero).

Not provided options:

* Function of length of single argument.
* Function of lengths of various arguments.

This means we only care about the maximum group size as we'll always want to
allocate to that group size at a minimum.

So we navigate the entire parse tree, and for each function we retrieve the type
from above.  This will require:

* Reducing the function to its numeric arguments.
* Looking up the type of function.
* Computing the maximum return size as a function of form `max(c(G, k))` where
  `k` is a constant presumably derived from external variables, or possibly 0.

For a single function:

* Confirm it is a known function by getting it and comparing it to our list.
* `match.call`.
* Identify all non-group parameters.
* Evaluate all non-group parameters and bind them to local symbols.
* Check that all the C parameters are double (either a group param, or double).
* Depending on function type
    * (if needed) Compute lengths of all non-group C parameters, and record max
      of them or zero in `K`.
    * Record shorthand size, e.g. `K`, `g`, or `max0(g, K)`, where `max0`
      returns 0 if any of `g`, `k`, are zero, and `K` is a scalar constant.
    * We thus need a structure that contains a constant, whether the constant is
      set (we need to distinguish zero), and whether the group size matters.

Across the full expression, given `g` is equal to max group size:

0. Initialize vector of temporary items.  This is an array of R_xlen_t sizes
   sorted by size and an array of integers denoting whether the corresponding
   entries are free or busy.
1. Confirm function is valid.
    * Either resolves against list.
    * Or does not contain any references against symbols in the grouped data.
2. Based on function type:
    * Invalid: STOP.
    * Valid no symbols: eval and return length as constant (SIDE EFFECTS?).
    * Valid referencing symbols: continue to 3.
3. Count function parameters that are of unknown length.
4. For each unknown parameter, recurse to 1.
5. Once all parameters are of known length, compute expression length.
6. Scan through free entries to see if any are big enough.
    * For entries with group size, use the biggest group size.
    * An entry is free if its depth is greater than one below the current level.
    * If yes, mark as busy with the current depth.
    * If no, find the spot to add a new entry and mark with current depth.
    * Process of adding an entry should preserve sorting of list.
5. Return expression length computed in 5.

Now that we have the expression length, compute the result vector size.  It
should be either a constant per group, or `g`.

The top level return should give us the result size, and our list should give us
the set of intermediate vectors we'll need.

## Parameter Types

There are:

* Numeric standard vectors from data
* Numeric standard vectors external
* Non-numeric data (do we even allow this?  How do offset work)
* Non-numeric externals

For non-numeric data, an example would be if someone wants to run e.g. a nested
tapply on each group (this is silly, should just group on the interaction, but
whatever).

Some of these will be "control" parameters, which loosely speaking are those
that are used in the code generation phase, and aren't necessarily (but can be)
passed on to the compiled code.

Key question right now is what gets into the `alloc.dat` structure.  Presumably
it is anything that could possibly feed a C parameter that it intends to compute
on, and more specifically, one that could be fed by the result of a previous
calculation.  Our key restriction is that the functions must all return doubles
no matter what.

Or maybe it is any parameter that in theory could be fed by group data.  Which
in theory is any numeric parameter that is not explicitly made to be a control
parameter?  Yes, this seems to be the most manageable approach.  This also
implies that all non-control parameters must be numeric.

Do we allow non-numeric, non-control parameters?  Is there any use for them
other than grouping?  Maybe something designating a type of calculation to be
done?  But when do we ever want those to be the same length as the vector?

## Interface

### R Level

Each special function must be pre-registered with some mechanism to distinguish
which parameters are vector ones that will be fed, vs which one should be
evaluated immediately for dispatch decisions (e.g. `na.rm=TRUE`).

How does this work for a newly defined function at the R level?  Does it matter?

### C Level

#### Basic

Target function:

* Array of double pointers the same length as it expects number of arguments.
* Int with number of arguments.
* Array of R_xlen_t with lengths of each of the arguments.
* Array of R_xlen_t with offsets of each of the arguments.
* Double array of right size for result.

Should we have some structs pre-defined to hold the above?

Can the function check it's doing the right thing in terms of argument count?
Would it be better to have the arguments provided explicitly (in which case we
really want structs?  Should the "Array of double pointers" already be at the
expected offsets (maybe).  Do we want iterators like R provides to walk through
all the arguments?

Maybe we don't want the lengths of each of the arguments; instead, we want the
length of the result, and the modulo to read all the other arguments not of that
length?

should accept an expects arguments.  All functions should be like that.

Parameters such as `na.rm=TRUE` actually cause dispatch to a different
function.  So when we register a function we need to declare this?  I guess it
should be optional and we use it to recreate `mean`, etc.

#### Allow SEXP

What about functions that accept non-double parameters?  In particular, we'll
have expressions that reference external/evaluated things, which might not be
double.  If we want to allow these to be fed to our functions, our functions
need to be able to accept SEXPs.  This substantially complicates the interface;
are there cases where we would want to do this?

An almost example is `quantile`, where the `probs` param is likely fetched from
elsewhere.  Or `filter`.  Essentially anything where the configure param
resolves to an arbitrary number of possibilities and thus can't be reduced to
different entry points.  `findInterval` might be a better example.

Seems like we have two choices:

* Allow additional SEXP arguments (and identification of what these are).
* Add ALTREP so that all arguments can be SEXP.

The latter seems cleaner.  What happens with the result vector at that point?
Probably can't be ALTREP as we'll need to modify it.

How much overhead does ALTREP add?  There is a good example from Gabe about
implementing window functions.

So is this worth doing?  What is the advantage of doing so.  More familiarity
for the implementer of new functions?  Maybe for now we just pass through as a
`moreArgs` list.  So we end up with `double **` for the data arguments, along
with offset and length, which could include some vectors that are not-data, and
a list of extra arguments.  Does this obviate the need for a configure function?
Probably, it then happens in C.  The sad thing about this is we have to redo all
the configure business for each call.  Hmm.

Related, could this eventually be made to use the actual R functions?  The
summary functions are all in "src/main/summary.c" and those are simple, but the
problem is they are all static and dispatched to by `do_summary`.  And calling
the latter will be very challenging from outside.

# Pre-Processing

Once we have the expression parsed, do we want to link in pre-compiled entry
points, or do we just want to generate the whole thing in one piece of generated
code?  Probably start with one piece of generated code.

    sum(((x - mean(x)) * (y - mean(y, na.rm=TRUE))) + z)

What does this become?  Each sub-expression needs to be generated in linearized
code.  There will be a vector of allocated vectors for temporary use.  So we
need a list where each element is a step, and each step should include:

> We probably want a version for variable argumens (e.g. `pmin`, and one for
> fixed argument count).

* That data structure, which is an array of double pointers with:
  * Group data at the beginning.
  * Non-group double data next.
  * Temporary arrays.
  * Result array.
* An integer count of parameters into the data structure.
* An array of offsets into the data structure.
* An array of offsets into each vector in the data structure.
* An array of lengths of each vector in the data structure.
* A `VECSCXP` of all the control parameters.

So in C our earlier expression will look like (this does not include `ctrl`):

Variable arg count:

    static void %s(
      double ** data, int narg, int * datai,
      R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Fixed arg count:

    static void %s(
      double ** data, R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Maybe an option without control?  So two switches:

* `vararg`
* include `ctrl`

Additionally, if we allow different signatures then they need different names.
To the extent we re-use a single function multiple times, we want to detect
this.

Each code gen call should return:

* Function definition.
* Function name.
* Function call (since we can have different # of arguments).

We can check that if multiple instances of a function name show up, the
definitions are the same, and then unique them out.


# Compilation

## Cost

For a super simple compile:

```
$ time clang -g -c -O2 test.c -o test.o

real	0m0.115s
user	0m0.065s
sys	0m0.045s
```

But first time calling clang can take a long time (e.g. 6 seconds).  One
question whether adding R stuff slows down the compilation.  Doing it with `R
CMD SHLIB` but then obviously gives us access to a lot more options:

```
time R CMD SHLIB test-r.c
clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -fno-common -std=c99 -pedantic -Wall -Wextra -c test-r.c -o test-r.o
clang -mmacosx-version-min=10.13 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o test-r.so test-r.o -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation

real	0m0.361s
user	0m0.235s
sys	0m0.112s
```

Also, nice thing about R CMD SHLIB is that it abstracts away calling the
compiler (on windows)?  Oddly, calling the plain c file is slower!  Well, at
least not faster, so the overhead is from firing up make, etc.

> Note It's not actually slower as we need the extra step to generate the .so.

## Loading

### Hack approach

Ideally we would be able to use `dyn.load`, but then we need `.Call` which
obviously we're trying to avoid.  This is going to be really complicated, see
"src/main/dodotcode.c".

So we can use `R CMD SHLIB` + `dyn.load`, but we now need to figure out how
`.Call` finds the symbol in that case.

    > xx <- dyn.load('test-r.so')
    > xx
    DLL name: test-r
    Filename: /Volumes/PERSONAL/repos/fapply/test-r.so
    Dynamic lookup: TRUE
    > .Call('test', runif(10), PACKAGE='test-r')
    [1] 6.270841

Looks like maybe we can use (cadged from `resolveNativeRoutine`):

    DL_FUNC *fun;
    char buf[MaxSymbolBytes];
    R_RegisteredNativeSymbol *symbol;
    R_RegisteredNativeSymbol symbol = {R_CALL_SYM, {NULL}, NULL};
    *fun = R_FindSymbol(buf, dll.DLLname, symbol);

**Bad News**: we need `Rf_RegisteredNativeSymbol` which is in Rdynpriv.h, which
is not installed, etc.  We can just dummy it since it is only used optionally.

Which is used in (where `*fun` above is loaded into `&ofun` below):

    args = resolveNativeRoutine(args, &ofun, &symbol, buf, NULL, NULL, call, env);
    retval = R_doDotCall(ofun, nargs, cargs, call);

All of these seem to be in "R_Ext/Rdynload.h" so I think we're okay (though not
strictly part of the API?).

### `R_GetCCallable`

An alternative we can use, but will likely require building a package like odin
does.

Is there a way to save the function and the compiled code other than via
package?  Maybe?


# vs Rcpp?

How is this different to Rcpp?  Very limited, but no learning curve.  Only
doubles.

# Interface

Now leaning towards something like this:

```
group_exec(slope, data, groups, x=v)              ## remap
group_exec_(slope, data, groups, remap=c(x='v'))  ## remap
```

But what if the `r2c` object contains `data`, or `groups`?  Then it becomes
impossible to remap those.  We could make those reserved terms in the
compilation analysis.  How does that evolve as we add more functions that have
different parameter names?  Do we just lock up too much?  The alternative seems
to be:

```
group_exec(r2c_slope, data, groups)()
group(r2c_slope, data, groups)()
slide(r2c_slope, data, width, ...)()
r2c_slope()
```

Do we require parameters?  Yes.

```
group(r2c_slope, data, groups)(x, y)
r2c_slope(x, y)
slide(r2c_slope, data, window=5)(x, y)
```

Setting the following as default just won't work, since the lookup semantics are
to look in the function environment (and would cause the recursive lookup
issue):

```
function(x=x, y=y)
```

So we can't have a function that observes R semantics but then also has the
unmapped parameters. So things like this won't work:

```
group(r2c_slope, data, groups)()
```

But this is okay:

```
group(r2c_slope, data, groups)(x, y)
```

Since we don't have a way to declare the mapping order in a way visible to
users.  So even if the print method does something like:

```
group(r2c_slope, data, groups)
## function(x, y) <...>
```

When in reality it's:

```
## function(...) <...>
```

What does `r2c_slope` look like:

```
r2c_slope
## function(x, y) <...>
```

Hmm, but are we satisfied with the data/groups specification?

```
group(r2c_slope, data, data['g'])(x, y)
with(data, group(r2c_slope, x, y, g))
```

How about this:

```
group(obj, ..., group)

groupwise(obj, group, ...)
groupwise(obj, data, .(group), .(remap))
```

If this is the NSE interface:

```
groupwise(data, group, obj(x=x, y=y))
obj(x=x, y=y)

with(data, groupwise(obj, list(x, y), list(g)))

with(data, r2c_group(obj, list(x, y), list(g)))

r2c_group(obj, x, g)

set.seed(1)
x <- y <- runif(1e5)
y[sample(1e5, 1e4)] <- NA
microbenchmark::microbenchmark(sum(x), sum(y), mean(x), mean(y), times=10)

set.seed(1)
x <- y <- runif(1e5)
y[sample(1e5, 1e4)] <- NA
microbenchmark::microbenchmark(sum(x), sum(y), mean(x), mean(y), times=10)
## Unit: microseconds
##     expr   min    lq  mean median    uq   max neval
##   sum(x)   299   315   342    335   367   413    10
##   sum(y) 18441 18684 19587  18973 20461 22382    10
##  mean(x)   234   265   281    276   294   343    10
##  mean(y) 33009 33201 34449  33421 34584 38751    10

```

So we need:

* data
* groups
* r2c object
* remap


```
groupwise(data, group, obj, x=x, y=y)

groupwise(obj, data, group, x=x, y=y)
groupwiseq(obj, data, group, x=x, y=y)

run(obj, x, y)
```

This allows arbitrary objects in ..., and we can require that they are named.
Group can be a single vector, does not need to be named, or it can be a list.

```
obj(x, y)
objg <- with(data, group(obj, x, y, g))
objg <- with(data, group(obj, x=x, y=y, g))
objg(x, y)
```

If unnamed we can rely on the symbol.



These are very appealing for group/window because we can easily re-use the same
shared object, and additionally 

```
obj <- r2c(<expr>)
obj$run(data)                # intended for code with loops.
obj$group(data, groups)
obj$slide(data, size, ...)
```

Are `groups` quoted or as is?  Do we need:

```
obj$groupq(data, groups)
```

What if we want to remap args?

```
obj$group(data, groups)(x=z, y=v)
obj $
  map(x=z, y=v) $
  group(data, groups)

group(remap(obj, x=x, y=y), data, groups)
```

Or should we really do:

```
slope <- r2c(
  function(x, y) (x - mean(x)) * (y - mean(y)) / (x - mean(x))^2
)
group(slope(x=v, y=z), data, groups)

with(data, group(slope(v, z), groups))
```




```
group(slope(x=v, y=z), data, groups)
group(slope, data, groups)                               # no remap
group_exec(slope, data, groups)                          # no remap
group_exec(slope(.remap=c(x="v", y="w")), data, groups)
group_exec(remap(slope, c(x="v", y="w")), data, groups)

group_exec(slope, data, groups)       ## no remap
group_exec(slope, data, groups, x=v)  ## remap


run(slope(x=v, y=z))
slide(slope(x=v, y=z), window=window)

slope(x=v, y=z) |> group(data, groups)
```

What would this do?  Run normally?

```
slope(x=v, y=z)
slope$group(data, groups)(x=v, y=z)
```

Then? Do we need:

```
group(slope, data, groups)(x=v, y=z)
group(slope(x=v, y=z), data, groups)     # Possible with NSE, but not ideal.
group(slope, data, groups)               # would this run without remapping?
group(slope, data, groups)()             # Or should it be this?

slope |> group(data, groups)(x, y)

slope$group(data, groups)(x, y)

group(slope, data, groups, map=.(x=v, y=z))
```

Is this confusing?  Yes, the duality of `slope` being a function but also not is
just weird.

What's the benefit of defining as a function?  Makes it easier to re-map the
variables in use, but then the remapping is weird.

```
slope$group(remap(data, x=v, y=z), groups)
remap(slope, x=v, y=z)
```


Old stuff:

```
fapply(X, INDEX, FUN)

group_exe(data, groups, obj)
window_exe(data, size, just, obj)
for_exe(data, obj)
```
