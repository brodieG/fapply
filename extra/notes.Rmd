# Reuse

We want to hoist stuff out so that instead of:

    (.R2C_SUB_2 <- mean(y)) -
    (.R2C_SUB_1 <- mean(x)) *
      (sum((.R2C_SUB_3 <- (x - .R2C_SUB_1)) * (y - .R2C_SUB_2)) /
        sum(.R2C_SUB_3^2)
      )

We have:

    .R2C_SUB_2 <- mean(y)
    .R2C_SUB_1 <- mean(x)
    .R2C_SUB_3 <- (x - .R2C_SUB_1)
    .R2C_SUB_2 - .R2C_SUB_1 * sum(.R2C_SUB_3 *
      (y - .R2C_SUB_2)) / sum(.R2C_SUB_3^2)

So maybe:

* For every symbol, find out the point at which it has been defined.
* For every expr to sub, find the earliest point it can be moved forward to:
  * Stopping at the first set of braces
* Move the sub-expression there if we're at a set of braces (if not don't sub?)
  * We could still possibly be safe if every nested call just had one parameter,
    and we add the constraint that all `r2c` expressions must evaluate their
    single parameter.
* For expressions that can't be moved?
* Perhaps this whole process needs to be done iteratively?


# Single Group Performance

Much slower than base, would be interesting to track down what's going on.  It's
particularly bad if we reverse the order, but it looks like we've got a whole
extra vector being allocated that maybe doesn't need to be.

    library(r2c)
    slope2 <- function(x, y) {
      mux <- mean(x)
      x_mux <- x - mux
      sum(x_mux * (y - mean(y))) / sum(x_mux^2)
    }
    r2c_slope2 <- r2cf(slope2)
    n <- 1e8
    a <- runif(n)
    b <- runif(n) * .5 + a * .5
    gc()
    ###             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ### Ncells    362973   19.4     698777   37.4         NA    677183   36.2
    ### Vcells 200733947 1531.5  362973722 2769.3      16384 300734934 2294.5
    system.time(slope2(a, b))
    ##   user  system elapsed 
    ##  0.834   0.192   1.026 
    gc()
    ##            used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    363549   19.5     698777   37.4         NA    677183   36.2
    ## Vcells 200735202 1531.5  579650024 4422.4      16384 400737393 3057.4
    system.time(r2c_slope2(a, b))
    ##   user  system elapsed 
    ##  0.807   0.210   1.069 
    gc()
    ##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    373739   20.0     698777   37.4         NA    677183   36.2
    ## Vcells 200759255 1531.7  579650024 4422.4      16384 550877897 4202.9
    system.time(slope2(a, b))
    ##   user  system elapsed 
    ##  0.867   0.228   1.094 
    gc()
    ##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    432238   23.1     885577   47.3         NA    677183   36.2
    ## Vcells 200891389 1532.7  579650024 4422.4      16384 550877897 4202.9
    system.time(r2c_slope2(a, b))
    ##   user  system elapsed 
    ##  0.864   0.326   1.265 
    gc()
    ##             used   (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
    ## Ncells    434879   23.3     885577   47.3         NA    677183   36.2
    ## Vcells 200897934 1532.8  579650024 4422.4      16384 550990626 4203.8

Ok, some more testing, the problem is almost certainly that there we generate an
integer group vector that is the difference here.  Almost certainly the group
order vector that isn't altrep as noted.  That might be enough to explain the
difference in timing (note there is a lot of sensitivity to the order in which
things happen above given we make big memory requests from the OS).

# Braces

# Assignments

# Linearized vs Not

One big advantage of the linearlized processing is that it lets us track lots of
relevant meta data in the same form.

One issue we're running into is that we lose some of the context.  For example,
in an assignment expression the first thing we hit in the linearized call is the
symbol to assign to, with no understanding that symbol is being assigned to
later.  So we'll try to resolve it and will fail because it does not exist yet.

For now the hack would be to leave the symbol unresolved?  Yes, but how do we
make sure that it does get resolved and allocated at the right time?  We can
mark it as an assignment symbol, along with the depth at which it should be
resolved, and once we get to the assignment call that matches that depth, we can
do all the update stuff.  I think if we just ignore it completely it's fine.  We
just need to record that it's an assign target.

# If / Loop

## Constraint Overview

The over riding constraint is that every symbol must resolve to an unambiguous
size.  `if` and `for` control structures can create ambiguity in sizes (e.g.
different sizes in branches of if, different sizes within the loop / loop can
act like a branch if not taken).

Disallow cases where a variable might not be created that is later used:

* for:
  * Zero iterations (maybe).
  * `break` / `continue`
* if:
  * Brand new symbol.
  * Not all branches contain it.
  * What if the symbol exists from elsewhere?

Additionally:

* for:
  * variables read and written from must be consistent size before and
    after loop
  * variables must remain constant size in the loop (e.g. cannot grow)
  * variables written to in loop must be either unchanged in size, or the loop
    must be guaranteed to have at least one iteration.
* if:
  * variables written to must be the same size across all branches; branches
    that don't write ok so long as prior var same size.

Need to think about what compatible allocations are.  Certainly different
branches must produce the same size result.  Does the result need to end up in
the same allocation id?

## Loop C Implementation

Probably the way a loop would work is:

```
void run(
  double ** data, R_xlen_t * lens, int ** di, int * narg, int * flag, SEXP ctrl
) {
  (void) narg; // unused
  (void) ctrl; // unused

  dibase = *di;
  flagbase= *flag;
  for(i = 0; i < n; ++i) {
    // - Loop Overhead ---------------------------------------------------------
    // Do something to update the value of `i` in the data
    ...

    // Then reset index, flags, more args
    di = dibase;
    flag = flag;

    // - Normal Code -----------------------------------------------------------
    // mean(x)
    mean(data, lens, *di++, *flag);
    ++flag;

    // x - mean(x)
    subtract(data, lens, *di++);
    ++flag;
  }
}
```

## Interesting Example

Pretty challenging dealing with loops.

```
x <- integer()
for(i in 1:10) {
  x <- c(x, i)
}
```

Each loop iteration is going to have a different size for x. The best we can
hope for is consistent size every iteration including the first.  This should
handle the case where the loop doesn't even run.

What about variables created in loops that are not taken?

## Tracking Implementation

### Implementation

Trick with all this is we need to check the state of the stack at every use of a
symbol, so we're back to doing the whole tree as described in (old)...

        x
      / | \
     a  b  |
     |  |  |
     a  b  x

Not only that, we can't even know if everything is the same size or not unless
we process the expressions.

So we do all the renaming and stuff first, and allocation time, we build the
symbol mapping tree for each symbol with each step in the evaluation.

```
x <- w
if(a) {
  x <- r
} else if (b) {
  x <- s
} else {
  if(c) {
    x <- t
  } else if (d) {
    x <- u
  }
}
```

`if` stack contains a count of how many `else` at any given `if` depth.  For
each depth, for each variable, what symbol-nodes it has been bound to (a symbol
node can contain the result of a nested if).

So the processing of an `if` should return a list with as many elements as there
are `else` clauses, plus 1 for the original, plus an indication if there is not
a terminal else.  Then, the function that initiated the processing of the `if`,
can check whether for each symbol, there are the right number of things.  For
those that don't have enough, add the previously bound symbol(s).  Then, flatten
all the lists in case there was any nesting.

Within each branch of the if, we don't have to worry about what's going on in
the alternate branches, only the symbol ambiguities accrued to date.

`for` loops are different because we need to forward check all the symbols due
to the same code being run more than once.  We simply disallow `for` loops from
changing a variable size.

Maybe overcomplicating this.  A `for` loop can only add a new rename for each

Ugh, and almost forgot about parameter order (it's okay, this is "unknowable" in
R anyway because it depends on when the params are touched inside functions, so
we have to declare it as a difference to `r2c`).

Maybe this isn't too bad, the main thing is to distinguish between data/external
symbols and new ones we add.  So when we enter a branch, we just create a copy
of the whole allocation structure, and when we exit, we compare to all the other
branches (including the original) and make sure they are compatible.  Actually,
for the `if` statements maybe we first compare across all branches and replace a
missing `else` with the original.  This should work recursively I think.  From
this perspective we don't need to keep any of the assignment symbol renames.
The only renames we care about are the subexpression substitutions.  The initial
renames were just to prevent invalid substitutions, but once that's handled, we
can revert them all (so we need to track original symbol).  This seems like it
could work.

Next steps:

* Tests (or do we wait to make sure this stuff actually works)?  Kind of, the
  code is all broken atm.
* If the first instance of an expression is assigned, and the assigned symbol is
  available, can we just use that as the name of the sub-expression?  Reasons
  not to are that these symbols can be overwritten (that's okay, renaming
  handles this), and it's also harder to debug because we won't be able to
  distinguish that a particular subexpression was replaced.  So probably lean
  against, as it would only "help" in the weird case where a user saved an
  intermediate expression, but then failed to use it.
* Start rebuilding the allocations mechanism.

## Track Substituted Expressions

* For the substituted ones it's trivial because their own index is the index
  of what should be referenced.
* For the one with the assignment....
* Seems like we really want for each call in the new tree, a reference to the
  old.

But, what we actually want is when we're going through the pre-process
linearization, for each thing in the tree we're traversing, have the original
call.  We can't do this for one tree alone.  Conceivably we could if we attached
the original call as an attribute, but stops working with symbols.  Maybe we
attach the original call to calls when there is a difference, and separately we
return a list of the calls corresponding to the re-use symbols.  This should
make it relatively straight-forward.

# Assignments

## Re-Writeable assignment

This raises an interesting point: if we allow modification of the assignment we
must create a new allocation and make a copy.  Maybe we can forward analyze and
see if future calls modify the symbol?

So, if each assignment generates a call, if it is to a RO variable it can record
the parameter and result ids as the same, and set the depth of that variable to
zero.  If it is a RW variable, then it allocates something to hold the copy, and
sets the depth of that to zero.

`preproc` will have separately generated a C command that is a either a NULL
operation (so that all the parameter incrementing stays in sync - hmm, looks
we do have some incrementing that doesn't require a call, e.g. for flag) or a
`memcpy`.  So `preproc` needs to track whether a variable is written to which it
can do in a second pass.  Actually not that trivial because `code.gen` is run
during the recursion stage, so we can't just go back and change it in the
linearized list.  We would have to defer running it, which means recording all
the parameters for it.

So how would we detect that an assign created variable is written to later?
preproc has the argument name.  It would then keep a top-level list of all the
assigned to arg names accumulated to date, and if at some point later down the
track a write function involves that parameter, it would know.  That means that

## Memory Re-Use

### Arith Assign

Can we use `+=` to avoid having to have memory allocated for both LHS and RHS
for the arithmetic operators?  Might complicate things substantially.  The main
problem with this is we have a protected expression for re-use, we cannot
overwrite it.  Additionally, we only ever want to overwrite the longer of the
two parameters.  So we could do this in the subset of situations where the first
argument is longer, but to do it `alloc` would have to realize this, know that
is a special vecrec + arith situation, and then set the result vector to be the
same.  We would also need to modify the arith code to have one additional
conditional layer that needs to be called once per iteration.

Better would be to have a second version of arith that does the `+=` business
and we substitute that one in for the calls that support it.  This doesn't
resolve `alloc` still having to do work.

It does create a second situation where we want to be able to code gen two
different pathways depending on the nature of subsequent code.  This should all
be able to happen at the preproc level though, I think.

### Do we do it already?

For the case of assigned to variables it seems maybe the code already does it,
as we see things like:

    ..$ :List of 4
    .. ..$ call: language square(x_mux)
    .. ..$ ids : int [1:2] 5 5
    .. ..$ ctrl: list()
    .. ..$ flag: int 0

In this case, `x_mux` is not referenced any further, and is thus free.  Seems
like this can work in some cases, but not all?

    f(x_mux, some_calc)

In this case, there is a risk that `some_calc` will end up using `x_mux`'s
allocation, so this is definitely too aggressive.  When a terminal symbol is
used, it should update the depth of the corresponding allocation.

## Memory

IIRC currently each call can re-use a memory slot deeper than itself.  This
makes sense in the context of a completely nested single line expression.  It
should extend naturally to assignments that are made at the top level.  Gets a
bit tricky if there is an assignment made inside a call (and also we need to be
able to distinguish between a param specification and an assignment; it's
probably okay though b/c of `match.call`).

So all assignments are made directly at the top level automatically since we do
not support `local` or anything like it.  Possibly instead of "top-level" we
just keep track of the current function's depth to allow for nested "r2c_fun"
functions.

We then need to figure out how to resolve symbols.

In the reduce step, we can just change the depth of the prior call's result on
the stack (remember to test sequential allocations `a <- b <- fun(c)`).

Things to do:

1. Allow preproc to match to newly created variables.
2. Add generator functions for RO and RW assignment.
3. Add a mechanism for alloc to know how to set the depth for assignment
   (actually this might be fine since `preproc` does it, problem is that `alloc`
   uses `depth + 1` to get parameters to current call, which won't work anymore.
4. Related to ^, change how `alloc` finds the parameters to each call.  We might
   need a separate flag to have `alloc` change the depth after identifying
   parameters, or alternative set a `callid` of some sort to identify parameters.
   Maybe have `preproc` add a `protect` flag.
5. Structure to track variables needs to contain:
   * Depth?
   * Size (including NA for group size).
   * Read Only flag.
   * Id of call that generated them (so we can go back and edit the call if an
     RO flag was toggled off).
6. Auto-free memory once there are no further calls to it (also true of
   sub-expressions).

First thing to solve then is allowing preproc to match to newly created
variables.  Hmm, not sure preproc is the one that does this.  preproc only
matches to parameter names.  So it's alloc that needs to do it.

We could do it by adding an additional match layer ahead of the `data.naked`
one.  If there is a match in this additional layer, then we use the variable
(and inherit size, etc.).

How does an overwrite work in this scheme?  It should mark the data allocation
as available, and ...

Also, things are a bit complicated by the call being in effect `"<-"(x, value)`.
We need to get the target name from the call, which isn't hard, but just doesn't
quite fit into the framework we have.

## Assignment and Control Variables

One big complication is that control variables could reference variables that
are created as part of the expression, e.g.:

    x <- a > b
    sum(y, na.rm=x)

How do we resolve this?  Currently the semantics are to treat symbols as if they
are the full data the symbols.  We could have a version of each assignment that
resolves also to the full data symbols used purely for the controls, but this is
very tricky because we don't want to carry out all those evaluations that define
each of the symbols unless we actually have to as those would compute on the
entire data vectors potentially.  We could define them as `delayedAssign`
into an environment, and then evaluate the control parameters with that as an
environment in the chain.

So every time we encounter an assignment, we need to record the assignment
expression as a `delayedAssign` in our special environment that has `data` for
parent.

Possibly even worse:

    sum(x, na.rm=(x <- y))

As it stands now we'll make a copy of `x` in the initial allocation pass,
whereas probably we would expect the opposite to happen (i.e. actually get `y`).
Probably we just warn people about it and let them make their bed.  Since we use
a new environment each time that `x` likely is lost.

## Assignment and Allocation

When we encounter an assignment, we need to know not to allocate additional
memory.  Additionally, we need to know to free whatever data was previously
pointed to by that symbol.

When we encounter a symbol, we need a mapping table to know whether the symbol
is an assigned symbol.  If it is, then we know it's size from the `alloc` value.

We want to track when each symbol is last used so we know we can trash the
allocation.  We can probably do this in a first pass through the linearized
calls where, for each symbol, we find the latest read reference to it.  Then,
anytime a symbol is "protected" by being written to, the "protected" value is
the last reference to it.  Each iteration then frees all the wrong depth stuff,
and any expired protections.

Any overwriting of a symbol will also effectively free the prior allocation (and
transfer the protection value to the new allocation (although they may be the
same size and re-use the slot).

## Renaming

Preprocess does match each call.  Whenever it matches a call, we can check if
any of the parameter expressions reference a prior expression, and at that point
we can go back and mark as protected.  Need to make sure anything data/external
is default protected.  Interestingly we don't need to auto-protect an
assignment, it only needs to be protected if at some point it is re-used (but it
would be interesting to let ppl know if it is never used).

So we want a structure with all the prior expressions so we can test them, and
go back and mark them as needing to be protected, and also the point at which
they are last used.

Assignment can use this.

Then, at the end we can go back and code gen everything, substituting when
appropriate functions that are dealing with protected parameters.

The renaming process likely needs to be a shadow rename, meaning we need the
original copy too.

Anytime we rename, we need to process all remaining expressions and rename them
too.  Likely we want a deferred rename, i.e. it happens just before each
expression is processed, so we need to accumulate all the renames.  If we don't
do this we have to maintain two copies of the call tree which is going to be
annoying to recurse through.

* Need a stack of renames.
* Need a stack of past expressions to identify re-use (we have this already from
  the linearized stack).

## Nested Functions

In order to support nested functions, we need to prevent renames of external
variables.  We also need to affect symbol resolution.  So essentially each
function layer will need a whole frame of `call_dat` or some such.  We will also
need to re-match to the outer function environment as that could cause renaming
of the data variables, etc.

What a nested function does is effectively:

    function(a, b) {
      a <- force(a)
      b <- force(b)
      ...
    }

In other words, it generates implicit bindings to its local variables.  To
shoehorn it into our framework we can just keep applying the renames on a
rolling basis, and they should then naturally expire by the time the function
exits scope.  So `preprocess` will need to identify a function to symbol in the
call, realize that it can get a body for it, bind the formals to force-renamed
symbols, and proceed as normal.  The allocator should then behave correctly from
that point forward, without needing an explicit frame stack.

What we can do is anytime we encounter a function, generate the assignment calls
for all the parameters.  We only allow constant default params, so should be
okay (but need to double check what we do with those at preprocess time).

One problem to resolve with the renaming is the nested function might use a
component from the outer one (e.g. internally it computes `mean(a)`, but
`mean(a)` has already been computed previously); the renaming would prevent
re-use of the expression.  So maybe we don't need to worry about renaming.  The
only real issue is how unbound symbols are resolved, and making sure that
such symbols are not resolved against symbols bound in the calling frame.

So it seems that anytime we enter a new function, we need to reset the portion
of the allocated data structure that we can look through.

## Preproc

Shouldn't actually generate any code?  But we need a mechanism to tell the
allocator that the result of the prior call should be assigned to the top level,
and we need to bind it to a symbol (duh) that will be on the search path.


# Function vs. Expression

## Free vs. Bound

One complexity to think about is that in the auto-generated "functions", there
is no possibility for a free symbol to exist, whereas in the manually specified
ones we might allow completely free symbols such as `pi`.  Currently (0.1.0) the
structure supports this at the expense of quietly finding symbols bound to e.g.
the global environment.  So we need to add a check that detects this.

How to default values work for functions?  This should be fine so long as they
are either control or flag parameters.   So we need to check as we process the
expression that we don't try to use a control/flag parameter as a data parameter
somewhere.

    > f <- r2cq(sum(x, na.rm=y) + sum(y, na.rm=x))
    > f(1, 2)
    Error in VALID_FUNS[[c(name, "ctrl.validate")]](stack.ctrl, stack.flag,  :
      `na.rm` should be type "logical" (is "double") in `sum(x, na.rm = y)`.
    > f(1, 2)

Seems like it should actually be fine except for the possibility that a variable
will get interpreted in full in one context (control) and in part in the other
(data).  We should disallow it as the described case should never be something
we actually want (and would be unsemantic), even though technically it could
work.

If we specify `formals` for expressions, we need to distinguish the case where
we don't specify any formals at all and want them all swallowed into the actual
formals, vs. the case where we want everything left unbound.  Maybe leave NULL?

For the case we do have formals, it might be as simple as initializing
`x[['sym.free']]`.  One thing is we'll probably have to add tracking of whether
a symbol is ever used or not to possibly report that there is an unused formal
(or at least think through the consequences of such; maybe none?).

To conclude, we allow free symbols either via the function formals or by giving
a list of symbols to bind to formals.

# Window Implementation

## Offset Direction

It really makes more sense for the offset to be the negative of what it is now.

## Start End

Does partial make sense for `rollby_`?  Should start end really subset, or
simply set where we start.  The latter is increasingly attractive.  What is the
correspondence with `rolli`?

We do really consider subsetting to be out of bounds of what this does.
Although it would be nice and could do it.

Actually, increasingly thinking that partial just doesn't make sense other than
for the regularly spaced data where you would expect a specific count of
observations inside the window.  There is the argument that you want windows to
have the same amount of "eligible space", but that's also achievable by setting
`start` and `end` at reasonable points wrt to `x`.

## Incomplete

A window is considered incomplete if the beginning of the data is to the right
of the left end of the first window, or the opposite.

So if:

    ileft == 0 && index[ileft] > left ||
    iright == imax && index[iright] < right

## Rationalize

We can rationalize the integer case to be:

* `width = n - 1`
* `bounds = "[]"`
* `index = seq_along(<data>)`

And it will be somewhat slower.  The biggest issue is that `seq_along()` could
return either numeric or integer, and we don't want to generate that whole
vector.  Maybe we just let it exist for this reason alone.  It would require a
bit of contortion to handle the special iterative case?

Maybe not, we would need to check whether the object is altrep sequence starting
at 1 and with a stride of 1, and then we could, for just the "[]" bounds case,
make the condition to check for be against the `i` instead of `index[ileft]`.

## Discrete vs Continuous

Is the "irregular" mode really about using a continuous variable, and thus the
base index points to a zero-width instantaneous moment (`POSIXct`?).  Whereas
instead the normal mode is pointing to a fixed width element.  What about
something like a `Date`?  Which is it?  If it is continuous, do we just take it
to be the instant the day switches, midnight?  Does it belong to the prior or
the subsequent day?

## Closed / Open intervals

Related to the above, how do we treat the ends of a window?  Do we give the
option of how to treat them?  What should be the default.

It's tricky because there is no consistent answer.  For no overlapping windows,
we get `n` if align is left or center, but `n-1` if it is right.

I think what makes most sense is to have a half closed interval.  We're picking
open on the right, so that e.g. if we pick a date, and we right align with width
1 day, we'll get all the entries from the prior day excluding anything that
happened at midnight, which we consider part of the current day.

Maybe in the future we'll give the option to do something different.

## End condition

Struggling a little bit to figure out the natural end condition.  If `align =
"left"` then it makes sense to have as many iterations as intervals.  We don't
need end to get a go at being an base index because it is included in the prior
window (notwithstanding smaller windows, etc.).

I guess `end` needs a shot at being a window because there is no guarantee it
was included in the prior window.  And it seems weird to use presence in the
prior window as reason to give it a shot or not, although in the special case of
`width = by` it does produce a "pleasing" outcome.

## Alternative Irregular Windows

Firstly we clearly need to rename our things.  There window size, window
spacing, and window contents.  Our implementation is regular in the first two.
We could allow:

* Specifying indices.
* Specifying window positions (where these could be the same as indices to
  simulate what slider does).
* Specifying window start/end points (equivalent to positions with variable
  widths?)

What should the names be?  Current `window_i_exec` is not actually irregular.

* Is it step vs. slide?  But both are step.
* Discrete vs continuous?
* Integer vs real?  Maybe.
* `window`
* `window_c`
* `window_ci`, this replaces `by` with `at`?

    gexe(fun, groups, data, MoreArgs, enclos)
    wexe(fun, width, data, MoreArgs, by, partial, align, enclos)
    wcexe(fun, width, data, MoreArgs, by, partial, align, enclos)
    wciexe(fun, width, index, data, MoreArgs, by, partial, align, enclos)

    gexe(fun, d, g, args, enclos)
    wexe(fun, d, w, by, align, partial, args, enclos)
    wcexe(fun, d, i, w, by, align, partial, args, enclos)
    wciexe(fun, d, i, w, at, align, partial, args, enclos)

    g_exe(fun, data, g, args, enclos)
    # discrete regular
    wd_exe(fun, data, w, by, align, partial, args, enclos)
    # continuous regular
    wc_exe(fun, data, i, w, by, align, start, end, partial, args, enclos)
    # continuous irregular
    wi_exe(fun, data, i, w, at, align, start, end, partial, args, enclos)

    g_exe
    wd_by_exe
    wd_at_exe
    wc_by_exe
    wc_at_exe

    g_exe

    wdby_exe
    wdat_exe

    wcby_exe
    wcat_exe

So is there a way to merge the discrete and continuous interfaces into one?  Not
really, they are pretty different.  But continuous can completely cover the
discrete case.

Actually, they are pretty close because the following produce the same value:

    wd_by(x, w=<odd number>)
    wc_by(x, seq_along(x), w=<odd number>)

One issue remaining is how to treat "align='right'".  In continuous it excludes
the base index value due to the open interval on the right.  But in discrete it
includes it.  If we defined it as `right = width - 1`, it works in continuous
too?

    |1|0|1|2|3|
        ^
      [ | )
      ( | ]
    [ | | | )
    ( | | | ]



    rolli_exec(f, data, n, align, by, partial, MoreArgs, enclos)

    rollby_exec(
      f, data, width, offset, by, x, start, end, bounds, partial,
      MoreArgs, enclos
    )
    rollat_exec(
      f, data, width, offset, at, x, bounds, partial, MoreArgs, enclos
    )
    rollin_exec(
      f, data, width, offset, x, left, right, bounds, partial, MoreArgs, enclos
    )


    rollby_exec(
      f, data, width, offset=w/2,
      x=seq_along(if(is.list(data)) data[[1L]] else data),
      by=(end - start) / (length(x) - 1L),
      start=x[1], end=x[length(x)],
      bounds="[)",
      partial=FALSE,
      MoreArgs=list(),
      enclos=parent.frame()
    )
    rollat_exec()

    auto_exec <- function(f, exec, ...) {

    }

    rollsum3 <- auto_exec(r2c_sum, roll_exec, width=3)
    rollsum3(1:10, align='right')






    w_exe(r2c_sum, x, w=5, partial=TRUE, args=list(na.rm=FALSE))
    g_exe(r2c_sum, x, g, na.rm=TRUE)

    wat_exe(
      f, x, w,
      i=seq_along(x), at=i, align='center',
      partial, args, enclos
    )

    winx()
    winatx()
    grpx()

    winX()
    winatX()
    grpX()

    window_by_exec
    window_at_exec




What does `partial` mean for the continuous case?

## r2c

Advance: group size -> by
Length: group size -> window

Additionally, need to handle cases where the window overflows and fill it.

Currently we advance the data pointer by group size, but instead we want to
advance by 'by'.

So we need to split the `grp_lens` into those two different things, that are the
same for the group mode.

Additionally, we need to add support for the recycling of the indices.  For
simplicity we could start by recycling them in R?  Not ideal as it creates big
vectors that are completely unnecessary for the default case of scalar offset
and scalar window.  And these vectors are large.

Need to ensure that we have nothing in the offset vector that causes a problem.

Likely want a special branch for the scalar-scalar default case.

Currently the code assumes that `g_lens` is equal in size to the number of
offsets, which certainly won't be the case if we're recycling.  This also means
that the looping is completely different because for the window case we're
recycling offset and group size until the time we run out of data.  So we're
going to have a double nested loop with a break.

We need a check that the final result is scalar as it's not guaranteed ex-ante?
We could guarantee it ex-ante, but that's dangerous if we're wrong (but then all
our size calculations would be wrong anyway).

What if we allow non-scalar results?  Probably immediate complication is that we
can't just go straight to "NA" result when windows are OOB.  If we do resolve
that, we need to do a first pass pre-computing the result vector size.

For now focus on developing the scalar step and scalar window size, and
recognize the possibility for:

* Vector of steps.
* Vector of window sizes.
* A location vector to allow data to have different positions than their rank
  in the input vector.

## For Benchmarks

* slider
* data.table
* zoo rollapply


## Others Window Implementations

### data.table

#### Key Points

* Slow and fast algorithm
* Adaptive width

#### Details

https://github.com/Rdatatable/data.table/pull/5441#issuecomment-1236104263

### Slider

#### Key Points

* Irregular series can still be windowed when providing an index.
* Uses segment trees for the fast calculations to reduce cost from O(n^2) to
  O(nlogn)
* Uses before/after instead of left/center/right for more flexible window
  computation.

#### Details

Interesting mention of "segment trees".  This allows to partially cache sums by
pre-aggregating the vector in a binary fashion, and then re-using the aggregates
when possible.

Also interesting that instead of using "left", "right", etc., it uses before and
after.

Would it work with variance?  What does it generalize to?

    sum((x - mean(x))^2)

Default is to return a list.

Concept of index column for irregular series.

Referenced paper also has a few other interesting topics:

* Partitions, where you compute the sliding function within each partition
  independently.
* Variable size windows, where you cannot easily deal with removal/addition
  window because you don't know how much you need to remove/add and doing so
  could require scanning the entire thing again.

We can compute the variance of two buckets from the individual stats according
to "Maintaining variance and k-medians over data stream windows." (referenced in
[1])

    n<i,j> = n<i> + n<j>
    u<i,j> = (n<i>*u<i> + n<j>*u<j>) / (n<i> + n<j>)
    V<i,j> = V<i> + V<j> + (n<i>*n<j> / n<i,j>) * (u<i> - u<j>)^2

So it should be possible to derive the equivalent formulas for most statistics
for combining buckets, but there might not be an automatic way of doing so given
a function (hard to believe there would be looking at the above).  Once we have
the statistics we can use segment trees to govern how we combine things.

So ultimately this business comes down to how much it costs in common use cases
to re-run the entire function.

[1]: https://www.researchgate.net/publication/221559611_Variance_estimation_over_sliding_windows/link/56a2194208ae2afab885c5ba/download

### runner

Doesn't seem to add too much interesting.

# Interface

## apply

Evaluating whether we should shift to `tapply`.

    function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)

What would that do?

    r2c_tapply(x, g.r2c, r2c_sum, ...)

Is it okay to have `MoreArgs` as dots?  Probably, the issue being conflict with
additional parameters, which in this case is just `enclos`.  In terms of
implementation we would just capture them in a list and pass them on.

Names?  We want to add window functions, so do we end up with:

    r2cgapply
    r2cwapply

Or

    gapply
    g_apply    # makes it clear not base R
    wapply
    w_apply

Another option

    ctapply
    c_tapply
    cwapply
    c_wapply

Should the name communicate in some way that these only run with `r2c` funs, or
do we wish to allow them to run with other functions?

By extension we should probably do:

    csum <- r2c(sum(x))

The misleading thing for that one is `sum` is also essentially just C code, but
for the more complicated cases that makes more sense.

## tapply

Semantic incongruities:

* We accept lists as the first parameter, so not based on "split" method.
* Functions can accept multiple parameters, and they are matched into `X`.
* Result is neither array nor list, always simplified into a vector or a
  data.frame.

`tmapply`?

    tmapply(FUN, INDEX, ..., MoreArgs)

Main issue here is what if we want to pass a data frame in ...?  This would
require always specifying each element individually.  Additionally,
if we have `MoreArgs` we're required to type it out.  We could use:

    tmapply(FUN, INDEX, DATA, MoreArgs)

This is pretty close.  Or maybe:

    tmapply(FUN, INDEX, DATA, ...)

Although same issue of wanting to pass a list in ..., and additionally name
overlap with other parameters.

    grapply(FUN, INDEX, DATA, MoreArgs)
    grapply(FUN, INDEX, DATA, ...)
    grapply(FUN, INDEX, X, ...)

Dots are appealing, except for the potential for name collision.

    grapply(FUN, INDEX, DATA, ...)
    group_apply(fun, index, data, MoreArgs=list())

    grexec(FUN, INDEX, DATA, ...)  # too close to gregexec?
    group_exec(fun, groups, data, MoreArgs=list())

    window_exec()
    window_apply()
    wapply()

Do we need to provide `enclos`?  Probably not.

## NSE

Things like:

    tcapply(
      iris[paste0('Sepal.', c('Width', 'Length'))],
      iris[['Species']], csum
    )

Are annoying.  This okay enough?

    with(iris, tcapply(list(Sepal.Width, Sepal.Length), Species, csum))
    with(iris, tcapply(l(Sepal.Width, Sepal.Length), Species, csum))

# Dots

## Trying to Get Dots Working normally

### Preprocess

Do we allow dots in `ctrl` and `flag`?  Maybe, they are not really used at all
in preprocess.  The only thing we see is computing of length of dots to
determine whether e.g. to use the multi-arg version of sum or not.  The only
thing we seem to preserve is the name of the argument, and it's not clear that's
actually used by alloc.

One challenge is how do we distinguish between these two:

    r2cq(sum(...))
    r2cq(sum(x, y, z))

In the former we want to literally match `...` against the dots argument,
whereas in the latter we want the `x`, `y`, `z` to be expanded out.  In a more
complex expression `...` might match more variables:

    r2cq(sum(x, y, z) + sum(...))

Produces:

    function(x, y, z, ...)

Consider for:

    function(..., na.rm=TRUE)

Things like:

    f(..., x, na.rm=y)

We need both dots and `x` to be matched to the actual dots.  So we need to
replace `...` in the call with e.g. something like `.R2C.DOTS`, which will be
recorded in the symbols as `...`.

### Alloc / Run

It seems here most of the work is done by the matching in `group_exec`.  Might
just work?

No, we need to expand out the dots in the linearized call and related data.
Seems right now the fields that need to be expanded are call, argn, depth, and
type, where we look for any `call` of type `leaf` that is `...`, and sub in as
many leaves as there are elements.  One issue to figure out is if it's a problem
that `argn` is just going to be "..." repeated.  Do we use this to match
anyplace?  We must for allocation.

`argn` is, what?  We're seeing:

     $ argn     : chr [1:4] "..." "..." "na.rm" ""

Not sure why there are two "...", although those do match to the `...` in the
formals of `sum`.  So `argn` is the argument name in the original.  But why do
we even use this in alloc?

Strongly suggests that we just cannot allow "..." to be matched to control
parameters?  Seems like we need to generate "..1", etc, to be orderly matched to
the data proper, and that data derived from "..." should get those names (unless
it already has names)?.

## Previous Notes

Do we allow dots in the call?  This could come up if there is an `r2c`
construction inside of a function.  But it's a bit silly because if we don't do
it inside a function we get a failure from dots not existing, but when we run
elsewhere we might not be inside the function anymore.

Let's say we allowed it, how would it play out?  Are we counting the args at
compile time (probably).

Should we disallow no free symbols?  Probably should be at least one, it
complicates logic a bit if we don't as we need to know the size of the result.

    f <- r2cq(sum(1, 2))
    f

Similarly, should this really work?

    > f5 <- (function(...) r2cq(sum(...)))(1:10, 2)
    > (function(...) f5(...))(1:10)  # okay
    [1] 57
    > sum(1:10
    + )
    [1] 55
    > f5
    function (..1)
    {

It's kind of an accident that we generate the `..1` symbol, and only because
`1:10` is a call:

    > (function(...) r2cq(sum(...)))(1, 2)
    function ()
    {

So probably just need to figure out why the call version generates the `..1`
parameter.  Has to do with the fact that for some reason dots generate `..1` for
calls, but not for constants.  So there is a weirdness that something that seems
like a constant (`1:10`) because R can't discern that it is and is in dots
generates a free symbol.

Maybe that's okay, and really what we need is a better error message when there
are no data columns, or a better way to manage that scenario.  When there are no
data columns all inputs are length 1 as that's the only way to truly generate a
constant?  Can we use this or is that begging for trouble?

We won't use that, we just require a free symbol.  There is the issue of
"constants' like 1:10 generating free symbols

Thinking about this a bit more on dots, all we need to do is make sure that the
code gen functions are aware of dots, and have code that can handle dots.  We
then need to make sure there are no issues of expanding or evaluating dots in
the compile stage, that functions that support dots get told they are getting
dots, and those that don't fail, and in the allocation stage the dots should
match gracefully to the data and everything should be great.

# Loop implementation

Loop variables can be created in a child frame to the data.

Each loop body will generate a `run` statement, e.g.:

```
run(...) {

}
run_loop_1(...) {
  for(intmax_t i = 0; i < len; ++i) {
    run(...);
  }
}
run_loop_0(...) {
  for(intmax_t i = 0; i < len; ++i) {
    statement_0(...);
    statement_1(...);
    run_loop_1(...);
  }
}
```

Probably want two types of loops, loops where we know the start and end as
constants or other derivable data (`seq_along`), and others where we have an
unknown vector?  Might not be able to allow the second type, because at that
point we can't know how large a vector that is assigned to with e.g. `x[i] <- y`
will get.

```
for(i in seq_along(x))
  y[i] <- x[i] + 1
```

Is the latter:

```
for(i in x)
  y[i] <- z[i] + 1
```

At the allocation stage we need to determine the size of `y`.  It's fine if
we're just referring to an external symbol (or even a group symbol) as we can
check the content sizes.  But things like:

```
for(i in rev(x))
```

Get messier.  Likely we just allow symbols, or `seq_along`, or constant
expressions like `a:b` where `a` and `b` are known at allocation time (changing
them inside loop is fine since):

> The seq in a for loop is evaluated at the start of the loop; changing it
> subsequently does not affect the loop.

But they can't be computed within the `r2c` code.

# Further Optimizations

## Overview

Thoughts to try to reduce the group overhead.

* How much overhead is just from `run`?
* Evaluate overhead from generating result labels.

It's actually significant: out of ~.9 secs for 1e8, .2 is spent on label
generation.  In this case we need to be careful due to hashing of labels.  We
can reduce this dramatically for the case we know that group.res.sizes are all
size one, and a little bit if we know they are all the same size.

We're actually getting a bit of overhead from generating the labels, so should
just generate them directly in C (and now we do).

It doesn't seem like order affects collapse much.

## Group Sizes

Group size computation is significant.  The need to do two pass is also not
super (group count, vs recording group sizes).  Group count is actually quite
fast because it can use vectorized instructions to increment the group counter
without the need for branches.

We also have the issue that we don't allow more than INT_MAX groups, and related
that we do allow R_xlen_t max group sizes (the former needs to be decided on as
a limitation, the latter needs to be decided on as a feature we want to keep).

Big question is whether we want to change the logic to avoid doing two passes.
We can do something that e.g. allocates 1/4 of the size (but it has to be for
labels and for group sizes), but if it fails will need to go to 1/2 the size or
larger.  Maybe start at 1/8.  There is an outer loop that checks if we hit the
last value in the loop, and if we did triggers running the inner loop again
after "growing" the vector (i.e. re-alloc, copying, unprotecting).

The other thing we could do is an analysis of the first 1000 elements to
estimate the group size, but this will fail miserably in the case where there is
a single e.g. 1K initial group, but then much smaller group sizes after.  So it
gets pretty complicated: you need to do that first 1K pass, assume e.g. smallest
group, and if you fail, fall back to old method.  If you succeed you're still
left with an over-allocated vector you likely need to subset or copy later
(unclear whether we are allowed to use "true length" for this, or whether we
want to manually generate the label vectors manually).

The pattern we're looking for is "increment counter on condition", which we can
use to increment a pointer.  But then we are going to need to write to write to
memory every iteration, which is probably worse.  So likely the best we can do
is reduce time by 1/3 by avoiding the initial group number counting, which seems
like a lot of complexity and work.  Instead maybe we really should have a
separate group computation.

## Notes vs Collapse

For a single stat like `sum(x)` collapse is more memory efficient because it
does not reorder the data vectors.  Also, while generating the radix sort
ordering vector is fast, the actual reordering of the vector is much slower.
Would be interesting to figure out why that is (actually this untrue, I was
remember the cases where we were hitting the memory limits with 1e8, it takes
the same amount to order as it does to re-order with the order index).

For multiple stats like in `slope(x)`, `r2c` is more efficient.

For smaller group counts, the hash algorithm from collapse seems much faster.

Idea: could we do something with only sorting the result at the end?  Seems like
it should be terrible to access the data vectors in an unsorted order though.

From analysis it looks like what's going on is that (unlike `GRP`), `fsum`
defaults to the hash method.  There are many pathways in the hash code, but for
our data it seems the most likely outcome is to end up in one where the hash
table size is the size of our input vector.

The hash function is just the value of the group index modulo the has table
size.

But for some reason this doesn't seem to have terrible corner cases.  Once we
get to small group sizes, we start underperforming the radix sort, but not by
massive amounts, and we're way faster for larger group sizes.

On my system this is the cost of accessing the vectors at random (well, this
includes the physical re-ordering):

> system.time(x[io])       # ordered
   user  system elapsed
  0.052   0.003   0.057
> system.time(x[ir])       # random
   user  system elapsed
  0.286   0.007   0.311

And most of the cost is just accessing the vector (`order_sum` below adds each
value to a single accumulator in the order specified by io).

> n <- 1e7
> x <- runif(n)
> io <- seq_len(n) - 1L
> ir <- sample(io)
> system.time(order_sum(x, io))
   user  system elapsed
  0.022   0.000   0.023
> system.time(order_sum(x, ir))
   user  system elapsed
  0.210   0.006   0.230

The key to collapse speed is that it reads the large input vectors in order, but
then writes to the small group vector out of order.

> system.time(fsum(x, g4, na.rm=FALSE))
   user  system elapsed
  0.081   0.004   0.087

This advantage falls apart as we grow the size of the group vector by increasing
the number of repeats:

> system.time(fsum(x, g6, na.rm=FALSE))
   user  system elapsed
  0.865   0.017   0.910

But exactly what is happening is not 100% clear:

    1e4 groups                             milliseconds
    fsum ------------------------------ : 98.79 -  0.00
        fsum.default ------------------ : 98.79 - 12.73
            qF ------------------------ : 86.06 -  0.00
                hashfact -------------- : 86.06 -  0.00
                    groupfact_sorted -- : 86.06 - 70.91
                        Csv ----------- : 15.15 - 15.15

    1e6 groups                             milliseconds
    fsum ------------------------------ : 872.0 -   0.0
        fsum.default ------------------ : 872.0 - 103.5
            qF ------------------------ : 768.5 -   0.0
                hashfact -------------- : 768.5 -   0.0
                    groupfact_sorted -- : 768.5 - 636.6
                        forder.int ---- :  81.8 -  81.8
                        Csv ----------- :  47.3 -  47.3

The main incremental cost is in groupfact_sorted, and we can tell from the C
trace that's almost all from `dupVecIndex` which is where each element of the
group vector is given the order of appearance of its group (which can be used to
map inputs to group positions, possibly even after re-ordering).

We're actually possibly faster if we take up the whole hash space, which
trounces the theory?  Eh, maybe not, we're still only loading 1e3 cache lines,
which actually will fit in L1 cache.

    all.range <- as.integer(seq(1, n, length.out=1e3L))
    g4.ar <- sample(all.range, n, replace=TRUE)
    treeprof::treeprof(fsum(x, g4.ar, na.rm=FALSE))
                                           milliseconds
    fsum ------------------------------ : 80.52 -  0.00
        fsum.default ------------------ : 80.52 -  9.25
            qF ------------------------ : 71.28 -  0.00
                hashfact -------------- : 71.28 -  0.00
                    groupfact_sorted -- : 71.28 - 54.77
                        Csv ----------- : 16.50 - 16.50

If this is true it should be the case that contiguous should remain fast at
higher sizes.  Let

    n <- 1e7; ng <- 1e3
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.069   0.003   0.072
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.064   0.004   0.068


    n <- 1e7; ng <- 1e4
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.132   0.003   0.136
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.077   0.003   0.080

Let's try with different types to see what happens:

    n <- 1e7; ng <- 1e4
    set.seed(1)
    x <- runif(n) * runif(n)
    all.range <- as.integer(seq(1, n, length.out=ng))
    g <- sample(ng, n, replace=TRUE)
    g.ar <- sample(all.range, n, replace=TRUE)
    gt <- as.character(g)
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.373   0.020   0.401
    gt <- as.character(g.ar)
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.412   0.016   0.435
    gt <- g + 0
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.621   0.030   0.668
    gt <- g.ar + 0
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.693   0.027   0.730
    gt <- factor(as.character(g))
    system.time(fsum(x, gt, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.032   0.000   0.033
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.085   0.007   0.094
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  0.763   0.020   0.802
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  0.142   0.016   0.160

Pretty much as expected.  Big benefit from factors.  So really the hashes work
very well with sequential integers, whereas the sorting version should be about
the same for everything?.

    system.time(order(x))
    ##   user  system elapsed
    ##  0.587   0.026   0.620
    system.time(order(g))
    ##   user  system elapsed
    ##  0.201   0.005   0.208
    gt <- as.character(g)
    system.time(order(gt, method='radix'))
    ##   user  system elapsed
    ##  0.271   0.006   0.281

Largest number of groups that will fit in L1 would be 16K, not enough to start
evicting out of L2 for the all range one.  To do that we need 32K entries.  At
64K entries we should still fit adjacent in L2.

    n <- 1e7; ng <- 2^16
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.358   0.008   0.371
    system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.136   0.004   0.140

This suggests a massive crash in performance as for all range half will have
been done in L2, and half out.  We can go up to 500K entries contiguous before
we start exhausting L2 cache, so let's try with 250K groups:

    n <- 1e7; ng <- 2^18
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.599   0.010   0.616
    system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed
    ##   0.343   0.007   0.353

**We're assuming no hash collisions whatsoever**

Hmm, not what I expected.  For this case we should be needing 16MB worth of
cache lines in the ar case, but only 1MB of cache for the contiguous case,
thinking about the hash alone.  But there are other memory accesses too:

* Seq - `iid <- px[i]` read entire vector sequentially.
* Rand - `hid <- h[iid]` read the hash per the input vector.
* Rand - `px[hid-1]` read input per hash value for first obs of group.
* Seq - `pans_i[i]` write

Basically, how much of the hash table can we keep in cache given competing
demands from the sequential reads and writes, and the random accesses.  So we
need a measure of age of the oldest hash access as a function of the number of
entries read?

For the 1e3 groups case, let's say on average we hit every group every thousand
reads (going to be more than that, but it's all probabilistic), then we're
using:

* 1000 x 2 x 4 (sequential integers, read and write).
* 1000 x 64 (random integers, but focused on the front part of the input)
* For the hash acces:
    * 1000 x 64 for the spaced out case
    * 1000 x 4 for the normal case

Or:

    x * 2 * 4 + x * 64 + x * 64  == x * (8 + 64 + 64) == 136 * x
    x * 2 * 4 + x * 64 + x * 4   == x * (8 + 64 + 4)  ==  76 * x

Second number is questionable, and also whether we can assume we're going to hit
every group value after ng writes and thus avoid eviction.

One conclusion from this is if we know the range and we know we won't have
collisions in the hash, we can dramatically reduce our cache utilization by
avoiding the random read back from the input vector to check whether the hash
hit is a match or a collision.

The most questionable number is the second one.  Not quite sure how to think
about the fact we're only reading from the section that contains every first
instance of a group, and at what point we start overlapping in cache lines.
Probably not a factor at 1e3.

So, per these numbers, in the contiguous case we would start thrashing L1 at 826
groups, and L2 at 27.6K, vs 481 and 15.4K.

    n <- 1e7; ng <- 1600000
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    g <- sample(ng, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    system.time(fsum(x, g, na.rm=FALSE))

Empirical tests:

     ng   all  cntg
    480  .086  .076
    826  .081  .074
   1600  .088  .076
   2400  .115  .072
   4800  .151  .074
   5200  .134  .081
   7500  .143  .091
  15000  .178  .097
  25000  .195  .124
  35000  .340  .148
  50000  .393  .156
  75000  .446  .192
 100000  .496  .200
 200000  .636  .349
 400000  .807  .549
 800000 1.040  .821
1600000 1.208 1.081

Empirical data suggests a 5-6x cache utilization advantage for contiguous.

    x * 2 * 4 + x * 64 + x * 64  == x * (8 + 64 + 64) == 136 * x
    x * 2 * 4 + x * 64 + x * 4   == x * (8 + 64 + 4)  ==  76 * x

So if we drop the middle term:

    x * 2 * 4 + x * 64  == x * (8 + 64) == 72 * x
    x * 2 * 4 + x * 4   == x * (8 + 4)  == 12 * x

We get very close, and also this matches the key break points, with 5.4K and 1K
entries for L1, and 175K and 30K entries for L2.  This matches the timings very
well.  Don't understand why the reads back from the input vector (which should
require whole cache line loads) don't seem to figure in this math.  Actually,
it's likely because of things being concentrated in the beginning of the vector
as speculated:

    > z <- 10000; sum(1:5000 %in% g[1:z])/z
    [1] 0.4358
    > z <- 10000; sum(1:5000 %in% g[1:z])/5000
    [1] 0.8716

So most initial group instances are going to be very concentrated, with only a
few stragglers.

Tradeoffs are:

* Only access ooo the smaller group vector (we would need to do this for every
  input vector).
* Only need to do the very slow stop once.

What's still not clear is why the magnitude of the slowdown is so large for
`groupfact_sorted` (`dupVecIndex`).  Most likely once the group vector gets
large enough we start trashing cache.  E.g. at 1e6 group size, we're at 2x cache
(4MB).  But that should affect both the generation of the hash as well as the
writing to the group vector.  It does, in fact the writing to the group vector
proportionately becomes much slower (assuming that's the unattributed time in
`fsum.default`, we see  271:51 (5.3:1) vs 501:160 (3.13) from the trace, so
increasingly the sum access becomes a bigger issue.

Running some timings on a larger memory system (16GB, 4MB of L3 cache).  Here
we're dealing with group sizes that in the contiguous case just fit in cache at
1e6 * 4 (n/100):

    set.seed(1)
    n <- 1e8
    x <- runif(n) * runif(n)
    g <- sample(ng, n, replace=TRUE)
    library(r2c)
    library(collapse)
    f <- r2cq(sum(x))

    ng <- n/10

    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  7.421   0.477   7.919
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ## 12.264   0.580  12.888

    ng <- n/100
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  7.174   0.571   7.766
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  5.101   0.401   5.520

    ng <- n/1000
    system.time(res <- group_exec(f, g, x))
    ##   user  system elapsed
    ##  6.512   0.543   7.077
    system.time(fsum(x, g, na.rm=FALSE))
    ##   user  system elapsed
    ##  1.280   0.184   1.471

    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  3.672   0.445   4.131

At n/1000 we're talking 400KB contiguous or 6.4MB, so for the latter just 50%
overflowing L3 cache.

    ng <- n/100
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    system.time(fsum(x, g.ar, na.rm=FALSE))
    ##   user  system elapsed
    ##  6.877   0.590   7.566

At 2n with n/100 we're 50% in cache for the contiguous case.  Maybe should try
the n/10 case with g.ar?

    set.seed(1)
    n <- 2e8
    ng <- n/100
    x <- runif(n) * runif(n)
    g <- sample(ng, n, replace=TRUE)
    ## system.time(res <- group_exec(f, g, x))
    ##    user  system elapsed
    ##  18.133   1.306  19.516
    ## system.time(fsum(x, g, na.rm=FALSE))
    ##    user  system elapsed
    ##  10.954   0.814  11.783
    all.range <- as.integer(seq(1, n, length.out=ng))
    g.ar <- sample(all.range, n, replace=TRUE)
    ## system.time(fsum(x, g.ar, na.rm=FALSE))
    ##    user  system elapsed
    ##  17.222   1.158  18.458

## Notes vs data.table

Not entirely sure what's going on with `data.table`.  For the 10e7 case we're
seeing (run 10x and profiled with instruments, cleaned up):

    2.87 s            0 s	  R_doDotCall
    2.41 s      967.00 ms	   gforce
    1.22 s            0 s	         R_doDotCall
    1.22 s      302.00 ms	          gsum
    908.00 ms   908.00 ms	           gather
    11.00 ms     11.00 ms	           platform_bzero$VARIANT$Haswell
    222.00 ms          0 s	    Rf_allocVector
    354.00 ms   340.00 ms	   uniqlist
    71.00 ms    20.00 ms	   uniqlengths
    29.00 ms    29.00 ms	   DYLD-STUB$$INTEGER

Only ~1/10th of the time seems to be spent within gsum itself.

# Benchmarks

* 1e7 with group size 10, 1000.
* Pre-sorted
* sum(x), sum(x + y)

In this case mean did not get inlined, whereas the others did.  Oddly subtract
shows up three times, but still not inlined.

    # 3.01 s   10.2%	54.00 ms	                       run
    # 1.90 s    6.4%	1.75 s	 	                       mean
    # 153.00 ms    0.5%	37.00 ms                        R_finite
    # 166.00 ms    0.5%	166.00 ms                      multiply
    # 160.00 ms    0.5%	160.00 ms                      sum
    # 156.00 ms    0.5%	156.00 ms                      sum
    # 128.00 ms    0.4%	128.00 ms                      subtract
    # 125.00 ms    0.4%	125.00 ms                      subtract
    # 109.00 ms    0.3%	109.00 ms                      sqr
    # 105.00 ms    0.3%	105.00 ms                      subtract
    # 96.00 ms    0.3%	96.00 ms                       divide
    # 6.00 ms    0.0%	6.00 ms	 	                       DYLD-STUB$$R_finite

We tried only having two calls to mean, but there is something preventing
inlining, and non-inlining is expensive.  Maybe R_FINITE prevents inlining?
Although that's weird b/c from timings, it seems the inlining stopped happening
after we revised the interface to have all the parameters.

`mean0` does show non-inlining.  Simplifying parameter structure does not seem
to allow `mean` to become inlined.

To conclude, there is no evidence that `mean` was ever inlined in the earlier
tests.  This was a whole big red herring for a performance "regression" that was
really just accidentally computing on `x` as integer instead of `numeric`.

Comparing to `fsum`, `fsum` appears to have less overhead (as it should), at
group sizes of 1000 its performance is comparable to `r2c`, at group sizes of 10
it is twice as fast (i.e. overhead for r2c is about what it takes to compute 10
additions).

It appears `fmean` runs a single pass.  Additionally both `fsum` and `fmean` use
64 bit accumulators, which performance wise is not an advantage on my CPU
(looking at timings at group size of 1000).  It's actually a small disadvantage.


# Done / Answered

* Make sure that no non-numeric data exists other than the stuff going into
  control.

Yes, we only append the numeric data.

# Other Implementations

## ast2ast

https://github.com/Konrad1991/ast2ast/

## sph-sc

Scheme to C via @wdkrnls (Kyle Andrews).

## armacmp

https://github.com/dirkschumacher/armacmp

## nCompiler

https://github.com/nimble-dev/nCompiler

## Graal

Main question here is how limiting the R-C boundary is.  Is it the case that
code such as `data.table` is not any faster?  Does that even run?  How many of
the packages actually work?  What stuff does not work?  Seems like there should
be a bit of stuff that doesn't per TK.


## Renjin

## Odin

Fascinating implementation, but the key issue seems to be that it seems to be
very much focused on the derivative syntax of DeSolve and thus abandons R
semantics.

Supports things such as (from array portion of vignette):

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ]))

But not:

    deriv(y[]) <- r[i] * y[i] * (1 - sum(ay[i, ] + 3))

I.e. it won't recursively construct the expressions.  But it can be done with:

    tmp[,] <- ay[i,j] ^ 2
    deriv(y[]) <- y[i] * (1 - sum(tmp))
    ...
    dim(tmp) <- c(n_spp, 10)   # probably can submit the 10 as user()?

Additionally, the concept of arrays is really a stand in for multiple variables.
In the above example `i %in% 1:4` so it's a four variable system and could be
written as such, not really the concept of vectorized data intended to be
aggregated.

We can get "vectors" in the sense we're used to in there, but they need to go in
as matrices (but that's okay, we can just do this with 1D matrices).


# Concept

1. Given a parsed R expression
2. Check that expression / function has symbols that resolve only to known
   functions.
3. Translate it into C code.
4. Compile it.
5. Feed it.

# What needs to happen

1. A compiled function that accepts N double pointers.
2. A manager to call the compiled function repeatedly with variations.

# Features

1. Standard arithmetic.
2. Sum. Mean.
3. Assignment?  Probably not.

# Translation

## Memory

* Calling function will allocate a vector sufficiently large to hold the
  largest result and pass that to each computing function.
* Computing functions may only write to that vector?  Specifically, computing
  functions may not allocate?

## Length Computation

Each known function should have some method of ex-ante determining the length of
the outputs as a function of the lengths of the inputs (e.g. `quantile` would be
the length of the `probs` argument).

## Ex Ante Size Computation

We need some view of the size pre-compilation, but by virtue of allowing
references to external objects we can't resolve them fully.  So we have options:

* Group size
* Constant size (knowable at compile time)
    * Scalar (e.g. result of `mean(x)`)
    * Other constant (e.g. result of `range`)
* External, could be anything (zero, scalar, group size, whatever)

At compilation time we may be able to emit better code if we know if the inputs
are scalar / group size, vs if we don't we'll need a per-group conditional to
decided.  So the code generation needs to be able to run with the partial
information.

So we need a pre-processing pass that does all the size computations, but not
the actual allocations, and then a second pass once the input sizes are known.

How do we handle something like:

    pmax(scalar, external, group, constant)

For "vecrec" probably need to resolve to the worst in order:

    scalar > constant > group > external > external_or_group

But we must always keep track of whether group is involved.

One big issue is we don't know until the evaluation stage what columns are going
to be group vs external because we haven't seen the data.frame.  So we can
compile highly efficient code if we know the situation, but less so otherwise.

Similarly, if we want to pick code based on the value of control parameters, we
need to evaluate them at compile time.  This severely cramps our style.  The
control param evaluation at compile time is probably okay.

To resolve the knowing the data columns, we have two choices:

* Force all data columns to be the same size (what about non-data params like
  `probs`)?
* Make the code capable of handling different lengths inputs.

For the latter, we could have the code react to each group, but it would be much
better if it could before running pick the correct one.  Probably not possible
without recompilation.

Some costs of not being able to do the full optimization:

    > n <- 1e7
    > x <- runif(n)
    > g <- sample(n)
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 0L))
       user  system elapsed
      1.077   0.005   1.087
    > sys.time(res <- run_group(shlib, "fun4", list(x), g, 1L))
       user  system elapsed
      1.076   0.004   1.085

These above are with `na.rm` as an explicit if/else, where group size is 1..

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed
      1.030   0.004   1.040

Here instead we assume `na.rm = FALSE` so no extra if/else.  For reference this
is what the above looks like with group size = 10:

    > sys.time(res <- run_group0(shlib, "fun3", list(x), g))
       user  system elapsed
      0.778   0.004   0.788

And finally back to group size = 1 we now add a `asInteger(VECTOR_ELT(x, 0))` to
retrieve the `na.rm` flag:

    > sys.time(res <- run_group1(shlib, "fun5", list(x), g, 0L))
       user  system elapsed
      1.147   0.002   1.149

So a 10% penalty, which suggests some value in allowing for a single flag to be
passed down.  But maybe this becomes a future feature.

For arithmetic, there is seems to be very little value in having the simplified
logic option (in fact, it looks slower, `fun7` is the one that goes straight to
the equal length loop).  So no huge cost.  At least when there is no contention
with any other processes.  Given that sorting is such a big part of the cost,
it's going to be hard to beat that.

    > sys.time(res1 <- run_group2(shlib, "fun6", list(x, y), g))
       user  system elapsed
      1.058   0.007   1.071
    > sys.time(res2 <- run_group2(shlib, "fun7", list(x, y), g))
       user  system elapsed
      1.069   0.008   1.084
    > sys.time({
    +   o <- order(g)
    +   xo <- x[o]
    +   yo <- y[o]
    + })
       user  system elapsed
      0.783   0.005   0.794
    > sys.time(res3 <- xo + yo)
       user  system elapsed
      0.022   0.000   0.022
    > all.equal(res1, res2)
    [1] TRUE
    > all.equal(res1, res3)
    [1] TRUE
    >

## Allocation

Once we get to the run stage, we'll have for each sub-call, the type size of the
result, and the inputs, and the type of function.  This means we can resolve the
size requirements to a specific number for external things, and for external or
group we now know the "external" part of it as well.  Since we'll know the max
group size, we can run through?

To do this properly we need to know which column each thing refers to, but we
should be able to do it linearly.

So start going column-wise through our size matrix.  First item should be
standalone (i.e. not a call with arguments).  Add it to the allocation (unless
it's data, in which case it is already there), evaluating first if needed, and
recording the position in the data as well as the size (possibly NA if group).
Keep accumulating this info in a stack until we get to a call, which will then
reduce it down to a single value, and proceed accordingly.

For each item, we need to know what it is:

* Control (evaluate, store, and record index)
* Symbol external (evaluate, store, record size and index)
* Symbol data (record index)
* Expression (recurse, record size and index)

## Simple arithmetic

Compute length of each vector.  Generate the full C expression with index or
pointer offset access.

    x <- runif(5e6)
    y <- runif(5e6)
    z <- runif(5e6)
    w <- runif(5e6)
    >
    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed
      0.025   0.000   0.025
    > sys.time(fapply:::test2(x, y, z, w))
       user  system elapsed
      0.018   0.000   0.018

It is slightly faster to do x + y + z + w (test2) than do pairwise additions
across the full vectors (test1).  Maybe because we only increment the loop
variable once?

With `MOD_ITERATE_CHECK2` and checking every 1e6:

    > sys.time(fapply:::test3(x, y, z, w))
     user  system elapsed
    0.036   0.000   0.038

Interestingly `MOD_ITERATE2` is no faster, so the check is basically free.

`R_ITERATE_CHECK` is comparable to pairwise addition (test1):

    > sys.time(fapply:::test1(x, y, z, w))
       user  system elapsed
      0.026   0.000   0.028
    > sys.time(fapply:::test5(x, y, z, w))
       user  system elapsed
      0.026   0.000   0.028


## Function Types

* Aggregation functions (result length == 1)
* Vectorized functions  (result length == 0 or longest vector)
* Arbitrary functions   (result length is known (above is degen case of this))

Maybe use iterator macros.  E.g. `MOD_ITERATE2_CHECK`.  These should work fine
and are fairly efficient (i.e. reset counter to zero intead of reading with
modulo).

When construction the expression, when do we need to recursively evaluate
sub-expressions?  E.g. in:

    x + y + z

Which is really:

   `+`(`+`(x, y), z)

How do we know we can just turn it into:

    for(i = 0; i < len; ++i) x[i] + y[i] + z[i];

Vs

    x + y + mean(z)
    x + y + sort(z)

It's really that there is no mixing moving of the vectors.  Associativity /
commutativity shouldn't matter as that's handled by C / parentheses.

If we have a complex expression with a lot of these then it potentially gets
tricky b/c we start requiring intermediate storage for computing the whole
expression.

So we need to distinguish between "vectorizable" functions that operate one
element at a time, and those that aren't that operate on more than one element
at a time (either by moving them, etc).  The latter have to be evaluated
separately.  So the "parser" will need to identify where such functions are and
then evaluate/reduce them to a form that can then be used in a vectorized
context.

Because of this we might just favor the iterative resolving of the expression.
This will be easier.  We could just have logic to try to make the biggest
vectorized expression.  This would avoid a lot of extraneous code.

So right now:

    x + y + z

Becomes

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];
    for(i = 0; i < n; ++i) res[i] = res[i] + z[i];

And:

    x + y + mean(z)

    double * res;
    for(i = 0; i < n; ++i) res[i] = x[i] + y[i];

    double tmp = 0;
    for(i = 0; i < n; ++i) tmp += z[i];
    tmp /= n;

    for(i = 0; i < n; ++i) res[i] += tmp;

So we'll need as many temporary scalars / vectors as there are arguments to
functions.  Maybe we don't allow anything but binary?  Hmm, so with:

    pmax(-x, log(y), mean(z), w - u)

In all cases we'll know the size each argument.  In the expression above we know
how many parameters there are so we can construct the correct code at
"compilation" time.  What about:

    pmax(pmin(y, z, w), pmin(w, q, f), mean(z + y))

Generated code:

    double * tmp0 = R_alloc();
    for(i = 0; i < n; ++i) {
      tmp[i] = z[i] + y[i];
    }
    double tmp1 = mean(z + y);

    for(i = 0; i < n; ++i) {
      tmp0 = pmin(y[i], z[i], w[i]);
      tmp2 = pmin(w[i], q[i], f[i]);
      res[i] = pmax(tmp2, tmp3, tmp1);
    }

And (numbers show depth):

    pmax(pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y))
         pmin(y, z, mean(w)), pmin(w, q, f), mean(z + y)
                    mean(w)                  mean(z + y)
                                                  z + y


Generated code.  Coords are (depth, param)

    // coords (0,0),(1,0)(2,2)
    double stmp0;
    stmp0 = mean(w);

    // coords (0,0),(1,0)
    double * vtmp0 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(y[i], z[i], stmp0);

    // coords (0,0),(2,0)
    // * stmp0 no longer needed
    double * vtmp1 = R_alloc();
    for(i = 0; i < n; ++i) vtmp0[i] = pmin(w[i], q[i], f[i]);

    // coords (0,0),(2,2),(3,0)
    double * vtmp2 = R_alloc();
    for(i = 0; i < n; ++i) vtmp2[i] = z[i] + w[i];

    // coords (0,0),(2,2),(2,0)
    stmp0 = mean(vtmp2);

    // coords (0,0)
    // * vtmp2 no longer needed, so re-use
    double * vtmp3 = R_alloc();
    for(i = 0; i < n; +++i) vtmp2[i] = pmax(vtmp0[i], stmp, vtmp1[i]);

We need to track how many variables we've generated and when each one is freed,
without garbage collection.  So for each variable we need to track the depth at
which they are used.

Ideally we'll know for each group exactly how many variables we'll need and of
which size.  So we want to allocate the minimal amount w/ re-use that we need.
Number of variables should be at worst `sum(pmax(length(args) - 1)` or some
such.

One problem is that computing ex-ante how big each of our intermediate
allocations is going to be could be costly.  For `pmax`, we have the group based
params that are known sizes, but there could be external params of unknown
sizes.  So we could reduce the size expectation to be something like
`max(c(G, a, b, c, ...))`.

Provided options:

* Length of group
* Constant length (mean/sum/prod->1; Max returns -Inf)
* Length of one specific argument (quantile)
* Recycled max of multiple arguments (different from plain max because if one of
  the arguments is known to be zero, then the lot is zero).

Not provided options:

* Function of length of single argument.
* Function of lengths of various arguments.

This means we only care about the maximum group size as we'll always want to
allocate to that group size at a minimum.

So we navigate the entire parse tree, and for each function we retrieve the type
from above.  This will require:

* Reducing the function to its numeric arguments.
* Looking up the type of function.
* Computing the maximum return size as a function of form `max(c(G, k))` where
  `k` is a constant presumably derived from external variables, or possibly 0.

For a single function:

* Confirm it is a known function by getting it and comparing it to our list.
* `match.call`.
* Identify all non-group parameters.
* Evaluate all non-group parameters and bind them to local symbols.
* Check that all the C parameters are double (either a group param, or double).
* Depending on function type
    * (if needed) Compute lengths of all non-group C parameters, and record max
      of them or zero in `K`.
    * Record shorthand size, e.g. `K`, `g`, or `max0(g, K)`, where `max0`
      returns 0 if any of `g`, `k`, are zero, and `K` is a scalar constant.
    * We thus need a structure that contains a constant, whether the constant is
      set (we need to distinguish zero), and whether the group size matters.

Across the full expression, given `g` is equal to max group size:

0. Initialize vector of temporary items.  This is an array of R_xlen_t sizes
   sorted by size and an array of integers denoting whether the corresponding
   entries are free or busy.
1. Confirm function is valid.
    * Either resolves against list.
    * Or does not contain any references against symbols in the grouped data.
2. Based on function type:
    * Invalid: STOP.
    * Valid no symbols: eval and return length as constant (SIDE EFFECTS?).
    * Valid referencing symbols: continue to 3.
3. Count function parameters that are of unknown length.
4. For each unknown parameter, recurse to 1.
5. Once all parameters are of known length, compute expression length.
6. Scan through free entries to see if any are big enough.
    * For entries with group size, use the biggest group size.
    * An entry is free if its depth is greater than one below the current level.
    * If yes, mark as busy with the current depth.
    * If no, find the spot to add a new entry and mark with current depth.
    * Process of adding an entry should preserve sorting of list.
5. Return expression length computed in 5.

Now that we have the expression length, compute the result vector size.  It
should be either a constant per group, or `g`.

The top level return should give us the result size, and our list should give us
the set of intermediate vectors we'll need.

## Parameter Types

There are:

* Numeric standard vectors from data
* Numeric standard vectors external
* Non-numeric data (do we even allow this?  How do offset work)
* Non-numeric externals

For non-numeric data, an example would be if someone wants to run e.g. a nested
tapply on each group (this is silly, should just group on the interaction, but
whatever).

Some of these will be "control" parameters, which loosely speaking are those
that are used in the code generation phase, and aren't necessarily (but can be)
passed on to the compiled code.

Key question right now is what gets into the `alloc.dat` structure.  Presumably
it is anything that could possibly feed a C parameter that it intends to compute
on, and more specifically, one that could be fed by the result of a previous
calculation.  Our key restriction is that the functions must all return doubles
no matter what.

Or maybe it is any parameter that in theory could be fed by group data.  Which
in theory is any numeric parameter that is not explicitly made to be a control
parameter?  Yes, this seems to be the most manageable approach.  This also
implies that all non-control parameters must be numeric.

Do we allow non-numeric, non-control parameters?  Is there any use for them
other than grouping?  Maybe something designating a type of calculation to be
done?  But when do we ever want those to be the same length as the vector?

## Interface

### R Level

Each special function must be pre-registered with some mechanism to distinguish
which parameters are vector ones that will be fed, vs which one should be
evaluated immediately for dispatch decisions (e.g. `na.rm=TRUE`).

How does this work for a newly defined function at the R level?  Does it matter?

### C Level

#### Basic

Target function:

* Array of double pointers the same length as it expects number of arguments.
* Int with number of arguments.
* Array of R_xlen_t with lengths of each of the arguments.
* Array of R_xlen_t with offsets of each of the arguments.
* Double array of right size for result.

Should we have some structs pre-defined to hold the above?

Can the function check it's doing the right thing in terms of argument count?
Would it be better to have the arguments provided explicitly (in which case we
really want structs?  Should the "Array of double pointers" already be at the
expected offsets (maybe).  Do we want iterators like R provides to walk through
all the arguments?

Maybe we don't want the lengths of each of the arguments; instead, we want the
length of the result, and the modulo to read all the other arguments not of that
length?

should accept an expects arguments.  All functions should be like that.

Parameters such as `na.rm=TRUE` actually cause dispatch to a different
function.  So when we register a function we need to declare this?  I guess it
should be optional and we use it to recreate `mean`, etc.

#### Allow SEXP

What about functions that accept non-double parameters?  In particular, we'll
have expressions that reference external/evaluated things, which might not be
double.  If we want to allow these to be fed to our functions, our functions
need to be able to accept SEXPs.  This substantially complicates the interface;
are there cases where we would want to do this?

An almost example is `quantile`, where the `probs` param is likely fetched from
elsewhere.  Or `filter`.  Essentially anything where the configure param
resolves to an arbitrary number of possibilities and thus can't be reduced to
different entry points.  `findInterval` might be a better example.

Seems like we have two choices:

* Allow additional SEXP arguments (and identification of what these are).
* Add ALTREP so that all arguments can be SEXP.

The latter seems cleaner.  What happens with the result vector at that point?
Probably can't be ALTREP as we'll need to modify it.

How much overhead does ALTREP add?  There is a good example from Gabe about
implementing window functions.

So is this worth doing?  What is the advantage of doing so.  More familiarity
for the implementer of new functions?  Maybe for now we just pass through as a
`moreArgs` list.  So we end up with `double **` for the data arguments, along
with offset and length, which could include some vectors that are not-data, and
a list of extra arguments.  Does this obviate the need for a configure function?
Probably, it then happens in C.  The sad thing about this is we have to redo all
the configure business for each call.  Hmm.

Related, could this eventually be made to use the actual R functions?  The
summary functions are all in "src/main/summary.c" and those are simple, but the
problem is they are all static and dispatched to by `do_summary`.  And calling
the latter will be very challenging from outside.

# Pre-Processing

Once we have the expression parsed, do we want to link in pre-compiled entry
points, or do we just want to generate the whole thing in one piece of generated
code?  Probably start with one piece of generated code.

    sum(((x - mean(x)) * (y - mean(y, na.rm=TRUE))) + z)

What does this become?  Each sub-expression needs to be generated in linearized
code.  There will be a vector of allocated vectors for temporary use.  So we
need a list where each element is a step, and each step should include:

> We probably want a version for variable argumens (e.g. `pmin`, and one for
> fixed argument count).

* That data structure, which is an array of double pointers with:
  * Group data at the beginning.
  * Non-group double data next.
  * Temporary arrays.
  * Result array.
* An integer count of parameters into the data structure.
* An array of offsets into the data structure.
* An array of offsets into each vector in the data structure.
* An array of lengths of each vector in the data structure.
* A `VECSCXP` of all the control parameters.

So in C our earlier expression will look like (this does not include `ctrl`):

Variable arg count:

    static void %s(
      double ** data, int narg, int * datai,
      R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Fixed arg count:

    static void %s(
      double ** data, R_xlen_t * off, R_xlen_t * len, SEXP ctrl
    ) {

Maybe an option without control?  So two switches:

* `vararg`
* include `ctrl`

Additionally, if we allow different signatures then they need different names.
To the extent we re-use a single function multiple times, we want to detect
this.

Each code gen call should return:

* Function definition.
* Function name.
* Function call (since we can have different # of arguments).

We can check that if multiple instances of a function name show up, the
definitions are the same, and then unique them out.


# Compilation

## Cost

For a super simple compile:

```
$ time clang -g -c -O2 test.c -o test.o

real	0m0.115s
user	0m0.065s
sys	0m0.045s
```

But first time calling clang can take a long time (e.g. 6 seconds).  One
question whether adding R stuff slows down the compilation.  Doing it with `R
CMD SHLIB` but then obviously gives us access to a lot more options:

```
time R CMD SHLIB test-r.c
clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -fno-common -std=c99 -pedantic -Wall -Wextra -c test-r.c -o test-r.o
clang -mmacosx-version-min=10.13 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o test-r.so test-r.o -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation

real	0m0.361s
user	0m0.235s
sys	0m0.112s
```

Also, nice thing about R CMD SHLIB is that it abstracts away calling the
compiler (on windows)?  Oddly, calling the plain c file is slower!  Well, at
least not faster, so the overhead is from firing up make, etc.

> Note It's not actually slower as we need the extra step to generate the .so.

## Loading

### Hack approach

Ideally we would be able to use `dyn.load`, but then we need `.Call` which
obviously we're trying to avoid.  This is going to be really complicated, see
"src/main/dodotcode.c".

So we can use `R CMD SHLIB` + `dyn.load`, but we now need to figure out how
`.Call` finds the symbol in that case.

    > xx <- dyn.load('test-r.so')
    > xx
    DLL name: test-r
    Filename: /Volumes/PERSONAL/repos/fapply/test-r.so
    Dynamic lookup: TRUE
    > .Call('test', runif(10), PACKAGE='test-r')
    [1] 6.270841

Looks like maybe we can use (cadged from `resolveNativeRoutine`):

    DL_FUNC *fun;
    char buf[MaxSymbolBytes];
    R_RegisteredNativeSymbol *symbol;
    R_RegisteredNativeSymbol symbol = {R_CALL_SYM, {NULL}, NULL};
    *fun = R_FindSymbol(buf, dll.DLLname, symbol);

**Bad News**: we need `Rf_RegisteredNativeSymbol` which is in Rdynpriv.h, which
is not installed, etc.  We can just dummy it since it is only used optionally.

Which is used in (where `*fun` above is loaded into `&ofun` below):

    args = resolveNativeRoutine(args, &ofun, &symbol, buf, NULL, NULL, call, env);
    retval = R_doDotCall(ofun, nargs, cargs, call);

All of these seem to be in "R_Ext/Rdynload.h" so I think we're okay (though not
strictly part of the API?).

### `R_GetCCallable`

An alternative we can use, but will likely require building a package like odin
does.

Is there a way to save the function and the compiled code other than via
package?  Maybe?


# vs Rcpp?

How is this different to Rcpp?  Very limited, but no learning curve.  Only
doubles.

# Interface

Now leaning towards something like this:

```
group_exec(slope, data, groups, x=v)              ## remap
group_exec_(slope, data, groups, remap=c(x='v'))  ## remap
```

But what if the `r2c` object contains `data`, or `groups`?  Then it becomes
impossible to remap those.  We could make those reserved terms in the
compilation analysis.  How does that evolve as we add more functions that have
different parameter names?  Do we just lock up too much?  The alternative seems
to be:

```
group_exec(r2c_slope, data, groups)()
group(r2c_slope, data, groups)()
slide(r2c_slope, data, width, ...)()
r2c_slope()
```

Do we require parameters?  Yes.

```
group(r2c_slope, data, groups)(x, y)
r2c_slope(x, y)
slide(r2c_slope, data, window=5)(x, y)
```

Setting the following as default just won't work, since the lookup semantics are
to look in the function environment (and would cause the recursive lookup
issue):

```
function(x=x, y=y)
```

So we can't have a function that observes R semantics but then also has the
unmapped parameters. So things like this won't work:

```
group(r2c_slope, data, groups)()
```

But this is okay:

```
group(r2c_slope, data, groups)(x, y)
```

Since we don't have a way to declare the mapping order in a way visible to
users.  So even if the print method does something like:

```
group(r2c_slope, data, groups)
## function(x, y) <...>
```

When in reality it's:

```
## function(...) <...>
```

What does `r2c_slope` look like:

```
r2c_slope
## function(x, y) <...>
```

Hmm, but are we satisfied with the data/groups specification?

```
group(r2c_slope, data, data['g'])(x, y)
with(data, group(r2c_slope, x, y, g))
```

How about this:

```
group(obj, ..., group)

groupwise(obj, group, ...)
groupwise(obj, data, .(group), .(remap))
```

If this is the NSE interface:

```
groupwise(data, group, obj(x=x, y=y))
obj(x=x, y=y)

with(data, groupwise(obj, list(x, y), list(g)))

with(data, r2c_group(obj, list(x, y), list(g)))

r2c_group(obj, x, g)

set.seed(1)
x <- y <- runif(1e5)
y[sample(1e5, 1e4)] <- NA
microbenchmark::microbenchmark(sum(x), sum(y), mean(x), mean(y), times=10)

set.seed(1)
x <- y <- runif(1e5)
y[sample(1e5, 1e4)] <- NA
microbenchmark::microbenchmark(sum(x), sum(y), mean(x), mean(y), times=10)
## Unit: microseconds
##     expr   min    lq  mean median    uq   max neval
##   sum(x)   299   315   342    335   367   413    10
##   sum(y) 18441 18684 19587  18973 20461 22382    10
##  mean(x)   234   265   281    276   294   343    10
##  mean(y) 33009 33201 34449  33421 34584 38751    10

```

So we need:

* data
* groups
* r2c object
* remap


```
groupwise(data, group, obj, x=x, y=y)

groupwise(obj, data, group, x=x, y=y)
groupwiseq(obj, data, group, x=x, y=y)

run(obj, x, y)
```

This allows arbitrary objects in ..., and we can require that they are named.
Group can be a single vector, does not need to be named, or it can be a list.

```
obj(x, y)
objg <- with(data, group(obj, x, y, g))
objg <- with(data, group(obj, x=x, y=y, g))
objg(x, y)
```

If unnamed we can rely on the symbol.



These are very appealing for group/window because we can easily re-use the same
shared object, and additionally

```
obj <- r2c(<expr>)
obj$run(data)                # intended for code with loops.
obj$group(data, groups)
obj$slide(data, size, ...)
```

Are `groups` quoted or as is?  Do we need:

```
obj$groupq(data, groups)
```

What if we want to remap args?

```
obj$group(data, groups)(x=z, y=v)
obj $
  map(x=z, y=v) $
  group(data, groups)

group(remap(obj, x=x, y=y), data, groups)
```

Or should we really do:

```
slope <- r2c(
  function(x, y) (x - mean(x)) * (y - mean(y)) / (x - mean(x))^2
)
group(slope(x=v, y=z), data, groups)

with(data, group(slope(v, z), groups))
```




```
group(slope(x=v, y=z), data, groups)
group(slope, data, groups)                               # no remap
group_exec(slope, data, groups)                          # no remap
group_exec(slope(.remap=c(x="v", y="w")), data, groups)
group_exec(remap(slope, c(x="v", y="w")), data, groups)

group_exec(slope, data, groups)       ## no remap
group_exec(slope, data, groups, x=v)  ## remap


run(slope(x=v, y=z))
slide(slope(x=v, y=z), window=window)

slope(x=v, y=z) |> group(data, groups)
```

What would this do?  Run normally?

```
slope(x=v, y=z)
slope$group(data, groups)(x=v, y=z)
```

Then? Do we need:

```
group(slope, data, groups)(x=v, y=z)
group(slope(x=v, y=z), data, groups)     # Possible with NSE, but not ideal.
group(slope, data, groups)               # would this run without remapping?
group(slope, data, groups)()             # Or should it be this?

slope |> group(data, groups)(x, y)

slope$group(data, groups)(x, y)

group(slope, data, groups, map=.(x=v, y=z))
```

Is this confusing?  Yes, the duality of `slope` being a function but also not is
just weird.

What's the benefit of defining as a function?  Makes it easier to re-map the
variables in use, but then the remapping is weird.

```
slope$group(remap(data, x=v, y=z), groups)
remap(slope, x=v, y=z)
```


Old stuff:

```
fapply(X, INDEX, FUN)

group_exe(data, groups, obj)
window_exe(data, size, just, obj)
for_exe(data, obj)
```
